filepath,name,docstring,methods_count,base_classes,variables_count,methods,has_docstring
c:\mygit\compuse\computer_use_demo\code_context_manager.py,CodeFile,,1,[],6,['to_message'],False
c:\mygit\compuse\computer_use_demo\code_context_manager.py,CodeContextManager,,10,[],0,"['__init__', 'update_file', 'get_context_messages', 'save_to_disk', 'load_from_disk', 'remove_file', 'apply_llm_response', 'replace_file_content', 'add_code_to_file', 'extract_dependencies']",False
c:\mygit\compuse\computer_use_demo\loop.py,OutputManager,Manages and formats tool outputs and responses.,6,[],0,"['__init__', 'save_image', 'format_tool_output', 'format_api_response', 'format_content_block', 'format_recent_conversation']",True
c:\mygit\compuse\computer_use_demo\loop.py,TokenTracker,Tracks total token usage across all iterations.,3,[],0,"['__init__', 'update', 'display']",True
c:\mygit\compuse\computer_use_demo\tools\bash_original.py,_BashSession,A session of a bash shell.,2,[],6,"['__init__', 'stop']",True
c:\mygit\compuse\computer_use_demo\tools\bash_original.py,BashTool,"A tool that allows the agent to run bash commands.
The tool parameters are defined by Anthropic and are not editable.",2,['BaseAnthropicTool'],3,"['__init__', 'to_params']",True
c:\mygit\compuse\computer_use_demo\tools\code_context_manager.py,CodeFile,,1,[],6,['to_message'],False
c:\mygit\compuse\computer_use_demo\tools\code_context_manager.py,CodeContextManager,,10,[],0,"['__init__', 'update_file', 'get_context_messages', 'save_to_disk', 'load_from_disk', 'remove_file', 'apply_llm_response', 'replace_file_content', 'add_code_to_file', 'extract_dependencies']",False
c:\mygit\compuse\computer_use_demo\tools\collection.py,ToolCollection,A collection of anthropic-defined tools.,2,[],0,"['__init__', 'to_params']",True
c:\mygit\compuse\computer_use_demo\tools\expert.py,GetExpertOpinionTool,"A tool takes a detailed description of the problem and everything that has been tried so far, and returns an expert opinion on the problem.",1,['BaseAnthropicTool'],3,['to_params'],True
c:\mygit\compuse\computer_use_demo\tools\playwright.py,WebNavigatorTool,"A versatile tool that uses Playwright to interact with the web, including reading information,
navigating websites, filling forms, extracting data, interacting with dynamic elements, and downloading files.
It also integrates with external APIs and includes enhanced error handling and #logging.",5,[],3,"['__init__', 'to_params', '_create_structured_content', 'integrate_external_api', 'get_session_history']",True
c:\mygit\compuse\computer_use_demo\tools\windows_navigation.py,WindowsNavigationTool,A tool specializing in navigating Windows and Windows applications using keyboard shortcuts.,4,[],3,"['__init__', '_load_shortcuts', 'to_params', 'get_session_history']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\payload_streamer.py,_stream_wrapper,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\payload_streamer.py,streamer,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\payload_streamer.py,StreamWrapperPayload,,1,['Payload'],0,['decode'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\payload_streamer.py,StreamPayload,,1,['StreamWrapperPayload'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\pytest_plugin.py,AiohttpClient,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\pytest_plugin.py,AiohttpServer,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\pytest_plugin.py,AiohttpRawServer,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\resolver.py,ThreadedResolver,"Threaded resolver.

Uses an Executor for synchronous getaddrinfo() calls.
concurrent.futures.ThreadPoolExecutor is used by default.",1,['AbstractResolver'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\resolver.py,AsyncResolver,Use the `aiodns` package to make asynchronous DNS lookups,1,['AbstractResolver'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\aiohttp\typedefs.py,Middleware,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\annotated_types\test_cases.py,Case,A test case for `annotated_types`.,0,['NamedTuple'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,AnthropicError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,APIError,,1,['AnthropicError'],3,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,APIResponseValidationError,,1,['APIError'],2,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,APIStatusError,Raised when an API response has a status code of 4xx or 5xx.,1,['APIError'],3,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,APIConnectionError,,1,['APIError'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,APITimeoutError,,1,['APIConnectionError'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,BadRequestError,,0,['APIStatusError'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,AuthenticationError,,0,['APIStatusError'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,PermissionDeniedError,,0,['APIStatusError'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,NotFoundError,,0,['APIStatusError'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,ConflictError,,0,['APIStatusError'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,UnprocessableEntityError,,0,['APIStatusError'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,RateLimitError,,0,['APIStatusError'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_exceptions.py,InternalServerError,,0,['APIStatusError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_qs.py,Querystring,,6,[],2,"['__init__', 'parse', 'stringify', 'stringify_items', '_stringify_item', '_primitive_value_to_str']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_qs.py,Options,,1,[],2,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_resource.py,SyncAPIResource,,2,[],1,"['__init__', '_sleep']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_resource.py,AsyncAPIResource,,1,[],1,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\asttokens\line_numbers.py,LineNumbers,"Class to convert between character offsets in a text string, and pairs (line, column) of 1-based
line and 0-based column numbers, as used by tokens and AST nodes.

This class expects unicode for input and stores positions in unicode. But it supports
translating to and from utf8 offsets, which are used by ast parsing.",4,['object'],0,"['__init__', 'from_utf8_col', 'line_to_offset', 'offset_to_line']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\asttokens\util.py,Token,"TokenInfo is an 8-tuple containing the same 5 fields as the tokens produced by the tokenize
module, and 3 additional ones useful for this module:

- [0] .type     Token type (see token.py)
- [1] .string   Token (a string)
- [2] .start    Starting (row, column) indices of the token (a 2-tuple of ints)
- [3] .end      Ending (row, column) indices of the token (a 2-tuple of ints)
- [4] .line     Original line (string)
- [5] .index    Index of the token in the list of tokens that it belongs to.
- [6] .startpos Starting character offset into the input text.
- [7] .endpos   Ending character offset into the input text.",1,"[""collections.namedtuple('Token', 'type string start end line index startpos endpos')""]",0,['__str__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\asttokens\util.py,NodeMethods,Helper to get `visit_{node_type}` methods given a node's class and cache the results.,2,['object'],0,"['__init__', 'get']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\attr\__init__.py,AttrsInstance,,0,['Protocol'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\dataclasses_json\core.py,_ExtendedEncoder,,1,['json.JSONEncoder'],0,['default'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\dataclasses_json\mm.py,_TimestampField,,2,['fields.Field'],0,"['_serialize', '_deserialize']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\dataclasses_json\mm.py,_IsoField,,2,['fields.Field'],0,"['_serialize', '_deserialize']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\dataclasses_json\mm.py,_UnionField,,3,['fields.Field'],0,"['__init__', '_serialize', '_deserialize']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\dataclasses_json\mm.py,_TupleVarLen,variable-length homogeneous tuples,1,['fields.List'],0,['_deserialize'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\dataclasses_json\utils.py,_NoArgs,,4,['object'],0,"['__bool__', '__len__', '__iter__', '__next__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\executing\_exceptions.py,KnownIssue,"Raised in case of an known problem. Mostly because of cpython bugs.
Executing.node gets set to None in this case.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\executing\_exceptions.py,VerifierFailure,"Thrown for an unexpected mapping from instruction to ast node
Executing.node gets set to None in this case.",1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\filelock\_soft.py,SoftFileLock,Simply watches the existence of the lock file.,2,['BaseFileLock'],0,"['_acquire', '_release']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\compression.py,SnappyFile,,5,['AbstractBufferedFile'],0,"['__init__', '_upload_chunk', 'seek', 'seekable', '_fetch_range']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\dircache.py,DirCache,"Caching of directory listings, in a structure like::

    {""path0"": [
        {""name"": ""path0/file0"",
         ""size"": 123,
         ""type"": ""file"",
         ...
        },
        {""name"": ""path0/file1"",
        },
        ...
        ],
     ""path1"": [...]
    }

Parameters to this class control listing expiry or indeed turn
caching off",9,['MutableMapping'],0,"['__init__', '__getitem__', 'clear', '__len__', '__contains__', '__setitem__', '__delitem__', '__iter__', '__reduce__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\exceptions.py,BlocksizeMismatchError,"Raised when a cached file is opened with a different blocksize than it was
written with",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\exceptions.py,FSTimeoutError,Raised when a fsspec function timed out occurs,0,['asyncio.TimeoutError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\fuse.py,FUSEr,,13,['Operations'],0,"['__init__', 'getattr', 'readdir', 'mkdir', 'rmdir', 'read', 'write', 'create', 'open', 'truncate', 'unlink', 'release', 'chmod']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\parquet.py,FastparquetEngine,,3,[],0,"['__init__', '_row_group_filename', '_parquet_byte_ranges']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\parquet.py,PyarrowEngine,,3,[],0,"['__init__', '_row_group_filename', '_parquet_byte_ranges']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\transaction.py,Transaction,"Filesystem transaction write context

Gathers files for deferred commit or discard, so that several write
operations can be finalized semi-atomically. This works by having this
instance as the ``.transaction`` attribute of the given filesystem",5,[],0,"['__init__', '__enter__', '__exit__', 'start', 'complete']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\transaction.py,FileActor,,4,[],0,"['__init__', 'commit', 'discard', 'append']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\transaction.py,DaskTransaction,,2,['Transaction'],0,"['__init__', 'complete']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\ftfy\__init__.py,ExplanationStep,"A step in an ExplainedText, explaining how to decode text.

The possible actions are:

- ""encode"": take in a string and encode it as bytes, with the given encoding
- ""decode"": take in bytes and decode them as a string, with the given encoding
- ""transcode"": convert bytes to bytes with a particular named function
- ""apply"": convert str to str with a particular named function

The `parameter` is the name of the encoding or function to use. If it's a
function, it must appear in the FIXERS dictionary.",1,['NamedTuple'],2,['__repr__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\ftfy\__init__.py,ExplainedText,"The return type from ftfy's functions that provide an ""explanation"" of which
steps it applied to fix the text, such as :func:`fix_and_explain()`.

When the 'explain' option is disabled, these functions return the same
type, but the `explanation` will be None.",0,['NamedTuple'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\ftfy\__init__.py,TextFixerConfig,"A TextFixerConfig object stores configuration options for ftfy.

It's implemented as a namedtuple with defaults, so you can instantiate
it by providing the values to change from their defaults as keyword arguments.
For example, to disable 'unescape_html' and keep the rest of the defaults::

    TextFixerConfig(unescape_html=False)

Here are the options and their default values:

- `unescape_html`: ""auto""

  Configures whether to replace HTML entities such as &amp; with the character
  they represent. ""auto"" says to do this by default, but disable it when a
  literal < character appears, indicating that the input is actual HTML and
  entities should be preserved. The value can be True, to always enable this
  fixer, or False, to always disable it.

- `remove_terminal_escapes`: True

  Removes ""ANSI"" terminal escapes, such as for changing the color of text in a
  terminal window.

- `fix_encoding`: True

  Detect mojibake and attempt to fix it by decoding the text in a different
  encoding standard.

  The following four options affect `fix_encoding` works, and do nothing if
  `fix_encoding` is False:

  - `restore_byte_a0`: True

    Allow a literal space (U+20) to be interpreted as a non-breaking space
    (U+A0) when that would make it part of a fixable mojibake string.

    Because spaces are very common characters, this could lead to false
    positives, but we try to apply it only when there's strong evidence for
    mojibake. Disabling `restore_byte_a0` is safer from false positives,
    but creates false negatives.

  - `replace_lossy_sequences`: True

    Detect mojibake that has been partially replaced by the characters
    '�' or '?'. If the mojibake could be decoded otherwise, replace the
    detected sequence with '�'.

  - `decode_inconsistent_utf8`: True

    When we see sequences that distinctly look like UTF-8 mojibake, but
    there's no consistent way to reinterpret the string in a new encoding,
    replace the mojibake with the appropriate UTF-8 characters anyway.

    This helps to decode strings that are concatenated from different
    encodings.

  - `fix_c1_controls`: True

    Replace C1 control characters (the useless characters U+80 - U+9B that
    come from Latin-1) with their Windows-1252 equivalents, like HTML5 does,
    even if the whole string doesn't decode as Latin-1.

- `fix_latin_ligatures`: True

  Replace common Latin-alphabet ligatures, such as ``ﬁ``, with the
  letters they're made of.

- `fix_character_width`: True

  Replace fullwidth Latin characters and halfwidth Katakana with
  their more standard widths.

- `uncurl_quotes`: True

  Replace curly quotes with straight quotes.

- `fix_line_breaks`: True

  Replace various forms of line breaks with the standard Unix line
  break, ``\n``.

- `fix_surrogates`: True

  Replace sequences of UTF-16 surrogate codepoints with the character
  they were meant to encode. This fixes text that was decoded with the
  obsolete UCS-2 standard, and allows it to support high-numbered
  codepoints such as emoji.

- `remove_control_chars`: True

  Remove certain control characters that have no displayed effect on text.

- `normalization`: ""NFC""

  Choose what kind of Unicode normalization is applied. Usually, we apply
  NFC normalization, so that letters followed by combining characters become
  single combined characters.

  Changing this to ""NFKC"" applies more compatibility conversions, such as
  replacing the 'micro sign' with a standard Greek lowercase mu, which looks
  identical. However, some NFKC normalizations change the meaning of text,
  such as converting ""10³"" to ""103"".

`normalization` can be None, to apply no normalization.

- `max_decode_length`: 1_000_000

  The maximum size of ""segment"" that ftfy will try to fix all at once.

- `explain`: True

  Whether to compute 'explanations', lists describing what ftfy changed.
  When this is False, the explanation will be None, and the code that
  builds the explanation will be skipped, possibly saving time.

  Functions that accept TextFixerConfig and don't return an explanation
  will automatically set `explain` to False.",0,['NamedTuple'],16,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_readers.py,ContentLengthReader,,3,[],0,"['__init__', '__call__', 'read_eof']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_readers.py,ChunkedReader,,3,[],0,"['__init__', '__call__', 'read_eof']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_readers.py,Http10Reader,,2,[],0,"['__call__', 'read_eof']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_receivebuffer.py,ReceiveBuffer,,10,[],0,"['__init__', '__iadd__', '__bool__', '__len__', '__bytes__', '_extract', 'maybe_extract_at_most', 'maybe_extract_next_line', 'maybe_extract_lines', 'is_next_line_obviously_invalid_request_line']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,CLIENT,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,SERVER,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,IDLE,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,SEND_RESPONSE,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,SEND_BODY,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,DONE,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,MUST_CLOSE,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,CLOSED,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,ERROR,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,MIGHT_SWITCH_PROTOCOL,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,SWITCHED_PROTOCOL,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,_SWITCH_UPGRADE,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,_SWITCH_CONNECT,,0,['Sentinel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_state.py,ConnectionState,,8,[],0,"['__init__', 'process_error', 'process_keep_alive_disabled', 'process_client_switch_proposal', 'process_event', '_fire_event_triggered_transitions', '_fire_state_triggered_transitions', 'start_next_cycle']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_util.py,ProtocolError,"Exception indicating a violation of the HTTP/1.1 protocol.

This as an abstract base class, with two concrete base classes:
:exc:`LocalProtocolError`, which indicates that you tried to do something
that HTTP/1.1 says is illegal, and :exc:`RemoteProtocolError`, which
indicates that the remote peer tried to do something that HTTP/1.1 says is
illegal. See :ref:`error-handling` for details.

In addition to the normal :exc:`Exception` features, it has one attribute:

.. attribute:: error_status_hint

   This gives a suggestion as to what status code a server might use if
   this error occurred as part of a request.

   For a :exc:`RemoteProtocolError`, this is useful as a suggestion for
   how you might want to respond to a misbehaving peer, if you're
   implementing a server.

   For a :exc:`LocalProtocolError`, this can be taken as a suggestion for
   how your peer might have responded to *you* if h11 had allowed you to
   continue.

   The default is 400 Bad Request, a generic catch-all for protocol
   violations.",1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_util.py,LocalProtocolError,,1,['ProtocolError'],0,['_reraise_as_remote_protocol_error'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_util.py,RemoteProtocolError,,0,['ProtocolError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_util.py,Sentinel,,2,['type'],0,"['__new__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_writers.py,BodyWriter,,3,[],0,"['__call__', 'send_data', 'send_eom']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_writers.py,ContentLengthWriter,,3,['BodyWriter'],0,"['__init__', 'send_data', 'send_eom']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_writers.py,ChunkedWriter,,2,['BodyWriter'],0,"['send_data', 'send_eom']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\_writers.py,Http10Writer,,2,['BodyWriter'],0,"['send_data', 'send_eom']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,ConnectionNotAvailable,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,ProxyError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,UnsupportedProtocol,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,ProtocolError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,RemoteProtocolError,,0,['ProtocolError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,LocalProtocolError,,0,['ProtocolError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,TimeoutException,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,PoolTimeout,,0,['TimeoutException'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,ConnectTimeout,,0,['TimeoutException'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,ReadTimeout,,0,['TimeoutException'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,WriteTimeout,,0,['TimeoutException'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,NetworkError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,ConnectError,,0,['NetworkError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,ReadError,,0,['NetworkError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_exceptions.py,WriteError,,0,['NetworkError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,AsyncLock,"This is a standard lock.

In the sync case `Lock` provides thread locking.
In the async case `AsyncLock` provides async locking.",2,[],0,"['__init__', 'setup']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,AsyncThreadLock,"This is a threading-only lock for no-I/O contexts.

In the sync case `ThreadLock` provides thread locking.
In the async case `AsyncThreadLock` is a no-op.",2,[],0,"['__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,AsyncEvent,,3,[],0,"['__init__', 'setup', 'set']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,AsyncSemaphore,,2,[],0,"['__init__', 'setup']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,AsyncShieldCancellation,,3,[],0,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,Lock,"This is a standard lock.

In the sync case `Lock` provides thread locking.
In the async case `AsyncLock` provides async locking.",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,ThreadLock,"This is a threading-only lock for no-I/O contexts.

In the sync case `ThreadLock` provides thread locking.
In the async case `AsyncThreadLock` is a no-op.",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,Event,,3,[],0,"['__init__', 'set', 'wait']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,Semaphore,,3,[],0,"['__init__', 'acquire', 'release']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_synchronization.py,ShieldCancellation,,2,[],0,"['__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_trace.py,Trace,,4,[],0,"['__init__', 'trace', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,ContentDecoder,,2,[],0,"['decode', 'flush']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,IdentityDecoder,Handle unencoded data.,2,['ContentDecoder'],0,"['decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,DeflateDecoder,"Handle 'deflate' decoding.

See: https://stackoverflow.com/questions/1838699",3,['ContentDecoder'],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,GZipDecoder,"Handle 'gzip' decoding.

See: https://stackoverflow.com/questions/1838699",3,['ContentDecoder'],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,BrotliDecoder,"Handle 'brotli' decoding.

Requires `pip install brotlipy`. See: https://brotlipy.readthedocs.io/
    or   `pip install brotli`. See https://github.com/google/brotli
Supports both 'brotlipy' and 'Brotli' packages since they share an import
name. The top branches are for 'brotlipy' and bottom branches for 'Brotli'",3,['ContentDecoder'],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,ZStandardDecoder,"Handle 'zstd' RFC 8878 decoding.

Requires `pip install zstandard`.
Can be installed as a dependency of httpx using `pip install httpx[zstd]`.",3,['ContentDecoder'],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,MultiDecoder,Handle the case where multiple encodings have been applied.,3,['ContentDecoder'],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,ByteChunker,Handles returning byte content in fixed-size chunks.,3,[],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,TextChunker,Handles returning text content in fixed-size chunks.,3,[],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,TextDecoder,Handles incrementally decoding bytes into text,3,[],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_decoders.py,LineDecoder,"Handles incrementally reading lines from text.

Has the same behaviour as the stdllib splitlines,
but handling the input iteratively.",3,[],0,"['__init__', 'decode', 'flush']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_types.py,SyncByteStream,,2,[],0,"['__iter__', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_types.py,AsyncByteStream,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx_sse\_decoders.py,SSEDecoder,,2,[],0,"['__init__', 'decode']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx_sse\_exceptions.py,SSEError,,0,['httpx.TransportError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\codec.py,Codec,,2,['codecs.Codec'],0,"['encode', 'decode']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\codec.py,IncrementalEncoder,,1,['codecs.BufferedIncrementalEncoder'],0,['_buffer_encode'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\codec.py,IncrementalDecoder,,1,['codecs.BufferedIncrementalDecoder'],0,['_buffer_decode'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\codec.py,StreamWriter,,0,"['Codec', 'codecs.StreamWriter']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\codec.py,StreamReader,,0,"['Codec', 'codecs.StreamReader']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\core.py,IDNAError,Base exception for all IDNA-encoding related problems,0,['UnicodeError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\core.py,IDNABidiError,Exception when bidirectional requirements are not satisfied,0,['IDNAError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\core.py,InvalidCodepoint,Exception when a disallowed or unallocated codepoint is used,0,['IDNAError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\idna\core.py,InvalidCodepointContext,Exception when the codepoint is not valid in the context it is used,0,['IDNAError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\bccache.py,Bucket,"Buckets are used to store the bytecode for one template.  It's created
and initialized by the bytecode cache and passed to the loading functions.

The buckets get an internal checksum from the cache assigned and use this
to automatically reject outdated cache material.  Individual bytecode
cache subclasses don't have to care about cache invalidation.",6,[],0,"['__init__', 'reset', 'load_bytecode', 'write_bytecode', 'bytecode_from_string', 'bytecode_to_string']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\bccache.py,BytecodeCache,"To implement your own bytecode cache you have to subclass this class
and override :meth:`load_bytecode` and :meth:`dump_bytecode`.  Both of
these methods are passed a :class:`~jinja2.bccache.Bucket`.

A very basic bytecode cache that saves the bytecode on the file system::

    from os import path

    class MyCache(BytecodeCache):

        def __init__(self, directory):
            self.directory = directory

        def load_bytecode(self, bucket):
            filename = path.join(self.directory, bucket.key)
            if path.exists(filename):
                with open(filename, 'rb') as f:
                    bucket.load_bytecode(f)

        def dump_bytecode(self, bucket):
            filename = path.join(self.directory, bucket.key)
            with open(filename, 'wb') as f:
                bucket.write_bytecode(f)

A more advanced version of a filesystem based bytecode cache is part of
Jinja.",7,[],0,"['load_bytecode', 'dump_bytecode', 'clear', 'get_cache_key', 'get_source_checksum', 'get_bucket', 'set_bucket']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\bccache.py,FileSystemBytecodeCache,"A bytecode cache that stores bytecode on the filesystem.  It accepts
two arguments: The directory where the cache items are stored and a
pattern string that is used to build the filename.

If no directory is specified a default cache directory is selected.  On
Windows the user's temp directory is used, on UNIX systems a directory
is created for the user in the system temp directory.

The pattern can be used to have multiple separate caches operate on the
same directory.  The default pattern is ``'__jinja2_%s.cache'``.  ``%s``
is replaced with the cache key.

>>> bcc = FileSystemBytecodeCache('/tmp/jinja_cache', '%s.cache')

This bytecode cache supports clearing of the cache using the clear method.",6,['BytecodeCache'],0,"['__init__', '_get_default_cache_dir', '_get_cache_filename', 'load_bytecode', 'dump_bytecode', 'clear']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\bccache.py,MemcachedBytecodeCache,"This class implements a bytecode cache that uses a memcache cache for
storing the information.  It does not enforce a specific memcache library
(tummy's memcache or cmemcache) but will accept any class that provides
the minimal interface required.

Libraries compatible with this class:

-   `cachelib <https://github.com/pallets/cachelib>`_
-   `python-memcached <https://pypi.org/project/python-memcached/>`_

(Unfortunately the django cache interface is not compatible because it
does not support storing binary data, only text. You can however pass
the underlying cache client to the bytecode cache which is available
as `django.core.cache.cache._client`.)

The minimal interface for the client passed to the constructor is this:

.. class:: MinimalClientInterface

    .. method:: set(key, value[, timeout])

        Stores the bytecode in the cache.  `value` is a string and
        `timeout` the timeout of the key.  If timeout is not provided
        a default timeout or no timeout should be assumed, if it's
        provided it's an integer with the number of seconds the cache
        item should exist.

    .. method:: get(key)

        Returns the value for the cache key.  If the item does not
        exist in the cache the return value must be `None`.

The other arguments to the constructor are the prefix for all keys that
is added before the actual cache key and the timeout for the bytecode in
the cache system.  We recommend a high (or no) timeout.

This bytecode cache does not support clearing of used items in the cache.
The clear method is a no-operation function.

.. versionadded:: 2.7
   Added support for ignoring memcache errors through the
   `ignore_memcache_errors` parameter.",3,['BytecodeCache'],0,"['__init__', 'load_bytecode', 'dump_bytecode']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\filters.py,_GroupTuple,,2,['t.NamedTuple'],2,"['__repr__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\meta.py,TrackingCodeGenerator,We abuse the code generator for introspection.,3,['CodeGenerator'],0,"['__init__', 'write', 'enter_frame']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\optimizer.py,Optimizer,,2,['NodeTransformer'],0,"['__init__', 'generic_visit']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\visitor.py,NodeVisitor,"Walks the abstract syntax tree and call visitor functions for every
node found.  The visitor functions may return values which will be
forwarded by the `visit` method.

Per default the visitor functions for the nodes are ``'visit_'`` +
class name of the node.  So a `TryFinally` node visit function would
be `visit_TryFinally`.  This behavior can be changed by overriding
the `get_visitor` function.  If no visitor function exists for a node
(return value `None`) the `generic_visit` visitor is used instead.",3,[],0,"['get_visitor', 'visit', 'generic_visit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\jinja2\visitor.py,NodeTransformer,"Walks the abstract syntax tree and allows modifications of nodes.

The `NodeTransformer` will walk the AST and use the return value of the
visitor functions to replace or remove the old node.  If the return
value of the visitor function is `None` the node will be removed
from the previous location otherwise it's replaced with the return
value.  The return value may be the original node in which case no
replacement takes place.",2,['NodeVisitor'],0,"['generic_visit', 'visit_list']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\chat_sessions.py,ChatSession,"Chat Session represents a single
conversation, channel, or other group of messages.",0,['TypedDict'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\html.py,ElementType,Element type as typed dict.,0,['TypedDict'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\html.py,HTMLHeaderTextSplitter,"Splitting HTML files based on specified headers.
Requires lxml package.",5,[],0,"['__init__', 'aggregate_elements_to_chunks', 'split_text_from_url', 'split_text', 'split_text_from_file']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\html.py,HTMLSectionSplitter,"Splitting HTML files based on specified tag and font sizes.
Requires lxml package.",7,[],0,"['__init__', 'split_documents', 'split_text', 'create_documents', 'split_html_by_headers', 'convert_possible_tags_to_header', 'split_text_from_file']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\konlpy.py,KonlpyTextSplitter,"Splitting text using Konlpy package.

It is good for splitting Korean text.",2,['TextSplitter'],0,"['__init__', 'split_text']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\latex.py,LatexTextSplitter,Attempts to split the text along Latex-formatted layout elements.,1,['RecursiveCharacterTextSplitter'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\nltk.py,NLTKTextSplitter,Splitting text using NLTK package.,2,['TextSplitter'],0,"['__init__', 'split_text']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\python.py,PythonCodeTextSplitter,Attempts to split the text along Python syntax.,1,['RecursiveCharacterTextSplitter'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\sentence_transformers.py,SentenceTransformersTokenTextSplitter,Splitting text to tokens using sentence model tokenizer.,5,['TextSplitter'],1,"['__init__', '_initialize_chunk_configuration', 'split_text', 'count_tokens', '_encode']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_text_splitters\spacy.py,SpacyTextSplitter,"Splitting text using Spacy package.


Per default, Spacy's `en_core_web_sm` model is used and
its default max_length is 1000000 (it is the length of maximum character
this model takes which can be increased for large files). For a faster, but
potentially less accurate splitting, you can use `pipeline='sentencizer'`.",2,['TextSplitter'],0,"['__init__', 'split_text']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\middleware.py,TracingMiddleware,"Middleware for propagating distributed tracing context using LangSmith.

This middleware checks for the 'langsmith-trace' header and propagates the
tracing context if present. It does not start new traces by default.
It is designed to work with ASGI applications.

Attributes:
    app: The ASGI application being wrapped.",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\run_helpers.py,LangSmithExtra,Any additional info to be injected into the run dynamically.,0,['TypedDict'],11,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\run_helpers.py,SupportsLangsmithExtra,"Implementations of this Protoc accept an optional langsmith_extra parameter.

Args:
    *args: Variable length arguments.
    langsmith_extra (Optional[LangSmithExtra): Optional dictionary of
        additional parameters for Langsmith.
    **kwargs: Keyword arguments.

Returns:
    R: The return type of the callable.",1,"['Protocol', 'Generic[P, R]']",0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\run_helpers.py,trace,"Manage a LangSmith run in context.

This class can be used as both a synchronous and asynchronous context manager.

Args:
    name (str): Name of the run.
    run_type (ls_client.RUN_TYPE_T, optional): Type of run (e.g., ""chain"", ""llm"", ""tool""). Defaults to ""chain"".
    inputs (Optional[Dict], optional): Initial input data for the run. Defaults to None.
    project_name (Optional[str], optional): Project name to associate the run with. Defaults to None.
    parent (Optional[Union[run_trees.RunTree, str, Mapping]], optional): Parent run. Can be a RunTree, dotted order string, or tracing headers. Defaults to None.
    tags (Optional[List[str]], optional): List of tags for the run. Defaults to None.
    metadata (Optional[Mapping[str, Any]], optional): Additional metadata for the run. Defaults to None.
    client (Optional[ls_client.Client], optional): LangSmith client for custom settings. Defaults to None.
    run_id (Optional[ls_client.ID_TYPE], optional): Preset identifier for the run. Defaults to None.
    reference_example_id (Optional[ls_client.ID_TYPE], optional): Associates run with a dataset example. Only for root runs in evaluation. Defaults to None.
    exceptions_to_handle (Optional[Tuple[Type[BaseException], ...]], optional): Exception types to ignore. Defaults to None.
    extra (Optional[Dict], optional): Extra data to send to LangSmith. Use 'metadata' instead. Defaults to None.

Examples:
    Synchronous usage:

    .. code-block:: python

        >>> with trace(""My Operation"", run_type=""tool"", tags=[""important""]) as run:
        ...     result = ""foo""  # Perform operation
        ...     run.metadata[""some-key""] = ""some-value""
        ...     run.end(outputs={""result"": result})

    Asynchronous usage:

    .. code-block:: python

        >>> async def main():
        ...     async with trace(""Async Operation"", run_type=""tool"", tags=[""async""]) as run:
        ...         result = ""foo""  # Await async operation
        ...         run.metadata[""some-key""] = ""some-value""
        ...         # ""end"" just adds the outputs and sets error to None
        ...         # The actual patching of the run happens when the context exits
        ...         run.end(outputs={""result"": result})
        >>> asyncio.run(main())

    Handling specific exceptions:

    .. code-block:: python

        >>> import pytest
        >>> import sys
        >>> with trace(""Test"", exceptions_to_handle=(pytest.skip.Exception,)):
        ...     if sys.platform == ""win32"": # Just an example
        ...         pytest.skip(""Skipping test for windows"")
        ...     result = ""foo""  # Perform test operation",5,[],0,"['__init__', '_setup', '_teardown', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\run_helpers.py,_TraceableContainer,Typed response when initializing a run a traceable.,0,['TypedDict'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\run_helpers.py,_ContainerInput,Typed response when initializing a run a traceable.,0,['TypedDict'],10,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\run_helpers.py,_TracedStreamBase,Base class for traced stream objects.,7,['Generic[T]'],0,"['__init__', '__getattr__', '__dir__', '__repr__', '__str__', '__del__', '_end_trace']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\run_helpers.py,_TracedStream,A wrapper for synchronous stream objects that handles tracing.,5,"['_TracedStreamBase', 'Generic[T]']",0,"['__init__', '__next__', '__iter__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\run_helpers.py,_TracedAsyncStream,A wrapper for asynchronous stream objects that handles tracing.,1,"['_TracedStreamBase', 'Generic[T]']",0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithError,An error occurred while communicating with the LangSmith API.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithAPIError,Internal server error while communicating with LangSmith.,0,['LangSmithError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithRequestTimeout,Client took too long to send request body.,0,['LangSmithError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithUserError,User error caused an exception when communicating with LangSmith.,0,['LangSmithError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithRateLimitError,You have exceeded the rate limit for the LangSmith API.,0,['LangSmithError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithAuthError,Couldn't authenticate with the LangSmith API.,0,['LangSmithError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithNotFoundError,Couldn't find the requested resource.,0,['LangSmithError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithConflictError,The resource already exists.,0,['LangSmithError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithConnectionError,Couldn't connect to the LangSmith API.,0,['LangSmithError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithWarning,Base class for warnings.,0,['UserWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithMissingAPIKeyWarning,Warning for missing API key.,0,['LangSmithWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,FilterPoolFullWarning,Filter urrllib3 warnings logged when the connection pool isn't reused.,2,['logging.Filter'],0,"['__init__', 'filter']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,FilterLangSmithRetry,Filter for retries from this lib.,1,['logging.Filter'],0,['filter'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,LangSmithRetry,Wrapper to filter logs with this name.,0,['Retry'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\utils.py,ContextThreadPoolExecutor,ThreadPoolExecutor that copies the context to the child thread.,2,['ThreadPoolExecutor'],0,"['submit', 'map']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_batched_visitor.py,BatchableCSTVisitor,"The low-level base visitor class for traversing a CST as part of a batched
set of traversals. This should be used in conjunction with the
:func:`~libcst.visit_batched` function or the
:func:`~libcst.MetadataWrapper.visit_batched` method from
:class:`~libcst.MetadataWrapper` to visit a tree.
Instances of this class cannot modify the tree.",1,"['CSTTypedVisitorFunctions', 'MetadataDependent']",0,['get_visitors'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_batched_visitor.py,_BatchedCSTVisitor,Internal visitor class to perform batched traversal over a tree.,5,['CSTVisitor'],3,"['__init__', 'on_visit', 'on_leave', 'on_visit_attribute', 'on_leave_attribute']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_flatten_sentinel.py,FlattenSentinel,"A :class:`FlattenSentinel` may be returned by a :meth:`CSTTransformer.on_leave`
method when one wants to replace a node with multiple nodes. The replaced
node must be contained in a `Sequence` attribute such as
:attr:`~libcst.Module.body`.  This is generally the case for
:class:`~libcst.BaseStatement` and :class:`~libcst.BaseSmallStatement`.
For example to insert a print before every return::

    def leave_Return(
        self, original_node: cst.Return, updated_node: cst.Return
    ) -> Union[cst.Return, cst.RemovalSentinel, cst.FlattenSentinel[cst.BaseSmallStatement]]:
        log_stmt = cst.Expr(cst.parse_expression(""print('returning')""))
        return cst.FlattenSentinel([log_stmt, updated_node])

Returning an empty :class:`FlattenSentinel` is equivalent to returning
:attr:`cst.RemovalSentinel.REMOVE` and is subject to its requirements.",3,['Sequence[CSTNodeT_co]'],1,"['__init__', '__getitem__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_visitors.py,CSTTransformer,"The low-level base visitor class for traversing a CST and creating an
updated copy of the original CST. This should be used in conjunction with
the :func:`~libcst.CSTNode.visit` method on a :class:`~libcst.CSTNode` to
visit each element in a tree starting with that node, and possibly returning
a new node in its place.

When visiting nodes using a :class:`CSTTransformer`, the return value of
:func:`~libcst.CSTNode.visit` will be a new tree with any changes made in
:func:`~libcst.CSTTransformer.on_leave` calls reflected in its children.",4,"['CSTTypedTransformerFunctions', 'MetadataDependent']",0,"['on_visit', 'on_leave', 'on_visit_attribute', 'on_leave_attribute']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_visitors.py,CSTVisitor,"The low-level base visitor class for traversing a CST. This should be used in
conjunction with the :func:`~libcst.CSTNode.visit` method on a
:class:`~libcst.CSTNode` to visit each element in a tree starting with that
node. Unlike :class:`CSTTransformer`, instances of this class cannot modify
the tree.

When visiting nodes using a :class:`CSTVisitor`, the return value of
:func:`~libcst.CSTNode.visit` will equal the passed in tree.",4,"['CSTTypedVisitorFunctions', 'MetadataDependent']",0,"['on_visit', 'on_leave', 'on_visit_attribute', 'on_leave_attribute']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\markdown_it\parser_block.py,ParserBlock,"ParserBlock#ruler -> Ruler

[[Ruler]] instance. Keep configuration of block rules.",3,[],0,"['__init__', 'tokenize', 'parse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\markdown_it\parser_core.py,ParserCore,,2,[],0,"['__init__', 'process']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\markdown_it\parser_inline.py,ParserInline,,4,[],0,"['__init__', 'skipToken', 'tokenize', 'parse']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\marshmallow\decorators.py,MarshmallowHook,,0,[],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\marshmallow\error_store.py,ErrorStore,,2,[],0,"['__init__', 'store_error']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\marshmallow\orderedset.py,OrderedSet,,10,['MutableSet'],0,"['__init__', '__len__', '__contains__', 'add', 'discard', '__iter__', '__reversed__', 'pop', '__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\marshmallow\utils.py,_Missing,,4,[],0,"['__bool__', '__copy__', '__deepcopy__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\marshmallow\warnings.py,RemovedInMarshmallow4Warning,,0,['DeprecationWarning'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mdurl\_parse.py,MutableURL,,3,[],0,"['__init__', 'parse', 'parse_host']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mdurl\_url.py,URL,,0,['NamedTuple'],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\identification.py,IdentificationMethods,,0,['object'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\applytype.py,PolyTranslationError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\applytype.py,PolyTranslator,"Make free type variables generic in the type if possible.

See docstring for apply_poly() for details.",8,['TypeTranslator'],0,"['__init__', 'collect_vars', 'visit_callable_type', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_type_alias_type', 'visit_instance']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\argmap.py,ArgTypeExpander,"Utility class for mapping actual argument types to formal arguments.

One of the main responsibilities is to expand caller tuple *args and TypedDict
**kwargs, and to keep track of which tuple/TypedDict items have already been
consumed.

Example:

   def f(x: int, *args: str) -> None: ...
   f(*(1, 'x', 1.1))

We'd call expand_actual_type three times:

  1. The first call would provide 'int' as the actual type of 'x' (from '1').
  2. The second call would provide 'str' as one of the actual types for '*args'.
  2. The third call would provide 'float' as one of the actual types for '*args'.

A single instance can process all the arguments for a single call. Each call
needs a separate instance since instances have per-call state.",2,[],0,"['__init__', 'expand_actual_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\checkmember.py,MemberContext,"Information and objects needed to type check attribute access.

Look at the docstring of analyze_member_access for more information.",4,[],0,"['__init__', 'named_type', 'not_ready_callback', 'copy_modified']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\checkmember.py,FreezeTypeVarsVisitor,,1,['TypeTraverserVisitor'],0,['visit_callable_type'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\checkpattern.py,PatternType,,0,['NamedTuple'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\checkpattern.py,PatternChecker,"Pattern checker.

This class checks if a pattern can match a type, what the type can be narrowed to, and what
type capture patterns should be inferred as.",21,['PatternVisitor[PatternType]'],9,"['__init__', 'accept', 'visit_as_pattern', 'visit_or_pattern', 'visit_value_pattern', 'visit_singleton_pattern', 'visit_sequence_pattern', 'get_sequence_type', 'contract_starred_pattern_types', 'expand_starred_pattern_types', 'visit_starred_pattern', 'visit_mapping_pattern', 'get_mapping_item_type', 'get_simple_mapping_item_type', 'visit_class_pattern', 'should_self_match', 'can_match_sequence', 'generate_types_from_names', 'update_type_map', 'construct_sequence_child', 'early_non_match']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\checkstrformat.py,ConversionSpecifier,,3,[],0,"['__init__', 'has_key', 'has_star']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\checkstrformat.py,StringFormatterChecker,"String interpolation/formatter type checker.

This class works closely together with checker.ExpressionChecker.",25,[],3,"['__init__', 'check_str_format_call', 'check_specs_in_format_call', 'perform_special_format_checks', 'find_replacements_in_call', 'get_expr_by_position', 'get_expr_by_name', 'auto_generate_keys', 'apply_field_accessors', 'validate_and_transform_accessors', 'check_str_interpolation', 'analyze_conversion_specifiers', 'check_simple_str_interpolation', 'check_mapping_str_interpolation', 'build_dict_type', 'build_replacement_checkers', 'replacement_checkers', 'checkers_for_star', 'check_placeholder_type', 'checkers_for_regular_type', 'check_s_special_cases', 'checkers_for_c_type', 'conversion_type', 'named_type', 'accept']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\config_parser.py,ConfigTOMLValueError,,0,['ValueError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\copytype.py,TypeShallowCopier,,22,['TypeVisitor[ProperType]'],0,"['visit_unbound_type', 'visit_any', 'visit_none_type', 'visit_uninhabited_type', 'visit_erased_type', 'visit_deleted_type', 'visit_instance', 'visit_type_var', 'visit_param_spec', 'visit_parameters', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_partial_type', 'visit_callable_type', 'visit_tuple_type', 'visit_typeddict_type', 'visit_literal_type', 'visit_union_type', 'visit_overloaded', 'visit_type_type', 'visit_type_alias_type', 'copy_common']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\dmypy_server.py,Server,,28,[],0,"['__init__', '_response_metadata', 'serve', 'run_command', 'cmd_status', 'cmd_stop', 'cmd_run', 'cmd_check', 'cmd_recheck', 'check', 'flush_caches', 'update_stats', 'following_imports', 'initialize_fine_grained', 'fine_grained_increment', 'fine_grained_increment_follow_imports', 'find_reachable_changed_modules', 'direct_imports', 'find_added_suppressed', 'increment_output', 'pretty_messages', 'update_sources', 'update_changed', 'find_changed', '_find_changed', 'cmd_inspect', 'cmd_suggest', 'cmd_hang']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\dmypy_util.py,WriteToConn,Helper class to write to a connection instead of standard output.,20,['TextIO'],0,"['__init__', '__enter__', '__exit__', '__iter__', '__next__', 'close', 'fileno', 'flush', 'isatty', 'read', 'readable', 'readline', 'readlines', 'seek', 'seekable', 'tell', 'truncate', 'write', 'writable', 'writelines']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\erasetype.py,EraseTypeVisitor,,21,['TypeVisitor[ProperType]'],0,"['visit_unbound_type', 'visit_any', 'visit_none_type', 'visit_uninhabited_type', 'visit_erased_type', 'visit_partial_type', 'visit_deleted_type', 'visit_instance', 'visit_type_var', 'visit_param_spec', 'visit_parameters', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_callable_type', 'visit_overloaded', 'visit_tuple_type', 'visit_typeddict_type', 'visit_literal_type', 'visit_union_type', 'visit_type_type', 'visit_type_alias_type']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\erasetype.py,TypeVarEraser,Implementation of type erasure,7,['TypeTranslator'],0,"['__init__', 'visit_type_var', 'visit_instance', 'visit_tuple_type', 'visit_type_var_tuple', 'visit_param_spec', 'visit_type_alias_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\erasetype.py,LastKnownValueEraser,"Removes the Literal[...] type that may be associated with any
Instance types.",3,['TypeTranslator'],0,"['visit_instance', 'visit_type_alias_type', 'visit_union_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\errorcodes.py,ErrorCode,,4,[],0,"['__init__', '__str__', '__eq__', '__hash__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\evalexpr.py,_NodeEvaluator,,44,['ExpressionVisitor[object]'],0,"['visit_int_expr', 'visit_str_expr', 'visit_bytes_expr', 'visit_float_expr', 'visit_complex_expr', 'visit_ellipsis', 'visit_star_expr', 'visit_name_expr', 'visit_member_expr', 'visit_yield_from_expr', 'visit_yield_expr', 'visit_call_expr', 'visit_op_expr', 'visit_comparison_expr', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_reveal_expr', 'visit_super_expr', 'visit_unary_expr', 'visit_assignment_expr', 'visit_list_expr', 'visit_dict_expr', 'visit_tuple_expr', 'visit_set_expr', 'visit_index_expr', 'visit_type_application', 'visit_lambda_expr', 'visit_list_comprehension', 'visit_set_comprehension', 'visit_dictionary_comprehension', 'visit_generator_expr', 'visit_slice_expr', 'visit_conditional_expr', 'visit_type_var_expr', 'visit_paramspec_expr', 'visit_type_var_tuple_expr', 'visit_type_alias_expr', 'visit_namedtuple_expr', 'visit_enum_call_expr', 'visit_typeddict_expr', 'visit_newtype_expr', 'visit__promote_expr', 'visit_await_expr', 'visit_temp_node']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\expandtype.py,HasGenericCallable,,2,['BoolTypeQuery'],0,"['__init__', 'visit_callable_type']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\expandtype.py,FreshenCallableVisitor,,2,['mypy.type_visitor.TypeTranslator'],0,"['visit_callable_type', 'visit_type_alias_type']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\expandtype.py,ExpandTypeVisitor,Visitor that substitutes type variables with values.,26,['TrivialSyntheticTypeTranslator'],1,"['__init__', 'visit_unbound_type', 'visit_any', 'visit_none_type', 'visit_uninhabited_type', 'visit_deleted_type', 'visit_erased_type', 'visit_instance', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_unpack_type', 'expand_unpack', 'visit_parameters', 'interpolate_args_for_unpack', 'visit_callable_type', 'visit_overloaded', 'expand_types_with_unpack', 'visit_tuple_type', 'visit_typeddict_type', 'visit_literal_type', 'visit_union_type', 'visit_partial_type', 'visit_type_type', 'visit_type_alias_type', 'expand_types']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\exprtotype.py,TypeTranslationError,Exception raised when an expression is not valid as a type.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\fixup.py,NodeFixer,,12,['NodeVisitor[None]'],1,"['__init__', 'visit_type_info', 'visit_symbol_table', 'visit_func_def', 'visit_overloaded_func_def', 'visit_decorator', 'visit_class_def', 'visit_type_var_expr', 'visit_paramspec_expr', 'visit_type_var_tuple_expr', 'visit_var', 'visit_type_alias']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\fixup.py,TypeFixer,,22,['TypeVisitor[None]'],0,"['__init__', 'visit_instance', 'visit_type_alias_type', 'visit_any', 'visit_callable_type', 'visit_overloaded', 'visit_erased_type', 'visit_deleted_type', 'visit_none_type', 'visit_uninhabited_type', 'visit_partial_type', 'visit_tuple_type', 'visit_typeddict_type', 'visit_literal_type', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_parameters', 'visit_unbound_type', 'visit_union_type', 'visit_type_type']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\freetree.py,TreeFreer,,1,['TraverserVisitor'],0,['visit_block'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\fscache.py,FileSystemCache,,15,[],0,"['__init__', 'set_package_root', 'flush', 'stat_or_none', 'init_under_package_root', '_fake_init', 'listdir', 'isfile', 'isfile_case', 'exists_case', 'isdir', 'exists', 'read', 'hash_digest', 'samefile']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\fswatcher.py,FileData,,0,['NamedTuple'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\fswatcher.py,FileSystemWatcher,"Watcher for file system changes among specific paths.

All file system access is performed using FileSystemCache. We
detect changed files by stat()ing them all and comparing hashes
of potentially changed files. If a file has both size and mtime
unmodified, the file is assumed to be unchanged.

An important goal of this class is to make it easier to eventually
use file system events to detect file changes.

Note: This class doesn't flush the file system cache. If you don't
manually flush it, changes won't be seen.",9,[],0,"['__init__', 'dump_file_data', 'set_file_data', 'add_watched_paths', 'remove_watched_paths', '_update', '_find_changed', 'find_changed', 'update_changed']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\gclogger.py,GcLogger,Context manager to log GC stats and overall time.,4,[],0,"['__enter__', 'gc_callback', '__exit__', 'get_stats']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\indirection.py,TypeIndirectionVisitor,Returns all module references within a particular type.,24,['TypeVisitor[Set[str]]'],0,"['__init__', 'find_modules', '_visit', 'visit_unbound_type', 'visit_any', 'visit_none_type', 'visit_uninhabited_type', 'visit_erased_type', 'visit_deleted_type', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_parameters', 'visit_instance', 'visit_callable_type', 'visit_overloaded', 'visit_tuple_type', 'visit_typeddict_type', 'visit_literal_type', 'visit_union_type', 'visit_partial_type', 'visit_type_type', 'visit_type_alias_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\infer.py,ArgumentInferContext,"Type argument inference context.

We need this because we pass around ``Mapping`` and ``Iterable`` types.
These types are only known by ``TypeChecker`` itself.
It is required for ``*`` and ``**`` argument inference.

https://github.com/python/mypy/issues/11144",0,['NamedTuple'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\inspections.py,SearchVisitor,Visitor looking for an expression whose span matches given one exactly.,2,['ExtendedTraverserVisitor'],0,"['__init__', 'visit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\inspections.py,SearchAllVisitor,Visitor looking for all expressions whose spans enclose given position.,2,['ExtendedTraverserVisitor'],0,"['__init__', 'visit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\inspections.py,InspectionEngine,Engine for locating and statically inspecting expressions.,21,[],0,"['__init__', 'reload_module', 'expr_type', 'object_type', 'collect_attrs', '_fill_from_dict', 'expr_attrs', 'format_node', 'collect_nodes', 'modules_for_nodes', 'expression_def', 'missing_type', 'missing_node', 'add_prefixes', 'run_inspection_by_exact_location', 'run_inspection_by_position', 'find_module', 'run_inspection', 'get_type', 'get_attrs', 'get_definition']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\join.py,InstanceJoiner,,3,[],0,"['__init__', 'join_instances', 'join_instances_via_supertype']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\join.py,TypeJoinVisitor,"Implementation of the least upper bound algorithm.

Attributes:
  s: The other (left) type operand.",24,['TypeVisitor[ProperType]'],0,"['__init__', 'visit_unbound_type', 'visit_union_type', 'visit_any', 'visit_none_type', 'visit_uninhabited_type', 'visit_deleted_type', 'visit_erased_type', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_parameters', 'visit_instance', 'visit_callable_type', 'visit_overloaded', 'join_tuples', 'visit_tuple_type', 'visit_typeddict_type', 'visit_literal_type', 'visit_partial_type', 'visit_type_type', 'visit_type_alias_type', 'default']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\literals.py,_Hasher,,45,['ExpressionVisitor[Optional[Key]]'],0,"['visit_int_expr', 'visit_str_expr', 'visit_bytes_expr', 'visit_float_expr', 'visit_complex_expr', 'visit_star_expr', 'visit_name_expr', 'visit_member_expr', 'visit_op_expr', 'visit_comparison_expr', 'visit_unary_expr', 'seq_expr', 'visit_list_expr', 'visit_dict_expr', 'visit_tuple_expr', 'visit_set_expr', 'visit_index_expr', 'visit_assignment_expr', 'visit_call_expr', 'visit_slice_expr', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_conditional_expr', 'visit_ellipsis', 'visit_yield_from_expr', 'visit_yield_expr', 'visit_reveal_expr', 'visit_super_expr', 'visit_type_application', 'visit_lambda_expr', 'visit_list_comprehension', 'visit_set_comprehension', 'visit_dictionary_comprehension', 'visit_generator_expr', 'visit_type_var_expr', 'visit_paramspec_expr', 'visit_type_var_tuple_expr', 'visit_type_alias_expr', 'visit_namedtuple_expr', 'visit_enum_call_expr', 'visit_typeddict_expr', 'visit_newtype_expr', 'visit__promote_expr', 'visit_await_expr', 'visit_temp_node']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\main.py,AugmentedHelpFormatter,,2,['argparse.RawDescriptionHelpFormatter'],0,"['__init__', '_fill_text']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\main.py,PythonExecutableInferenceError,Represents a failure to infer the version or executable while searching.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\main.py,CapturableArgumentParser,"Override ArgumentParser methods that use sys.stdout/sys.stderr directly.

This is needed because hijacking sys.std* is not thread-safe,
yet output must be captured to properly support mypy.api.run.",6,['argparse.ArgumentParser'],0,"['__init__', 'print_usage', 'print_help', '_print_message', 'exit', 'error']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\main.py,CapturableVersionAction,"Supplement CapturableArgumentParser to handle --version.

This is nearly identical to argparse._VersionAction except,
like CapturableArgumentParser, it allows output to be captured.

Another notable difference is that version is mandatory.
This allows removing a line in __call__ that falls back to parser.version
(which does not appear to exist).",2,['argparse.Action'],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\meet.py,TypeMeetVisitor,,25,['TypeVisitor[ProperType]'],0,"['__init__', 'visit_unbound_type', 'visit_any', 'visit_union_type', 'visit_none_type', 'visit_uninhabited_type', 'visit_deleted_type', 'visit_erased_type', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_parameters', 'visit_instance', 'visit_callable_type', 'visit_overloaded', 'meet_tuples', 'visit_tuple_type', 'visit_typeddict_type', 'visit_literal_type', 'visit_partial_type', 'visit_type_type', 'visit_type_alias_type', 'meet', 'default']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\message_registry.py,ErrorMessage,,2,['NamedTuple'],2,"['format', 'with_additional_msg']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\mixedtraverser.py,MixedTraverserVisitor,Recursive traversal of both Node and Type objects.,17,"['TraverserVisitor', 'TypeTraverserVisitor']",0,"['__init__', 'visit_var', 'visit_func', 'visit_class_def', 'visit_type_alias_expr', 'visit_type_var_expr', 'visit_typeddict_expr', 'visit_namedtuple_expr', 'visit__promote_expr', 'visit_newtype_expr', 'visit_assignment_stmt', 'visit_for_stmt', 'visit_with_stmt', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_type_application', 'visit_optional_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\moduleinspect.py,ModuleProperties,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\moduleinspect.py,InspectError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\moduleinspect.py,ModuleInspect,"Perform runtime introspection of modules in a separate process.

Reuse the process for multiple modules for efficiency. However, if there is an
error, retry using a fresh process to avoid cross-contamination of state between
modules.

We use a separate process to isolate us from many side effects. For example, the
import of a module may kill the current process, and we want to recover from that.

Always use in a with statement for proper clean-up:

  with ModuleInspect() as m:
      p = m.get_package_properties('urllib.parse')",7,[],0,"['__init__', '_start', 'close', 'get_package_properties', '_get_from_queue', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\mro.py,MroError,Raised if a consistent mro cannot be determined for a class.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\reachability.py,MarkImportsUnreachableVisitor,Visitor that flags all imports nested within a node as unreachable.,3,['TraverserVisitor'],0,"['visit_import', 'visit_import_from', 'visit_import_all']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\reachability.py,MarkImportsMypyOnlyVisitor,Visitor that sets is_mypy_only (which affects priority).,4,['TraverserVisitor'],0,"['visit_import', 'visit_import_from', 'visit_import_all', 'visit_func_def']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\refinfo.py,RefInfoVisitor,,5,['TraverserVisitor'],0,"['__init__', 'visit_name_expr', 'visit_member_expr', 'visit_func_def', 'record_ref_expr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\semanal_enum.py,EnumCallAnalyzer,,7,[],0,"['__init__', 'process_enum_call', 'check_enum_call', 'build_enum_call_typeinfo', 'parse_enum_call_args', 'fail_enum_call_arg', 'fail']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\semanal_newtype.py,NewTypeAnalyzer,,7,[],0,"['__init__', 'process_newtype_declaration', 'analyze_newtype_declaration', 'check_newtype_args', 'build_newtype_typeinfo', 'make_argument', 'fail']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\semanal_pass1.py,SemanticAnalyzerPreAnalysis,"Analyze reachability of blocks and imports and other local things.

This runs before semantic analysis, so names have not been bound. Imports are
also not resolved yet, so we can only access the current module.

This determines static reachability of blocks and imports due to version and
platform checks, among others.

The main entry point is 'visit_file'.

Reachability of imports needs to be determined very early in the build since
this affects which modules will ultimately be processed.

Consider this example:

  import sys

  def do_stuff() -> None:
      if sys.version_info >= (3, 10):
          import xyz  # Only available in Python 3.10+
          xyz.whatever()
      ...

The block containing 'import xyz' is unreachable in Python 3 mode. The import
shouldn't be processed in Python 3 mode, even if the module happens to exist.",13,['TraverserVisitor'],0,"['visit_file', 'visit_func_def', 'visit_class_def', 'visit_import_from', 'visit_import_all', 'visit_import', 'visit_if_stmt', 'visit_block', 'visit_match_stmt', 'visit_assignment_stmt', 'visit_expression_stmt', 'visit_return_stmt', 'visit_for_stmt']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\semanal_typeargs.py,TypeArgumentAnalyzer,,14,['MixedTraverserVisitor'],0,"['__init__', 'visit_mypy_file', 'visit_func', 'visit_class_def', 'visit_block', 'visit_type_alias_type', 'visit_tuple_type', 'visit_callable_type', 'visit_instance', 'validate_args', 'visit_unpack_type', 'check_type_var_values', 'fail', 'note']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\semanal_typeddict.py,TypedDictAnalyzer,,15,[],0,"['__init__', 'analyze_typeddict_classdef', 'add_keys_and_types_from_base', 'analyze_base_args', 'map_items_to_base', 'analyze_typeddict_classdef_fields', 'extract_meta_info', 'check_typeddict', 'parse_typeddict_args', 'parse_typeddict_fields_with_types', 'fail_typeddict_arg', 'build_typeddict_typeinfo', 'is_typeddict', 'fail', 'note']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\split_namespace.py,SplitNamespace,,4,['argparse.Namespace'],0,"['__init__', '_get', '__setattr__', '__getattr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\stubdoc.py,ArgSig,Signature info for a single argument.,5,[],0,"['__init__', 'is_star_arg', 'is_star_kwarg', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\stubdoc.py,FunctionSig,,4,['NamedTuple'],3,"['is_special_method', 'has_catchall_args', 'is_catchall_signature', 'format_sig']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\stubdoc.py,DocStringParser,Parse function signatures in documentation.,4,[],0,"['__init__', 'add_token', 'reset', 'get_signatures']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,TraverserVisitor,"A parse tree visitor that traverses the parse tree during visiting.

It does not perform any actions outside the traversal. Subclasses
should override visit methods to perform actions during
traversal. Calling the superclass method allows reusing the
traversal implementation.",58,['NodeVisitor[None]'],0,"['__init__', 'visit_mypy_file', 'visit_block', 'visit_func', 'visit_func_def', 'visit_overloaded_func_def', 'visit_class_def', 'visit_decorator', 'visit_expression_stmt', 'visit_assignment_stmt', 'visit_operator_assignment_stmt', 'visit_while_stmt', 'visit_for_stmt', 'visit_return_stmt', 'visit_assert_stmt', 'visit_del_stmt', 'visit_if_stmt', 'visit_raise_stmt', 'visit_try_stmt', 'visit_with_stmt', 'visit_match_stmt', 'visit_type_alias_stmt', 'visit_member_expr', 'visit_yield_from_expr', 'visit_yield_expr', 'visit_call_expr', 'visit_op_expr', 'visit_comparison_expr', 'visit_slice_expr', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_reveal_expr', 'visit_assignment_expr', 'visit_unary_expr', 'visit_list_expr', 'visit_tuple_expr', 'visit_dict_expr', 'visit_set_expr', 'visit_index_expr', 'visit_generator_expr', 'visit_dictionary_comprehension', 'visit_list_comprehension', 'visit_set_comprehension', 'visit_conditional_expr', 'visit_type_application', 'visit_lambda_expr', 'visit_star_expr', 'visit_await_expr', 'visit_super_expr', 'visit_as_pattern', 'visit_or_pattern', 'visit_value_pattern', 'visit_sequence_pattern', 'visit_starred_pattern', 'visit_mapping_pattern', 'visit_class_pattern', 'visit_import', 'visit_import_from']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,ExtendedTraverserVisitor,"This is a more flexible traverser.

In addition to the base traverser it:
    * has visit_ methods for leaf nodes
    * has common method that is called for all nodes
    * allows to skip recursing into a node

Note that this traverser still doesn't visit some internal
mypy constructs like _promote expression and Var.",79,['TraverserVisitor'],0,"['visit', 'visit_mypy_file', 'visit_import', 'visit_import_from', 'visit_import_all', 'visit_func_def', 'visit_overloaded_func_def', 'visit_class_def', 'visit_global_decl', 'visit_nonlocal_decl', 'visit_decorator', 'visit_type_alias', 'visit_block', 'visit_expression_stmt', 'visit_assignment_stmt', 'visit_operator_assignment_stmt', 'visit_while_stmt', 'visit_for_stmt', 'visit_return_stmt', 'visit_assert_stmt', 'visit_del_stmt', 'visit_if_stmt', 'visit_break_stmt', 'visit_continue_stmt', 'visit_pass_stmt', 'visit_raise_stmt', 'visit_try_stmt', 'visit_with_stmt', 'visit_match_stmt', 'visit_int_expr', 'visit_str_expr', 'visit_bytes_expr', 'visit_float_expr', 'visit_complex_expr', 'visit_ellipsis', 'visit_star_expr', 'visit_name_expr', 'visit_member_expr', 'visit_yield_from_expr', 'visit_yield_expr', 'visit_call_expr', 'visit_op_expr', 'visit_comparison_expr', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_reveal_expr', 'visit_super_expr', 'visit_assignment_expr', 'visit_unary_expr', 'visit_list_expr', 'visit_dict_expr', 'visit_tuple_expr', 'visit_set_expr', 'visit_index_expr', 'visit_type_application', 'visit_lambda_expr', 'visit_list_comprehension', 'visit_set_comprehension', 'visit_dictionary_comprehension', 'visit_generator_expr', 'visit_slice_expr', 'visit_conditional_expr', 'visit_type_var_expr', 'visit_paramspec_expr', 'visit_type_var_tuple_expr', 'visit_type_alias_expr', 'visit_namedtuple_expr', 'visit_enum_call_expr', 'visit_typeddict_expr', 'visit_newtype_expr', 'visit_await_expr', 'visit_as_pattern', 'visit_or_pattern', 'visit_value_pattern', 'visit_singleton_pattern', 'visit_sequence_pattern', 'visit_starred_pattern', 'visit_mapping_pattern', 'visit_class_pattern']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,ReturnSeeker,,2,['TraverserVisitor'],0,"['__init__', 'visit_return_stmt']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,FuncCollectorBase,,2,['TraverserVisitor'],0,"['__init__', 'visit_func_def']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,YieldSeeker,,2,['FuncCollectorBase'],0,"['__init__', 'visit_yield_expr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,YieldFromSeeker,,2,['FuncCollectorBase'],0,"['__init__', 'visit_yield_from_expr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,AwaitSeeker,,2,['TraverserVisitor'],0,"['__init__', 'visit_await_expr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,ReturnCollector,,2,['FuncCollectorBase'],0,"['__init__', 'visit_return_stmt']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,YieldCollector,,3,['FuncCollectorBase'],0,"['__init__', 'visit_assignment_stmt', 'visit_yield_expr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\traverser.py,YieldFromCollector,,3,['FuncCollectorBase'],0,"['__init__', 'visit_assignment_stmt', 'visit_yield_from_expr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\treetransform.py,TransformVisitor,"Transform a semantically analyzed AST (or subtree) to an identical copy.

Use the node() method to transform an AST node.

Subclass to perform a non-identity transform.

Notes:

 * This can only be used to transform functions or classes, not top-level
   statements, and/or modules as a whole.
 * Do not duplicate TypeInfo nodes. This would generally not be desirable.
 * Only update some name binding cross-references, but only those that
   refer to Var, Decorator or FuncDef nodes, not those targeting ClassDef or
   TypeInfo nodes.
 * Types are not transformed, but you can override type() to also perform
   type transformation.

TODO nested classes and functions have not been tested well enough",104,['NodeVisitor[Node]'],0,"['__init__', 'visit_mypy_file', 'visit_import', 'visit_import_from', 'visit_import_all', 'copy_argument', 'visit_func_def', 'visit_lambda_expr', 'copy_function_attributes', 'visit_overloaded_func_def', 'visit_class_def', 'visit_global_decl', 'visit_nonlocal_decl', 'visit_block', 'visit_decorator', 'visit_var', 'visit_expression_stmt', 'visit_assignment_stmt', 'duplicate_assignment', 'visit_operator_assignment_stmt', 'visit_while_stmt', 'visit_for_stmt', 'visit_return_stmt', 'visit_assert_stmt', 'visit_del_stmt', 'visit_if_stmt', 'visit_break_stmt', 'visit_continue_stmt', 'visit_pass_stmt', 'visit_raise_stmt', 'visit_try_stmt', 'visit_with_stmt', 'visit_as_pattern', 'visit_or_pattern', 'visit_value_pattern', 'visit_singleton_pattern', 'visit_sequence_pattern', 'visit_starred_pattern', 'visit_mapping_pattern', 'visit_class_pattern', 'visit_match_stmt', 'visit_star_expr', 'visit_int_expr', 'visit_str_expr', 'visit_bytes_expr', 'visit_float_expr', 'visit_complex_expr', 'visit_ellipsis', 'visit_name_expr', 'duplicate_name', 'visit_member_expr', 'copy_ref', 'visit_yield_from_expr', 'visit_yield_expr', 'visit_await_expr', 'visit_call_expr', 'visit_op_expr', 'visit_comparison_expr', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_reveal_expr', 'visit_super_expr', 'visit_assignment_expr', 'visit_unary_expr', 'visit_list_expr', 'visit_dict_expr', 'visit_tuple_expr', 'visit_set_expr', 'visit_index_expr', 'visit_type_application', 'visit_list_comprehension', 'visit_set_comprehension', 'visit_dictionary_comprehension', 'visit_generator_expr', 'duplicate_generator', 'visit_slice_expr', 'visit_conditional_expr', 'visit_type_var_expr', 'visit_paramspec_expr', 'visit_type_var_tuple_expr', 'visit_type_alias_expr', 'visit_newtype_expr', 'visit_namedtuple_expr', 'visit_enum_call_expr', 'visit_typeddict_expr', 'visit__promote_expr', 'visit_temp_node', 'node', 'mypyfile', 'expr', 'stmt', 'pattern', 'optional_expr', 'block', 'optional_block', 'statements', 'expressions', 'optional_expressions', 'blocks', 'names', 'optional_names', 'type', 'optional_type', 'types']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\treetransform.py,FuncMapInitializer,"This traverser creates mappings from nested FuncDefs to placeholder FuncDefs.

The placeholders will later be replaced with transformed nodes.",2,['TraverserVisitor'],0,"['__init__', 'visit_func_def']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\tvar_scope.py,TypeVarLikeNamespaceSetter,Set namespace for all TypeVarLikeTypes types.,4,['TypeTraverserVisitor'],0,"['__init__', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\tvar_scope.py,TypeVarLikeScope,"Scope that holds bindings for type variables and parameter specifications.

Node fullname -> TypeVarLikeType.",10,[],0,"['__init__', 'get_function_scope', 'allow_binding', 'method_frame', 'class_frame', 'new_unique_func_id', 'bind_new', 'bind_existing', 'get_binding', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\typeops.py,TypeVarExtractor,,5,['TypeQuery[List[TypeVarLikeType]]'],0,"['__init__', '_merge', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\typestate.py,TypeState,"This class provides subtype caching to improve performance of subtype checks.
It also holds protocol fine grained dependencies.

Note: to avoid leaking global state, 'reset_all_subtype_caches()' should be called
after a build has finished and after a daemon shutdown. This subtype cache only exists for
performance reasons, resetting subtype caches for a class has no semantic effect.
The protocol dependencies however are only stored here, and shouldn't be deleted unless
not needed any more (e.g. during daemon shutdown).",16,[],11,"['__init__', 'is_assumed_subtype', 'is_assumed_proper_subtype', 'get_assumptions', 'reset_all_subtype_caches', 'reset_subtype_caches_for', 'reset_all_subtype_caches_for', 'is_cached_subtype_check', 'is_cached_negative_subtype_check', 'record_subtype_cache_entry', 'record_negative_subtype_cache_entry', 'reset_protocol_deps', 'record_protocol_subtype_check', '_snapshot_protocol_deps', 'update_protocol_deps', 'add_all_protocol_deps']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\typetraverser.py,TypeTraverserVisitor,Visitor that traverses all components of a type,27,['SyntheticTypeVisitor[None]'],0,"['visit_any', 'visit_uninhabited_type', 'visit_none_type', 'visit_erased_type', 'visit_deleted_type', 'visit_type_var', 'visit_param_spec', 'visit_parameters', 'visit_type_var_tuple', 'visit_literal_type', 'visit_instance', 'visit_callable_type', 'visit_tuple_type', 'visit_typeddict_type', 'visit_union_type', 'visit_overloaded', 'visit_type_type', 'visit_callable_argument', 'visit_unbound_type', 'visit_type_list', 'visit_ellipsis_type', 'visit_placeholder_type', 'visit_partial_type', 'visit_raw_expression_type', 'visit_type_alias_type', 'visit_unpack_type', 'traverse_types']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\util.py,DecodeError,"Exception raised when a file cannot be decoded due to an unknown encoding type.

Essentially a wrapper for the LookupError raised by `bytearray.decode`",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\util.py,IdMapper,"Generate integer ids for objects.

Unlike id(), these start from 0 and increment by 1, and ids won't
get reused across the life-time of IdMapper.

Assume objects don't redefine __eq__ or __hash__.",2,[],0,"['__init__', 'id']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\util.py,FancyFormatter,"Apply color and bold font to terminal output.

This currently only works on Linux and Mac.",11,[],0,"['__init__', 'initialize_vt100_colors', 'initialize_win_colors', 'initialize_unix_colors', 'style', 'fit_in_terminal', 'colorize', 'highlight_quote_groups', 'underline_link', 'format_success', 'format_error']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\errors.py,Errors,,6,[],0,"['__init__', 'error', 'note', 'warning', 'new_messages', 'flush_errors']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\namegen.py,NameGenerator,"Utility for generating distinct C names from Python names.

Since C names can't use '.' (or unicode), some care is required to
make C names generated from Python names unique. Also, we want to
avoid generating overly long C names since they make the generated
code harder to read.

Note that we don't restrict ourselves to a 32-character distinguishing
prefix guaranteed by the C standard since all the compilers we care
about at the moment support longer names without issues.

For names that are exported in a shared library (not static) use
exported_name() instead.

Summary of the approach:

* Generate a unique name prefix from suffix of fully-qualified
  module name used for static names. If only compiling a single
  module, this can be empty. For example, if the modules are
  'foo.bar' and 'foo.baz', the prefixes can be 'bar_' and 'baz_',
  respectively. If the modules are 'bar.foo' and 'baz.foo', the
  prefixes will be 'bar_foo_' and 'baz_foo_'.

* Replace '.' in the Python name with '___' in the C name. (And
  replace the unlikely but possible '___' with '___3_'. This
  collides '___' with '.3_', but this is OK because names
  may not start with a digit.)

The generated should be internal to a build and thus the mapping is
arbitrary. Just generating names '1', '2', ... would be correct,
though not very usable.",2,[],0,"['__init__', 'private_name']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\options.py,CompilerOptions,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\rt_subtype.py,RTSubtypeVisitor,"Is left a runtime subtype of right?

A few special cases such as right being 'object' are handled in
is_runtime_subtype and don't need to be covered here.",8,['RTypeVisitor[bool]'],0,"['__init__', 'visit_rinstance', 'visit_runion', 'visit_rprimitive', 'visit_rtuple', 'visit_rstruct', 'visit_rarray', 'visit_rvoid']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\sametype.py,SameTypeVisitor,,8,['RTypeVisitor[bool]'],0,"['__init__', 'visit_rinstance', 'visit_runion', 'visit_rprimitive', 'visit_rtuple', 'visit_rstruct', 'visit_rarray', 'visit_rvoid']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\subtype.py,SubtypeVisitor,"Is left a subtype of right?

A few special cases such as right being 'object' are handled in
is_subtype and don't need to be covered here.",8,['RTypeVisitor[bool]'],0,"['__init__', 'visit_rinstance', 'visit_runion', 'visit_rprimitive', 'visit_rtuple', 'visit_rstruct', 'visit_rarray', 'visit_rvoid']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXException,Base class for exceptions in NetworkX.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXError,Exception for a serious error in NetworkX,0,['NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXPointlessConcept,"Raised when a null graph is provided as input to an algorithm
that cannot use it.

The null graph is sometimes considered a pointless concept [1]_,
thus the name of the exception.

Notes
-----
Null graphs and empty graphs are often used interchangeably but they
are well defined in NetworkX. An ``empty_graph`` is a graph with ``n`` nodes
and 0 edges, and a ``null_graph`` is a graph with 0 nodes and 0 edges.

References
----------
.. [1] Harary, F. and Read, R. ""Is the Null Graph a Pointless
   Concept?""  In Graphs and Combinatorics Conference, George
   Washington University.  New York: Springer-Verlag, 1973.",0,['NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXAlgorithmError,Exception for unexpected termination of algorithms.,0,['NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXUnfeasible,"Exception raised by algorithms trying to solve a problem
instance that has no feasible solution.",0,['NetworkXAlgorithmError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXNoPath,"Exception for algorithms that should return a path when running
on graphs where such a path does not exist.",0,['NetworkXUnfeasible'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXNoCycle,"Exception for algorithms that should return a cycle when running
on graphs where such a cycle does not exist.",0,['NetworkXUnfeasible'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,HasACycle,"Raised if a graph has a cycle when an algorithm expects that it
will have no cycles.",0,['NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXUnbounded,"Exception raised by algorithms trying to solve a maximization
or a minimization problem instance that is unbounded.",0,['NetworkXAlgorithmError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NetworkXNotImplemented,Exception raised by algorithms not implemented for a type of graph.,0,['NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,NodeNotFound,Exception raised if requested node is not present in the graph,0,['NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,AmbiguousSolution,"Raised if more than one valid solution exists for an intermediary step
of an algorithm.

In the face of ambiguity, refuse the temptation to guess.
This may occur, for example, when trying to determine the
bipartite node sets in a disconnected bipartite graph when
computing bipartite matchings.",0,['NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,ExceededMaxIterations,"Raised if a loop iterates too many times without breaking.

This may occur, for example, in an algorithm that computes
progressively better approximations to a value but exceeds an
iteration bound specified by the user.",0,['NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\exception.py,PowerIterationFailedConvergence,"Raised when the power iteration method fails to converge within a
specified iteration limit.

`num_iterations` is the number of iterations that have been
completed when this exception was raised.",1,['ExceededMaxIterations'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\lazy_imports.py,DelayedImportErrorModule,,2,['types.ModuleType'],0,"['__init__', '__getattr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_pytesttester.py,PytestTester,"Pytest test runner.

A test function is typically added to a package's __init__.py like so::

  from numpy._pytesttester import PytestTester
  test = PytestTester(__name__).test
  del PytestTester

Calling this test function finds and runs all tests associated with the
module and all its sub-modules.

Attributes
----------
module_name : str
    Full path to the package to test.

Parameters
----------
module_name : module name
    The name of the module to test.

Notes
-----
Unlike the previous ``nose``-based implementation, this class is not
publicly exposed as it performs some ``numpy``-specific warning
suppression.",2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\markers.py,InvalidMarker,"An invalid marker was found, users should refer to PEP 508.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\markers.py,UndefinedComparison,An invalid operation was attempted on a value that doesn't support it.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\markers.py,UndefinedEnvironmentName,"A name was attempted to be used that does not exist inside of the
environment.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\markers.py,Environment,,0,['TypedDict'],11,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\markers.py,Marker,,6,[],0,"['__init__', '__str__', '__repr__', '__hash__', '__eq__', 'evaluate']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\requirements.py,InvalidRequirement,"An invalid requirement was found, users should refer to PEP 508.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\requirements.py,Requirement,"Parse a requirement.

Parse a given requirement string into its parts, such as name, specifier,
URL, and extras. Raises InvalidRequirement on a badly-formed requirement
string.",6,[],0,"['__init__', '_iter_parts', '__str__', '__repr__', '__hash__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\utils.py,InvalidName,An invalid distribution name; users should refer to the packaging user guide.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\utils.py,InvalidWheelFilename,"An invalid wheel filename was found, users should refer to PEP 427.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\utils.py,InvalidSdistFilename,"An invalid sdist filename was found, users should refer to the packaging user guide.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_manylinux.py,_GLibCVersion,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_musllinux.py,_MuslVersion,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_parser.py,Node,,4,[],0,"['__init__', '__str__', '__repr__', 'serialize']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_parser.py,Variable,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_parser.py,Value,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_parser.py,Op,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_parser.py,ParsedRequirement,,0,['NamedTuple'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_structures.py,InfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\packaging\_structures.py,NegativeInfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pbr\util.py,DefaultGetDict,"Like defaultdict, but get() also sets and returns the default value.",1,['defaultdict'],0,['get'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\BdfFontFile.py,BdfFontFile,Font file plugin for the X11 BDF format.,1,['FontFile.FontFile'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ContainerIO.py,ContainerIO,"A file object that provides read access to a part of an existing
file (for example a TAR file).",20,['IO[AnyStr]'],0,"['__init__', 'isatty', 'seekable', 'seek', 'tell', 'readable', 'read', 'readline', 'readlines', 'writable', 'write', 'writelines', 'truncate', '__enter__', '__exit__', '__iter__', '__next__', 'fileno', 'flush', 'close']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\FontFile.py,FontFile,Base class for raster font file handlers.,4,[],1,"['__init__', '__getitem__', 'compile', 'save']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\GimpGradientFile.py,GradientFile,,1,[],1,['getpalette'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\GimpGradientFile.py,GimpGradientFile,File handler for GIMP's gradient format.,1,['GradientFile'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageDraw.py,ImageDraw,,25,[],1,"['__init__', 'getfont', '_getfont', '_getink', 'arc', 'bitmap', 'chord', 'ellipse', 'circle', 'line', 'shape', 'pieslice', 'point', 'polygon', 'regular_polygon', 'rectangle', 'rounded_rectangle', '_multiline_check', '_multiline_split', '_multiline_spacing', 'text', 'multiline_text', 'textlength', 'textbbox', 'multiline_textbbox']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageDraw2.py,Pen,Stores an outline color and width.,1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageDraw2.py,Brush,Stores a fill color,1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageDraw2.py,Font,Stores a TrueType font and color,1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageDraw2.py,Draw,(Experimental) WCK-style drawing interface,14,[],0,"['__init__', 'flush', 'render', 'settransform', 'arc', 'chord', 'ellipse', 'line', 'pieslice', 'polygon', 'rectangle', 'text', 'textbbox', 'textlength']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageEnhance.py,_Enhance,,1,[],2,['enhance'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageEnhance.py,Color,"Adjust image color balance.

This class can be used to adjust the colour balance of an image, in
a manner similar to the controls on a colour TV set. An enhancement
factor of 0.0 gives a black and white image. A factor of 1.0 gives
the original image.",1,['_Enhance'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageEnhance.py,Contrast,"Adjust image contrast.

This class can be used to control the contrast of an image, similar
to the contrast control on a TV set. An enhancement factor of 0.0
gives a solid gray image. A factor of 1.0 gives the original image.",1,['_Enhance'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageEnhance.py,Brightness,"Adjust image brightness.

This class can be used to control the brightness of an image.  An
enhancement factor of 0.0 gives a black image. A factor of 1.0 gives the
original image.",1,['_Enhance'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageEnhance.py,Sharpness,"Adjust image sharpness.

This class can be used to adjust the sharpness of an image. An
enhancement factor of 0.0 gives a blurred image, a factor of 1.0 gives the
original image, and a factor of 2.0 gives a sharpened image.",1,['_Enhance'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageMath.py,_Operand,"Wraps an image operand, providing standard operators",34,[],0,"['__init__', '__fixup', 'apply', '__bool__', '__abs__', '__pos__', '__neg__', '__add__', '__radd__', '__sub__', '__rsub__', '__mul__', '__rmul__', '__truediv__', '__rtruediv__', '__mod__', '__rmod__', '__pow__', '__rpow__', '__invert__', '__and__', '__rand__', '__or__', '__ror__', '__xor__', '__rxor__', '__lshift__', '__rshift__', '__eq__', '__ne__', '__lt__', '__le__', '__gt__', '__ge__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageMode.py,ModeDescriptor,Wrapper for mode strings.,1,['NamedTuple'],5,['__str__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageMorph.py,LutBuilder,"A class for building a MorphLut from a descriptive language

The input patterns is a list of a strings sequences like these::

    4:(...
       .1.
       111)->1

(whitespaces including linebreaks are ignored). The option 4
describes a series of symmetry operations (in this case a
4-rotation), the pattern is described by:

- . or X - Ignore
- 1 - Pixel is on
- 0 - Pixel is off

The result of the operation is described after ""->"" string.

The default is to return the current pixel value, which is
returned if no other match is found.

Operations:

- 4 - 4 way rotation
- N - Negate
- 1 - Dummy op for no other operation (an op must always be given)
- M - Mirroring

Example::

    lb = LutBuilder(patterns = [""4:(... .1. 111)->1""])
    lut = lb.build_lut()",7,[],0,"['__init__', 'add_patterns', 'build_default_lut', 'get_lut', '_string_permute', '_pattern_permute', 'build_lut']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageMorph.py,MorphOp,A class for binary morphological operators,7,[],0,"['__init__', 'apply', 'match', 'get_on_pixels', 'load_lut', 'save_lut', 'set_lut']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageOps.py,SupportsGetMesh,"An object that supports the ``getmesh`` method, taking an image as an
argument, and returning a list of tuples. Each tuple contains two tuples,
the source box as a tuple of 4 integers, and a tuple of 8 integers for the
final quadrilateral, in order of top left, bottom left, bottom right, top
right.",1,['Protocol'],0,['getmesh'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageSequence.py,Iterator,"This class implements an iterator object that can be used to loop
over an image sequence.

You can use the ``[]`` operator to access elements by index. This operator
will raise an :py:exc:`IndexError` if you try to access a nonexistent
frame.

:param im: An image object.",4,[],0,"['__init__', '__getitem__', '__iter__', '__next__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageTk.py,PhotoImage,"A Tkinter-compatible photo image.  This can be used
everywhere Tkinter expects an image object.  If the image is an RGBA
image, pixels having alpha 0 are treated as transparent.

The constructor takes either a PIL image, or a mode and a size.
Alternatively, you can use the ``file`` or ``data`` options to initialize
the photo image object.

:param image: Either a PIL image, or a mode string.  If a mode string is
              used, a size must also be given.
:param size: If the first argument is a mode string, this defines the size
             of the image.
:keyword file: A filename to load the image from (using
               ``Image.open(file)``).
:keyword data: An 8-bit string containing image data (as loaded from an
               image file).",6,[],0,"['__init__', '__del__', '__str__', 'width', 'height', 'paste']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageTk.py,BitmapImage,"A Tkinter-compatible bitmap image.  This can be used everywhere Tkinter
expects an image object.

The given image must have mode ""1"".  Pixels having value 0 are treated as
transparent.  Options, if any, are passed on to Tkinter.  The most commonly
used option is ``foreground``, which is used to specify the color for the
non-transparent parts.  See the Tkinter documentation for information on
how to specify colours.

:param image: A PIL image.",5,[],0,"['__init__', '__del__', 'width', 'height', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageWin.py,HDC,"Wraps an HDC integer. The resulting object can be passed to the
:py:meth:`~PIL.ImageWin.Dib.draw` and :py:meth:`~PIL.ImageWin.Dib.expose`
methods.",2,[],0,"['__init__', '__int__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageWin.py,HWND,"Wraps an HWND integer. The resulting object can be passed to the
:py:meth:`~PIL.ImageWin.Dib.draw` and :py:meth:`~PIL.ImageWin.Dib.expose`
methods, instead of a DC.",2,[],0,"['__init__', '__int__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageWin.py,Dib,"A Windows bitmap with the given mode and size.  The mode can be one of ""1"",
""L"", ""P"", or ""RGB"".

If the display requires a palette, this constructor creates a suitable
palette and associates it with the image. For an ""L"" image, 128 graylevels
are allocated. For an ""RGB"" image, a 6x6x6 colour cube is used, together
with 20 graylevels.

To make sure that palettes work properly under Windows, you must call the
``palette`` method upon certain events from Windows.

:param image: Either a PIL image, or a mode string. If a mode string is
              used, a size must also be given.  The mode can be one of ""1"",
              ""L"", ""P"", or ""RGB"".
:param size: If the first argument is a mode string, this
             defines the size of the image.",7,[],0,"['__init__', 'expose', 'draw', 'query_palette', 'paste', 'frombytes', 'tobytes']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageWin.py,Window,Create a Window with the given title size.,8,[],0,"['__init__', '__dispatcher', 'ui_handle_clear', 'ui_handle_damage', 'ui_handle_destroy', 'ui_handle_repair', 'ui_handle_resize', 'mainloop']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\ImageWin.py,ImageWindow,Create an image window which displays the given image.,2,['Window'],0,"['__init__', 'ui_handle_repair']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\PSDraw.py,PSDraw,"Sets up printing to the given file. If ``fp`` is omitted,
``sys.stdout.buffer`` is assumed.",8,[],0,"['__init__', 'begin_document', 'end_document', 'setfont', 'line', 'rectangle', 'text', 'image']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\TarIO.py,TarIO,A file object that provides read access to a given member of a TAR file.,1,['ContainerIO.ContainerIO[bytes]'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\TiffTags.py,_TagInfo,,0,['NamedTuple'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\TiffTags.py,TagInfo,,2,['_TagInfo'],1,"['__new__', 'cvt_enum']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\_typing.py,SupportsRead,,1,['Protocol[_T_co]'],0,['read'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\PIL\__init__.py,UnidentifiedImageError,"Raised in :py:meth:`PIL.Image.open` if an image cannot be opened and identified.

If a PNG image raises this error, setting :data:`.ImageFile.LOAD_TRUNCATED_IMAGES`
to true may allow the image to be opened after all. The setting will ignore missing
data and checksum failures.",0,['OSError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\aliases.py,AliasPath,"Usage docs: https://docs.pydantic.dev/2.9/concepts/alias#aliaspath-and-aliaschoices

A data class used by `validation_alias` as a convenience to create aliases.

Attributes:
    path: A list of string or integer aliases.",3,[],1,"['__init__', 'convert_to_aliases', 'search_dict_for_path']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\aliases.py,AliasChoices,"Usage docs: https://docs.pydantic.dev/2.9/concepts/alias#aliaspath-and-aliaschoices

A data class used by `validation_alias` as a convenience to create aliases.

Attributes:
    choices: A list containing a string or `AliasPath`.",2,[],1,"['__init__', 'convert_to_aliases']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\aliases.py,AliasGenerator,"Usage docs: https://docs.pydantic.dev/2.9/concepts/alias#using-an-aliasgenerator

A data class used by `alias_generator` as a convenience to create various aliases.

Attributes:
    alias: A callable that takes a field name and returns an alias for it.
    validation_alias: A callable that takes a field name and returns a validation alias for it.
    serialization_alias: A callable that takes a field name and returns a serialization alias for it.",2,[],3,"['_generate_alias', 'generate_aliases']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\config.py,ConfigDict,A TypedDict for configuring Pydantic behaviour.,0,['TypedDict'],42,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\functional_serializers.py,PlainSerializer,"Plain serializers use a function to modify the output of serialization.

This is particularly helpful when you want to customize the serialization for annotated types.
Consider an input of `list`, which will be serialized into a space-delimited string.

```python
from typing import List

from typing_extensions import Annotated

from pydantic import BaseModel, PlainSerializer

CustomStr = Annotated[
    List, PlainSerializer(lambda x: ' '.join(x), return_type=str)
]

class StudentModel(BaseModel):
    courses: CustomStr

student = StudentModel(courses=['Math', 'Chemistry', 'English'])
print(student.model_dump())
#> {'courses': 'Math Chemistry English'}
```

Attributes:
    func: The serializer function.
    return_type: The return type for the function. If omitted it will be inferred from the type annotation.
    when_used: Determines when this serializer should be used. Accepts a string with values `'always'`,
        `'unless-none'`, `'json'`, and `'json-unless-none'`. Defaults to 'always'.",1,[],3,['__get_pydantic_core_schema__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\functional_serializers.py,WrapSerializer,"Wrap serializers receive the raw inputs along with a handler function that applies the standard serialization
logic, and can modify the resulting value before returning it as the final output of serialization.

For example, here's a scenario in which a wrap serializer transforms timezones to UTC **and** utilizes the existing `datetime` serialization logic.

```python
from datetime import datetime, timezone
from typing import Any, Dict

from typing_extensions import Annotated

from pydantic import BaseModel, WrapSerializer

class EventDatetime(BaseModel):
    start: datetime
    end: datetime

def convert_to_utc(value: Any, handler, info) -> Dict[str, datetime]:
    # Note that `handler` can actually help serialize the `value` for
    # further custom serialization in case it's a subclass.
    partial_result = handler(value, info)
    if info.mode == 'json':
        return {
            k: datetime.fromisoformat(v).astimezone(timezone.utc)
            for k, v in partial_result.items()
        }
    return {k: v.astimezone(timezone.utc) for k, v in partial_result.items()}

UTCEventDatetime = Annotated[EventDatetime, WrapSerializer(convert_to_utc)]

class EventModel(BaseModel):
    event_datetime: UTCEventDatetime

dt = EventDatetime(
    start='2024-01-01T07:00:00-08:00', end='2024-01-03T20:00:00+06:00'
)
event = EventModel(event_datetime=dt)
print(event.model_dump())
'''
{
    'event_datetime': {
        'start': datetime.datetime(
            2024, 1, 1, 15, 0, tzinfo=datetime.timezone.utc
        ),
        'end': datetime.datetime(
            2024, 1, 3, 14, 0, tzinfo=datetime.timezone.utc
        ),
    }
}
'''

print(event.model_dump_json())
'''
{""event_datetime"":{""start"":""2024-01-01T15:00:00Z"",""end"":""2024-01-03T14:00:00Z""}}
'''
```

Attributes:
    func: The serializer function to be wrapped.
    return_type: The return type for the function. If omitted it will be inferred from the type annotation.
    when_used: Determines when this serializer should be used. Accepts a string with values `'always'`,
        `'unless-none'`, `'json'`, and `'json-unless-none'`. Defaults to 'always'.",1,[],3,['__get_pydantic_core_schema__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\warnings.py,PydanticDeprecationWarning,"A Pydantic specific deprecation warning.

This warning is raised when using deprecated functionality in Pydantic. It provides information on when the
deprecation was introduced and the expected version in which the corresponding functionality will be removed.

Attributes:
    message: Description of the warning.
    since: Pydantic version in what the deprecation was introduced.
    expected_removal: Pydantic version in what the corresponding functionality expected to be removed.",2,['DeprecationWarning'],3,"['__init__', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\warnings.py,PydanticDeprecatedSince20,A specific `PydanticDeprecationWarning` subclass defining functionality deprecated since Pydantic 2.0.,1,['PydanticDeprecationWarning'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\warnings.py,PydanticDeprecatedSince26,A specific `PydanticDeprecationWarning` subclass defining functionality deprecated since Pydantic 2.6.,1,['PydanticDeprecationWarning'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\warnings.py,PydanticDeprecatedSince29,A specific `PydanticDeprecationWarning` subclass defining functionality deprecated since Pydantic 2.9.,1,['PydanticDeprecationWarning'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\warnings.py,GenericBeforeBaseModelWarning,,0,['Warning'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\warnings.py,PydanticExperimentalWarning,"A Pydantic specific experimental functionality warning.

This warning is raised when using experimental functionality in Pydantic.
It is raised to warn users that the functionality may change or be removed in future versions of Pydantic.",0,['Warning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic_core\__init__.py,ErrorDetails,,0,['_TypedDict'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic_core\__init__.py,InitErrorDetails,,0,['_TypedDict'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic_core\__init__.py,ErrorTypeInfo,Gives information about errors.,0,['_TypedDict'],6,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic_core\__init__.py,MultiHostHost,A host part of a multi-host URL.,0,['_TypedDict'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydeps\arguments.py,Argument,,9,['object'],0,"['__init__', '__json__', 'typename', 'typefn', 'pytype', 'argname', 'help', 'default', 'add_to_parser']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydeps\arguments.py,Namespace,,7,['object'],0,"['__init__', '__repr__', 'items', '__getitem__', '__setitem__', '__delitem__', '__getattr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydeps\arguments.py,Arguments,,6,['object'],0,"['__init__', 'load_config_files', 'write_default_config', 'parse_args', 'add', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydeps\colors.py,ColorSpace,,4,['object'],0,"['__init__', 'add_to_tree', 'color', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydeps\depgraph2dot.py,PyDepGraphDot,,2,['object'],0,"['__init__', 'render']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydeps\depgraph2dot.py,CycleGraphDot,,2,['object'],0,"['__init__', 'render']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydeps\dummymodule.py,DummyModule,"We create a file that imports the module to be investigated.
    ",5,['object'],0,"['__init__', 'text', 'legal_module_name', 'print_header', 'print_import']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydeps\mf27.py,ModuleFinder,,3,['NativeModuleFinder'],0,"['import_hook', 'load_module', 'scan_code']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\midi.py,Input,"Input is used to get midi input from midi devices.
Input(device_id)
Input(device_id, buffer_size)

buffer_size - the number of input events to be buffered waiting to
  be read using Input.read()",5,[],0,"['__init__', '_check_open', 'close', 'read', 'poll']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\midi.py,Output,"Output is used to send midi to an output device
Output(device_id)
Output(device_id, latency = 0)
Output(device_id, buffer_size = 4096)
Output(device_id, latency, buffer_size)

The buffer_size specifies the number of output events to be
buffered waiting for output.  (In some cases -- see below --
PortMidi does not buffer output at all and merely passes data
to a lower-level API, in which case buffersize is ignored.)

latency is the delay in milliseconds applied to timestamps to determine
when the output should actually occur. (If latency is < 0, 0 is
assumed.)

If latency is zero, timestamps are ignored and all output is delivered
immediately. If latency is greater than zero, output is delayed until
the message timestamp plus the latency. (NOTE: time is measured
relative to the time source indicated by time_proc. Timestamps are
absolute, not relative delays or offsets.) In some cases, PortMidi
can obtain better timing than your application by passing timestamps
along to the device driver or hardware. Latency may also help you
to synchronize midi data to audio data by matching midi latency to
the audio buffer latency.",11,[],0,"['__init__', '_check_open', 'close', 'abort', 'write', 'write_short', 'write_sys_ex', 'note_on', 'note_off', 'set_instrument', 'pitch_bend']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\midi.py,MidiException,"exception that pygame.midi functions and classes can raise
MidiException(errno)",2,['Exception'],0,"['__init__', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\_camera_opencv.py,Camera,,10,[],0,"['__init__', 'start', 'stop', '_check_open', 'get_size', 'set_controls', 'get_controls', 'query_image', 'get_image', 'get_raw']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\_camera_opencv.py,CameraMac,,1,['Camera'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\_camera_vidcapture.py,Camera,,10,[],0,"['__init__', 'display_capture_filter_properties', 'display_capture_pin_properties', 'set_resolution', 'get_buffer', 'start', 'set_controls', 'stop', 'get_image', 'get_surface']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygments\cmdline.py,HelpFormatter,,1,['argparse.HelpFormatter'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygments\util.py,ClassNotFound,Raised if one of the lookup functions didn't find a matching class.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygments\util.py,OptionError,"This exception will be raised by all option processing functions if
the type or value of the argument is not correct.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygments\util.py,Future,"Generic class to defer some work.

Handled specially in RegexLexerMeta, to support regex string construction at
first use.",1,[],0,['get'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygments\util.py,UnclosingTextIOWrapper,,1,['TextIOWrapper'],0,['close'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pysnooper\tracer.py,UnavailableSource,,1,['object'],0,['__getitem__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pysnooper\tracer.py,FileWriter,,2,['object'],0,"['__init__', 'write']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pysnooper\tracer.py,Tracer,"Snoop on the function, writing everything it's doing to stderr.

This is useful for debugging.

When you decorate a function with `@pysnooper.snoop()`
or wrap a block of code in `with pysnooper.snoop():`, you'll get a log of
every line that ran in the function and a play-by-play of every local
variable that changed.

If stderr is not easily accessible for you, you can redirect the output to
a file::

    @pysnooper.snoop('/my/log/file.log')

See values of some expressions that aren't local variables::

    @pysnooper.snoop(watch=('foo.bar', 'self.x[""whatever""]'))

Expand values to see all their attributes or items of lists/dictionaries:

    @pysnooper.snoop(watch_explode=('foo', 'self'))

(see Advanced Usage in the README for more control)

Show snoop lines for functions that your function calls::

    @pysnooper.snoop(depth=2)

Start all snoop lines with a prefix, to grep for them easily::

    @pysnooper.snoop(prefix='ZZZ ')

On multi-threaded apps identify which thread are snooped in output::

    @pysnooper.snoop(thread_info=True)

Customize how values are represented as strings::

    @pysnooper.snoop(custom_repr=((type1, custom_repr_func1),
                     (condition2, custom_repr_func2), ...))

Variables and exceptions get truncated to 100 characters by default. You
can customize that:

    @pysnooper.snoop(max_variable_length=200)

You can also use `max_variable_length=None` to never truncate them.

Show timestamps relative to start time rather than wall time::

    @pysnooper.snoop(relative_time=True)

The output is colored for easy viewing by default, except on Windows.
Disable colors like so:

    @pysnooper.snoop(color=False)",10,[],0,"['__init__', '__call__', '_wrap_class', '_wrap_function', 'write', '__enter__', '__exit__', '_is_internal_frame', 'set_thread_info_padding', 'trace']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\auth.py,AuthBase,Base class that all auth implementations derive from,1,[],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\auth.py,HTTPBasicAuth,Attaches HTTP Basic Authentication to the given Request object.,4,['AuthBase'],0,"['__init__', '__eq__', '__ne__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\auth.py,HTTPProxyAuth,Attaches HTTP Proxy Authentication to a given Request object.,1,['HTTPBasicAuth'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\auth.py,HTTPDigestAuth,Attaches HTTP Digest Authentication to the given Request object.,8,['AuthBase'],0,"['__init__', 'init_per_thread_state', 'build_digest_header', 'handle_redirect', 'handle_401', '__call__', '__eq__', '__ne__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,RequestException,"There was an ambiguous exception that occurred while handling your
request.",1,['IOError'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,InvalidJSONError,A JSON error occurred.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,JSONDecodeError,Couldn't decode the text into json,2,"['InvalidJSONError', 'CompatJSONDecodeError']",0,"['__init__', '__reduce__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,HTTPError,An HTTP error occurred.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,ConnectionError,A Connection error occurred.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,ProxyError,A proxy error occurred.,0,['ConnectionError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,SSLError,An SSL error occurred.,0,['ConnectionError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,Timeout,"The request timed out.

Catching this error will catch both
:exc:`~requests.exceptions.ConnectTimeout` and
:exc:`~requests.exceptions.ReadTimeout` errors.",0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,ConnectTimeout,"The request timed out while trying to connect to the remote server.

Requests that produced this error are safe to retry.",0,"['ConnectionError', 'Timeout']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,ReadTimeout,The server did not send any data in the allotted amount of time.,0,['Timeout'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,URLRequired,A valid URL is required to make a request.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,TooManyRedirects,Too many redirects.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,MissingSchema,The URL scheme (e.g. http or https) is missing.,0,"['RequestException', 'ValueError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,InvalidSchema,The URL scheme provided is either invalid or unsupported.,0,"['RequestException', 'ValueError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,InvalidURL,The URL provided was somehow invalid.,0,"['RequestException', 'ValueError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,InvalidHeader,The header value provided was somehow invalid.,0,"['RequestException', 'ValueError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,InvalidProxyURL,The proxy URL provided is invalid.,0,['InvalidURL'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,ChunkedEncodingError,The server declared chunked encoding but sent an invalid chunk.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,ContentDecodingError,Failed to decode response content.,0,"['RequestException', 'BaseHTTPError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,StreamConsumedError,The content for this response was already consumed.,0,"['RequestException', 'TypeError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,RetryError,Custom retries logic failed,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,UnrewindableBodyError,Requests encountered an error when trying to rewind a body.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,RequestsWarning,Base warning for Requests.,0,['Warning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,FileModeWarning,"A file was opened in text mode, but Requests determined its binary length.",0,"['RequestsWarning', 'DeprecationWarning']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\exceptions.py,RequestsDependencyWarning,An imported dependency doesn't match the expected version range.,0,['RequestsWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\structures.py,CaseInsensitiveDict,"A case-insensitive ``dict``-like object.

Implements all methods and operations of
``MutableMapping`` as well as dict's ``copy``. Also
provides ``lower_items``.

All keys are expected to be strings. The structure remembers the
case of the last key to be set, and ``iter(instance)``,
``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``
will contain case-sensitive keys. However, querying and contains
testing is case insensitive::

    cid = CaseInsensitiveDict()
    cid['Accept'] = 'application/json'
    cid['aCCEPT'] == 'application/json'  # True
    list(cid) == ['Accept']  # True

For example, ``headers['content-encoding']`` will return the
value of a ``'Content-Encoding'`` response header, regardless
of how the header name was originally stored.

If the constructor, ``.update``, or equality comparison
operations are given keys that have equal ``.lower()``s, the
behavior is undefined.",10,['MutableMapping'],0,"['__init__', '__setitem__', '__getitem__', '__delitem__', '__iter__', '__len__', 'lower_items', '__eq__', 'copy', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests\structures.py,LookupDict,Dictionary lookup object.,4,['dict'],0,"['__init__', '__repr__', '__getitem__', 'get']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\exceptions.py,StreamingError,Used in :mod:`requests_toolbelt.downloadutils.stream`.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\exceptions.py,VersionMismatchError,"Used to indicate a version mismatch in the version of requests required.

The feature in use requires a newer version of Requests to function
appropriately but the version installed is not sufficient.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\exceptions.py,RequestsVersionTooOld,"Used to indicate that the Requests version is too old.

If the version of Requests is too old to support a feature, we will issue
this warning to the user.",0,['Warning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\streaming_iterator.py,StreamingIterator,"This class provides a way of allowing iterators with a known size to be
streamed instead of chunked.

In requests, if you pass in an iterator it assumes you want to use
chunked transfer-encoding to upload the data, which not all servers
support well. Additionally, you may want to set the content-length
yourself to avoid this but that will not work. The only way to preempt
requests using a chunked transfer-encoding and forcing it to stream the
uploads is to mimic a very specific interace. Instead of having to know
these details you can instead just use this class. You simply provide the
size and iterator and pass the instance of StreamingIterator to requests
via the data parameter like so:

.. code-block:: python

    from requests_toolbelt import StreamingIterator

    import requests

    # Let iterator be some generator that you already have and size be
    # the size of the data produced by the iterator

    r = requests.post(url, data=StreamingIterator(size, iterator))

You can also pass file-like objects to :py:class:`StreamingIterator` in
case requests can't determize the filesize itself. This is the case with
streaming file objects like ``stdin`` or any sockets. Wrapping e.g. files
that are on disk with ``StreamingIterator`` is unnecessary, because
requests can determine the filesize itself.

Naturally, you should also set the `Content-Type` of your upload
appropriately because the toolbelt will not attempt to guess that for you.",2,['object'],0,"['__init__', 'read']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\streaming_iterator.py,_IteratorAsBinaryFile,,4,['object'],0,"['__init__', '_get_bytes', '_load_bytes', 'read']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\ansi.py,_AnsiToken,Result of ansi tokenized string.,0,['NamedTuple'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\ansi.py,AnsiDecoder,Translate ANSI code in to styled Text.,3,[],0,"['__init__', 'decode', 'decode_line']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\bar.py,Bar,"Renders a solid block bar.

Args:
    size (float): Value for the end of the bar.
    begin (float): Begin point (between 0 and size, inclusive).
    end (float): End point (between 0 and size, inclusive).
    width (int, optional): Width of the bar, or ``None`` for maximum width. Defaults to None.
    color (Union[Color, str], optional): Color of the bar. Defaults to ""default"".
    bgcolor (Union[Color, str], optional): Color of bar background. Defaults to ""default"".",4,['JupyterMixin'],0,"['__init__', '__repr__', '__rich_console__', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\box.py,Box,"Defines characters to render boxes.

┌─┬┐ top
│ ││ head
├─┼┤ head_row
│ ││ mid
├─┼┤ row
├─┼┤ foot_row
│ ││ foot
└─┴┘ bottom

Args:
    box (str): Characters making up box.
    ascii (bool, optional): True if this box uses ascii characters only. Default is False.",8,[],0,"['__init__', '__repr__', '__str__', 'substitute', 'get_plain_headed_box', 'get_top', 'get_row', 'get_bottom']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\columns.py,Columns,"Display renderables in neat columns.

Args:
    renderables (Iterable[RenderableType]): Any number of Rich renderables (including str).
    width (int, optional): The desired width of the columns, or None to auto detect. Defaults to None.
    padding (PaddingDimensions, optional): Optional padding around cells. Defaults to (0, 1).
    expand (bool, optional): Expand columns to full width. Defaults to False.
    equal (bool, optional): Arrange in to equal sized columns. Defaults to False.
    column_first (bool, optional): Align items from top to bottom (rather than left to right). Defaults to False.
    right_to_left (bool, optional): Start column from right hand side. Defaults to False.
    align (str, optional): Align value (""left"", ""right"", or ""center"") or None for default. Defaults to None.
    title (TextType, optional): Optional title for Columns.",3,['JupyterMixin'],0,"['__init__', 'add_renderable', '__rich_console__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\constrain.py,Constrain,"Constrain the width of a renderable to a given number of characters.

Args:
    renderable (RenderableType): A renderable object.
    width (int, optional): The maximum width (in characters) to render. Defaults to 80.",3,['JupyterMixin'],0,"['__init__', '__rich_console__', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,ConsoleError,An error in console operation.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,StyleError,An error in styles.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,StyleSyntaxError,Style was badly formatted.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,MissingStyle,No such style.,0,['StyleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,StyleStackError,Style stack is invalid.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,NotRenderableError,Object is not renderable.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,MarkupError,Markup was badly formatted.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,LiveError,Error related to Live display.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\errors.py,NoAltScreen,Alt screen mode was required.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\live_render.py,LiveRender,"Creates a renderable that may be updated.

Args:
    renderable (RenderableType): Any renderable object.
    style (StyleType, optional): An optional style to apply to the renderable. Defaults to """".",5,[],0,"['__init__', 'set_renderable', 'position_cursor', 'restore_cursor', '__rich_console__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\logging.py,RichHandler,"A logging handler that renders output with Rich. The time / level / message and file are displayed in columns.
The level is color coded, and the message is syntax highlighted.

Note:
    Be careful when enabling console markup in log messages if you have configured logging for libraries not
    under your control. If a dependency writes messages containing square brackets, it may not produce the intended output.

Args:
    level (Union[int, str], optional): Log level. Defaults to logging.NOTSET.
    console (:class:`~rich.console.Console`, optional): Optional console instance to write logs.
        Default will use a global console instance writing to stdout.
    show_time (bool, optional): Show a column for the time. Defaults to True.
    omit_repeated_times (bool, optional): Omit repetition of the same time. Defaults to True.
    show_level (bool, optional): Show a column for the level. Defaults to True.
    show_path (bool, optional): Show the path to the original log call. Defaults to True.
    enable_link_path (bool, optional): Enable terminal link of path column to file. Defaults to True.
    highlighter (Highlighter, optional): Highlighter to style log messages, or None to use ReprHighlighter. Defaults to None.
    markup (bool, optional): Enable console markup in log messages. Defaults to False.
    rich_tracebacks (bool, optional): Enable rich tracebacks with syntax highlighting and formatting. Defaults to False.
    tracebacks_width (Optional[int], optional): Number of characters used to render tracebacks, or None for full width. Defaults to None.
    tracebacks_code_width (int, optional): Number of code characters used to render tracebacks, or None for full width. Defaults to 88.
    tracebacks_extra_lines (int, optional): Additional lines of code to render tracebacks, or None for full width. Defaults to None.
    tracebacks_theme (str, optional): Override pygments theme used in traceback.
    tracebacks_word_wrap (bool, optional): Enable word wrapping of long tracebacks lines. Defaults to True.
    tracebacks_show_locals (bool, optional): Enable display of locals in tracebacks. Defaults to False.
    tracebacks_suppress (Sequence[Union[str, ModuleType]]): Optional sequence of modules or paths to exclude from traceback.
    tracebacks_max_frames (int, optional): Optional maximum number of frames returned by traceback.
    locals_max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.
        Defaults to 10.
    locals_max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to 80.
    log_time_format (Union[str, TimeFormatterCallable], optional): If ``log_time`` is enabled, either string for strftime or callable that formats the time. Defaults to ""[%x %X] "".
    keywords (List[str], optional): List of words to highlight instead of ``RichHandler.KEYWORDS``.",5,['Handler'],2,"['__init__', 'get_level_text', 'emit', 'render_message', 'render']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\region.py,Region,Defines a rectangular region of the screen.,0,['NamedTuple'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\repr.py,ReprError,An error occurred when attempting to build a repr.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\rule.py,Rule,"A console renderable to draw a horizontal rule (line).

Args:
    title (Union[str, Text], optional): Text to render in the rule. Defaults to """".
    characters (str, optional): Character(s) used to draw the line. Defaults to ""─"".
    style (StyleType, optional): Style of Rule. Defaults to ""rule.line"".
    end (str, optional): Character at end of Rule. defaults to ""\\n""
    align (str, optional): How to align the title, one of ""left"", ""center"", or ""right"". Defaults to ""center"".",5,['JupyterMixin'],0,"['__init__', '__repr__', '__rich_console__', '_rule_line', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\screen.py,Screen,"A renderable that fills the terminal screen and crops excess.

Args:
    renderable (RenderableType): Child renderable.
    style (StyleType, optional): Optional background style. Defaults to None.",2,[],1,"['__init__', '__rich_console__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\spinner.py,Spinner,"A spinner animation.

Args:
    name (str): Name of spinner (run python -m rich.spinner).
    text (RenderableType, optional): A renderable to display at the right of the spinner (str or Text typically). Defaults to """".
    style (StyleType, optional): Style for spinner animation. Defaults to None.
    speed (float, optional): Speed factor for animation. Defaults to 1.0.

Raises:
    KeyError: If name isn't one of the supported spinner animations.",5,[],0,"['__init__', '__rich_console__', '__rich_measure__', 'render', 'update']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\styled.py,Styled,"Apply a style to a renderable.

Args:
    renderable (RenderableType): Any renderable.
    style (StyleType): A style to apply across the entire renderable.",3,[],0,"['__init__', '__rich_console__', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\terminal_theme.py,TerminalTheme,"A color theme used when exporting console content.

Args:
    background (Tuple[int, int, int]): The background color.
    foreground (Tuple[int, int, int]): The foreground (text) color.
    normal (List[Tuple[int, int, int]]): A list of 8 normal intensity colors.
    bright (List[Tuple[int, int, int]], optional): A list of 8 bright colors, or None
        to repeat normal intensity. Defaults to None.",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\_inspect.py,Inspect,"A renderable to inspect any Python Object.

Args:
    obj (Any): An object to inspect.
    title (str, optional): Title to display over inspect result, or None use type. Defaults to None.
    help (bool, optional): Show full help text rather than just first paragraph. Defaults to False.
    methods (bool, optional): Enable inspection of callables. Defaults to False.
    docs (bool, optional): Also render doc strings. Defaults to True.
    private (bool, optional): Show private attributes (beginning with underscore). Defaults to False.
    dunder (bool, optional): Show attributes starting with double underscore. Defaults to False.
    sort (bool, optional): Sort attributes alphabetically. Defaults to True.
    all (bool, optional): Show all attributes. Defaults to False.
    value (bool, optional): Pretty print value of object. Defaults to True.",6,['JupyterMixin'],0,"['__init__', '_make_title', '__rich__', '_get_signature', '_render', '_get_formatted_doc']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\_log_render.py,LogRender,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\_null_file.py,NullFile,,19,['IO[str]'],0,"['close', 'isatty', 'read', 'readable', 'readline', 'readlines', 'seek', 'seekable', 'tell', 'truncate', 'writable', 'writelines', '__next__', '__iter__', '__enter__', '__exit__', 'write', 'flush', 'fileno']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\_ratio.py,Edge,Any object that defines an edge (such as Layout).,0,['Protocol'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\_windows.py,WindowsConsoleFeatures,Windows features available.,0,[],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\rich\__main__.py,ColorBox,,2,[],0,"['__rich_console__', '__rich_measure__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\archive_util.py,UnrecognizedFormat,Couldn't recognize the archive type,0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\depends.py,Require,A prerequisite to building or installing a distribution,6,[],0,"['__init__', 'full_name', 'version_ok', 'get_version', 'is_present', 'is_current']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\errors.py,InvalidConfigError,Error used for invalid configurations.,0,['OptionError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\errors.py,RemovedConfigError,Error used for configurations that were deprecated and removed.,0,['OptionError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\errors.py,RemovedCommandError,"Error used for commands that have been removed in setuptools.

Since ``setuptools`` is built on ``distutils``, simply removing a command
from ``setuptools`` will make the behavior fall back to ``distutils``; this
error is raised if a command exists in ``distutils`` but has been actively
removed in ``setuptools``.",0,"['BaseError', 'RuntimeError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\errors.py,PackageDiscoveryError,"Impossible to perform automatic discovery of packages and/or modules.

The current project layout or given discovery options can lead to problems when
scanning the project directory.

Setuptools might also refuse to complete auto-discovery if an error prone condition
is detected (e.g. when a project is organised as a flat-layout but contains
multiple directories that can be taken as top-level packages inside a single
distribution [*]_). In these situations the users are encouraged to be explicit
about which packages to include or to make the discovery parameters more specific.

.. [*] Since multi-package distributions are uncommon it is very likely that the
   developers did not intend for all the directories to be packaged, and are just
   leaving auxiliary code in the repository top-level, such as maintenance-related
   scripts.",0,"['BaseError', 'RuntimeError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\extension.py,Extension,"Describes a single extension module.

This means that all source files will be compiled into a single binary file
``<module path>.<suffix>`` (with ``<module path>`` derived from ``name`` and
``<suffix>`` defined by one of the values in
``importlib.machinery.EXTENSION_SUFFIXES``).

In the case ``.pyx`` files are passed as ``sources and`` ``Cython`` is **not**
installed in the build environment, ``setuptools`` may also try to look for the
equivalent ``.cpp`` or ``.c`` files.

:arg str name:
  the full name of the extension, including any packages -- ie.
  *not* a filename or pathname, but Python dotted name

:arg list[str|os.PathLike[str]] sources:
  list of source filenames, relative to the distribution root
  (where the setup script lives), in Unix form (slash-separated)
  for portability.  Source files may be C, C++, SWIG (.i),
  platform-specific resource files, or whatever else is recognized
  by the ""build_ext"" command as source for a Python extension.

:keyword list[str] include_dirs:
  list of directories to search for C/C++ header files (in Unix
  form for portability)

:keyword list[tuple[str, str|None]] define_macros:
  list of macros to define; each macro is defined using a 2-tuple:
  the first item corresponding to the name of the macro and the second
  item either a string with its value or None to
  define it without a particular value (equivalent of ""#define
  FOO"" in source or -DFOO on Unix C compiler command line)

:keyword list[str] undef_macros:
  list of macros to undefine explicitly

:keyword list[str] library_dirs:
  list of directories to search for C/C++ libraries at link time

:keyword list[str] libraries:
  list of library names (not filenames or paths) to link against

:keyword list[str] runtime_library_dirs:
  list of directories to search for C/C++ libraries at run time
  (for shared extensions, this is when the extension is loaded).
  Setting this will cause an exception during build on Windows
  platforms.

:keyword list[str] extra_objects:
  list of extra files to link with (eg. object files not implied
  by 'sources', static library that must be explicitly specified,
  binary resource files, etc.)

:keyword list[str] extra_compile_args:
  any extra platform- and compiler-specific information to use
  when compiling the source files in 'sources'.  For platforms and
  compilers where ""command line"" makes sense, this is typically a
  list of command-line arguments, but for other platforms it could
  be anything.

:keyword list[str] extra_link_args:
  any extra platform- and compiler-specific information to use
  when linking object files together to create the extension (or
  to create a new static Python interpreter).  Similar
  interpretation as for 'extra_compile_args'.

:keyword list[str] export_symbols:
  list of symbols to be exported from a shared extension.  Not
  used on all platforms, and not generally necessary for Python
  extensions, which typically export exactly one symbol: ""init"" +
  extension_name.

:keyword list[str] swig_opts:
  any extra options to pass to SWIG if a source file has the .i
  extension.

:keyword list[str] depends:
  list of files that the extension depends on

:keyword str language:
  extension language (i.e. ""c"", ""c++"", ""objc""). Will be detected
  from the source extensions if not provided.

:keyword bool optional:
  specifies that a build failure in the extension should not abort the
  build process, but simply not install the failing extension.

:keyword bool py_limited_api:
  opt-in flag for the usage of :doc:`Python's limited API <python:c-api/stable>`.

:raises setuptools.errors.PlatformError: if ``runtime_library_dirs`` is
  specified on Windows. (since v63)",2,['_Extension'],4,"['__init__', '_convert_pyx_sources_to_lang']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\extension.py,Library,"Just like a regular Extension, but built as a library instead",0,['Extension'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\dispatch.py,DispatchExtensionManager,"Loads all plugins and filters on execution.

This is useful for long-running processes that need to pass
different inputs to different extensions.

:param namespace: The namespace for the entry points.
:type namespace: str
:param check_func: Function to determine which extensions to load.
:type check_func: callable
:param invoke_on_load: Boolean controlling whether to invoke the
    object returned by the entry point after the driver is loaded.
:type invoke_on_load: bool
:param invoke_args: Positional arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_args: tuple
:param invoke_kwds: Named arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_kwds: dict
:param propagate_map_exceptions: Boolean controlling whether exceptions
    are propagated up through the map call or whether they are logged and
    then ignored
:type invoke_on_load: bool",2,['EnabledExtensionManager'],0,"['map', 'map_method']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\dispatch.py,NameDispatchExtensionManager,"Loads all plugins and filters on execution.

This is useful for long-running processes that need to pass
different inputs to different extensions and can predict the name
of the extensions before calling them.

The check_func argument should return a boolean, with ``True``
indicating that the extension should be loaded and made available
and ``False`` indicating that the extension should be ignored.

:param namespace: The namespace for the entry points.
:type namespace: str
:param check_func: Function to determine which extensions to load.
:type check_func: callable
:param invoke_on_load: Boolean controlling whether to invoke the
    object returned by the entry point after the driver is loaded.
:type invoke_on_load: bool
:param invoke_args: Positional arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_args: tuple
:param invoke_kwds: Named arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_kwds: dict
:param propagate_map_exceptions: Boolean controlling whether exceptions
    are propagated up through the map call or whether they are logged and
    then ignored
:type invoke_on_load: bool
:param on_load_failure_callback: Callback function that will be called when
    an entrypoint can not be loaded. The arguments that will be provided
    when this is called (when an entrypoint fails to load) are
    (manager, entrypoint, exception)
:type on_load_failure_callback: function
:param verify_requirements: Use setuptools to enforce the
    dependencies of the plugin(s) being loaded. Defaults to False.
:type verify_requirements: bool",4,['DispatchExtensionManager'],0,"['__init__', '_init_plugins', 'map', 'map_method']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\enabled.py,EnabledExtensionManager,"Loads only plugins that pass a check function.

The check_func argument should return a boolean, with ``True``
indicating that the extension should be loaded and made available
and ``False`` indicating that the extension should be ignored.

:param namespace: The namespace for the entry points.
:type namespace: str
:param check_func: Function to determine which extensions to load.
:type check_func: callable, taking an :class:`Extension`
    instance as argument
:param invoke_on_load: Boolean controlling whether to invoke the
    object returned by the entry point after the driver is loaded.
:type invoke_on_load: bool
:param invoke_args: Positional arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_args: tuple
:param invoke_kwds: Named arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_kwds: dict
:param propagate_map_exceptions: Boolean controlling whether exceptions
    are propagated up through the map call or whether they are logged and
    then ignored
:type propagate_map_exceptions: bool
:param on_load_failure_callback: Callback function that will be called when
    an entrypoint can not be loaded. The arguments that will be provided
    when this is called (when an entrypoint fails to load) are
    (manager, entrypoint, exception)
:type on_load_failure_callback: function
:param verify_requirements: Use setuptools to enforce the
    dependencies of the plugin(s) being loaded. Defaults to False.
:type verify_requirements: bool",2,['ExtensionManager'],0,"['__init__', '_load_one_plugin']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\exception.py,NoUniqueMatch,"There was more than one extension, or none, that matched the query.",0,['RuntimeError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\exception.py,NoMatches,There were no extensions with the driver name found.,0,['NoUniqueMatch'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\exception.py,MultipleMatches,There were multiple matches for the given name.,0,['NoUniqueMatch'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\hook.py,HookManager,"Coordinate execution of multiple extensions using a common name.

:param namespace: The namespace for the entry points.
:type namespace: str
:param name: The name of the hooks to load.
:type name: str
:param invoke_on_load: Boolean controlling whether to invoke the
    object returned by the entry point after the driver is loaded.
:type invoke_on_load: bool
:param invoke_args: Positional arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_args: tuple
:param invoke_kwds: Named arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_kwds: dict
:param on_load_failure_callback: Callback function that will be called when
    an entrypoint can not be loaded. The arguments that will be provided
    when this is called (when an entrypoint fails to load) are
    (manager, entrypoint, exception)
:type on_load_failure_callback: function
:param verify_requirements: Use setuptools to enforce the
    dependencies of the plugin(s) being loaded. Defaults to False.
:type verify_requirements: bool
:type on_missing_entrypoints_callback: function
:param warn_on_missing_entrypoint: Flag to control whether failing
    to load a plugin is reported via a log mess. Only applies if
    on_missing_entrypoints_callback is None.
:type warn_on_missing_entrypoint: bool",3,['NamedExtensionManager'],0,"['__init__', '_init_attributes', '__getitem__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\_cache.py,Cache,,5,[],0,"['__init__', '_get_data_for_path', 'get_group_all', 'get_group_named', 'get_single']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\tenacity\nap.py,sleep_using_event,Sleep strategy that waits on an event to be set.,2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\decoder.py,TomlDecodeError,Base toml Exception / Error.,1,['ValueError'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\decoder.py,CommentValue,,4,['object'],0,"['__init__', '__getitem__', '__setitem__', 'dump']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\decoder.py,InlineTableDict,Sentinel subclass of dict for inline tables.,0,['object'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\decoder.py,TomlDecoder,,13,['object'],0,"['__init__', 'get_empty_table', 'get_empty_inline_table', 'load_inline_object', '_get_split_on_quotes', 'load_line', '_load_line_multiline_str', 'load_value', 'bounded_string', '_load_array_isstrarray', 'load_array', 'preserve_comment', 'embed_comments']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\decoder.py,TomlPreserveCommentDecoder,,3,['TomlDecoder'],0,"['__init__', 'preserve_comment', 'embed_comments']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\encoder.py,TomlEncoder,,6,['object'],0,"['__init__', 'get_empty_table', 'dump_list', 'dump_inline_table', 'dump_value', 'dump_sections']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\encoder.py,TomlPreserveInlineDictEncoder,,1,['TomlEncoder'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\encoder.py,TomlArraySeparatorEncoder,,2,['TomlEncoder'],0,"['__init__', 'dump_list']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\encoder.py,TomlNumpyEncoder,,2,['TomlEncoder'],0,"['__init__', '_dump_int']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\encoder.py,TomlPreserveCommentEncoder,,1,['TomlEncoder'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\encoder.py,TomlPathlibEncoder,,2,['TomlEncoder'],0,"['_dump_pathlib_path', 'dump_value']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\ordered.py,TomlOrderedDecoder,,1,['TomlDecoder'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\ordered.py,TomlOrderedEncoder,,1,['TomlEncoder'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\toml\tz.py,TomlTz,,5,['tzinfo'],0,"['__init__', '__deepcopy__', 'tzname', 'utcoffset', 'dst']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\hub.py,_Faketqdm,,7,[],0,"['__init__', 'update', 'set_description', 'write', 'close', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\library.py,Library,"A class to create libraries that can be used to register new operators or
override operators in existing libraries from Python.
A user can optionally pass in a dispatch keyname if they only want to register
kernels corresponding to only one specific dispatch key.

To create a library to override operators in an existing library (with name ns), set the kind to ""IMPL"".
To create a new library (with name ns) to register new operators, set the kind to ""DEF"".
To create a fragment of a possibly existing library to register operators (and bypass
the limitation that there is only one library for a given namespace), set the kind to
""FRAGMENT"".

Args:
    ns: library name
    kind: ""DEF"", ""IMPL"" (default: ""IMPL""), ""FRAGMENT""
    dispatch_key: PyTorch dispatch key (default: """")",9,[],0,"['__init__', '__repr__', 'define', '_register_fake', '_register_torch_dispatch_rule', '_impl_with_aoti_compile', 'impl', 'fallback', '_destroy']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\torch_version.py,TorchVersion,"A string with magic powers to compare to both Version and iterables!
Prior to 1.10.0 torch.__version__ was stored as a str and so many did
comparisons against torch.__version__ as if it were a str. In order to not
break them we have TorchVersion which masquerades as a str while also
having the ability to compare against both packaging.version.Version as
well as tuples of values, eg. (1, 2, 1)
Examples:
    Comparing a TorchVersion object to a Version object
        TorchVersion('1.10.0a') > Version('1.10.0a')
    Comparing a TorchVersion object to a Tuple object
        TorchVersion('1.10.0a') > (1, 2)    # 1.2
        TorchVersion('1.10.0a') > (1, 2, 1) # 1.2.1
    Comparing a TorchVersion object against a string
        TorchVersion('1.10.0a') > '1.2'
        TorchVersion('1.10.0a') > '1.2.1'",2,['str'],0,"['_convert_to_version', '_cmp_wrapper']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\types.py,Storage,,11,[],4,"['__deepcopy__', '_new_shared', '_write_file', 'element_size', 'is_shared', 'share_memory_', 'nbytes', 'cpu', 'data_ptr', 'from_file', '_new_with_file']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_sources.py,SourceContext,,1,['SourceRangeFactory'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_sources.py,ParsedDef,,0,['NamedTuple'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_tensor_str.py,__PrinterOptions,,0,[],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_tensor_str.py,_Formatter,,3,[],0,"['__init__', 'width', 'format']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_utils.py,KeyErrorMessage,str subclass that returns itself in repr,1,['str'],0,['__repr__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_utils.py,ExceptionWrapper,Wraps an exception plus traceback to communicate across threads,2,[],0,"['__init__', 'reraise']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_utils.py,_ClassPropertyDescriptor,,2,[],0,"['__init__', '__get__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_utils.py,_LazySeedTracker,,4,[],0,"['__init__', 'queue_seed_all', 'queue_seed', 'get_calls']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_utils.py,CallbackRegistry,,3,['Generic[P]'],0,"['__init__', 'add_callback', 'fire_callbacks']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_utils_internal.py,JustKnobsConfig,"Represents a lazily loaded config

This is designed to be used to specify a value in a config.

i.e. foo.bar = JustknobsConfig(name=""//foo:bar"", env_name=""FORCE_FOO_BAR"")

Call .get() in order to access the value
i.e. if foo.bar.get():

Note that the value is fetched once, and then not allowed to change. This
means less suprises, at the downside that you may have to restart a job
to pick up an update.

It can also be set explicitly via set - i.e.
foo.bar = JustknobsConfig(name=""//foo:bar"")
foo.bar.set(True)

Note that this does allow for no JK name (so that you can use this to replace old configurations).",5,[],0,"['__init__', 'set', 'get', '__str__', '__bool__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_VF.py,VFModule,,2,['types.ModuleType'],1,"['__init__', '__getattr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_weights_only_unpickler.py,_safe_globals,,3,[],0,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_weights_only_unpickler.py,Unpickler,,4,[],0,"['__init__', 'load', 'pop_mark', 'persistent_load']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\gen_lazy_tensor.py,default_args,,0,[],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\local.py,Locals,,0,['threading.local'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\yaml_utils.py,YamlLoader,,1,['Loader'],0,['construct_mapping'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\unimport\exceptions.py,UnimportBaseException,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\unimport\exceptions.py,UnknownConfigKeyException,,2,['UnimportBaseException'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\unimport\exceptions.py,ConfigFileNotFound,,2,"['FileNotFoundError', 'UnimportBaseException']",0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\unimport\exceptions.py,UnsupportedConfigFile,,2,['UnimportBaseException'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\unimport\meta.py,MakeSingletonWithParams,,1,['type'],1,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\urllib3\poolmanager.py,PoolKey,"All known keyword arguments that could be provided to the pool manager, its
pools, or the underlying connections.

All custom key schemes should include the fields in this key at a minimum.",0,['typing.NamedTuple'],29,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\urllib3\poolmanager.py,PoolManager,"Allows for arbitrary requests while transparently keeping track of
necessary connection pools for you.

:param num_pools:
    Number of connection pools to cache before discarding the least
    recently used pool.

:param headers:
    Headers to include with all requests, unless other headers are given
    explicitly.

:param \**connection_pool_kw:
    Additional parameters are used to create fresh
    :class:`urllib3.connectionpool.ConnectionPool` instances.

Example:

.. code-block:: python

    import urllib3

    http = urllib3.PoolManager(num_pools=2)

    resp1 = http.request(""GET"", ""https://google.com/"")
    resp2 = http.request(""GET"", ""https://google.com/mail"")
    resp3 = http.request(""GET"", ""https://yahoo.com/"")

    print(len(http.pools))
    # 2",12,['RequestMethods'],2,"['__init__', '__enter__', '__exit__', '_new_pool', 'clear', 'connection_from_host', 'connection_from_context', 'connection_from_pool_key', 'connection_from_url', '_merge_pool_kwargs', '_proxy_requires_url_absolute_form', 'urlopen']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\urllib3\poolmanager.py,ProxyManager,"Behaves just like :class:`PoolManager`, but sends all requests through
the defined proxy, using the CONNECT method for HTTPS URLs.

:param proxy_url:
    The URL of the proxy to be used.

:param proxy_headers:
    A dictionary containing headers that will be sent to the proxy. In case
    of HTTP they are being sent with each request, while in the
    HTTPS/CONNECT case they are sent only once. Could be used for proxy
    authentication.

:param proxy_ssl_context:
    The proxy SSL context is used to establish the TLS connection to the
    proxy when using HTTPS proxies.

:param use_forwarding_for_https:
    (Defaults to False) If set to True will forward requests to the HTTPS
    proxy to be made on behalf of the client instead of creating a TLS
    tunnel via the CONNECT method. **Enabling this flag means that request
    and response headers and content will be visible from the HTTPS proxy**
    whereas tunneling keeps request and response headers and content
    private.  IP address, target hostname, SNI, and port are always visible
    to an HTTPS proxy even when this flag is disabled.

:param proxy_assert_hostname:
    The hostname of the certificate to verify against.

:param proxy_assert_fingerprint:
    The fingerprint of the certificate to verify against.

Example:

.. code-block:: python

    import urllib3

    proxy = urllib3.ProxyManager(""https://localhost:3128/"")

    resp1 = proxy.request(""GET"", ""https://google.com/"")
    resp2 = proxy.request(""GET"", ""https://httpbin.org/"")

    print(len(proxy.pools))
    # 1

    resp3 = proxy.request(""GET"", ""https://httpbin.org/"")
    resp4 = proxy.request(""GET"", ""https://twitter.com/"")

    print(len(proxy.pools))
    # 3",4,['PoolManager'],0,"['__init__', 'connection_from_host', '_set_proxy_headers', 'urlopen']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\urllib3\_base_connection.py,ProxyConfig,,0,['typing.NamedTuple'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\urllib3\_base_connection.py,_ResponseOptions,,0,['typing.NamedTuple'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\composer.py,ComposerError,,0,['MarkedYAMLError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\composer.py,Composer,,9,[],0,"['__init__', 'check_node', 'get_node', 'get_single_node', 'compose_document', 'compose_node', 'compose_scalar_node', 'compose_sequence_node', 'compose_mapping_node']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\cyaml.py,CBaseLoader,,1,"['CParser', 'BaseConstructor', 'BaseResolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\cyaml.py,CSafeLoader,,1,"['CParser', 'SafeConstructor', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\cyaml.py,CFullLoader,,1,"['CParser', 'FullConstructor', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\cyaml.py,CUnsafeLoader,,1,"['CParser', 'UnsafeConstructor', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\cyaml.py,CLoader,,1,"['CParser', 'Constructor', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\cyaml.py,CBaseDumper,,1,"['CEmitter', 'BaseRepresenter', 'BaseResolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\cyaml.py,CSafeDumper,,1,"['CEmitter', 'SafeRepresenter', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\cyaml.py,CDumper,,1,"['CEmitter', 'Serializer', 'Representer', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\dumper.py,BaseDumper,,1,"['Emitter', 'Serializer', 'BaseRepresenter', 'BaseResolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\dumper.py,SafeDumper,,1,"['Emitter', 'Serializer', 'SafeRepresenter', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\dumper.py,Dumper,,1,"['Emitter', 'Serializer', 'Representer', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\error.py,Mark,,3,[],0,"['__init__', 'get_snippet', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\error.py,YAMLError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\error.py,MarkedYAMLError,,2,['YAMLError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,Event,,2,['object'],0,"['__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,NodeEvent,,1,['Event'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,CollectionStartEvent,,1,['NodeEvent'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,CollectionEndEvent,,0,['Event'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,StreamStartEvent,,1,['Event'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,StreamEndEvent,,0,['Event'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,DocumentStartEvent,,1,['Event'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,DocumentEndEvent,,1,['Event'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,AliasEvent,,0,['NodeEvent'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,ScalarEvent,,1,['NodeEvent'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,SequenceStartEvent,,0,['CollectionStartEvent'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,SequenceEndEvent,,0,['CollectionEndEvent'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,MappingStartEvent,,0,['CollectionStartEvent'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\events.py,MappingEndEvent,,0,['CollectionEndEvent'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\loader.py,BaseLoader,,1,"['Reader', 'Scanner', 'Parser', 'Composer', 'BaseConstructor', 'BaseResolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\loader.py,FullLoader,,1,"['Reader', 'Scanner', 'Parser', 'Composer', 'FullConstructor', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\loader.py,SafeLoader,,1,"['Reader', 'Scanner', 'Parser', 'Composer', 'SafeConstructor', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\loader.py,Loader,,1,"['Reader', 'Scanner', 'Parser', 'Composer', 'Constructor', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yaml\loader.py,UnsafeLoader,,1,"['Reader', 'Scanner', 'Parser', 'Composer', 'Constructor', 'Resolver']",0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yarl\_quoting_py.py,_Quoter,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\yarl\_quoting_py.py,_Unquoter,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta_api_error.py,BetaAPIError,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta_authentication_error.py,BetaAuthenticationError,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta_error_response.py,BetaErrorResponse,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta_invalid_request_error.py,BetaInvalidRequestError,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta_not_found_error.py,BetaNotFoundError,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta_overloaded_error.py,BetaOverloadedError,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta_permission_error.py,BetaPermissionError,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta_rate_limit_error.py,BetaRateLimitError,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\completion.py,Completion,,0,['BaseModel'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\completion_create_params.py,CompletionCreateParamsBase,,0,['TypedDict'],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\completion_create_params.py,CompletionCreateParamsNonStreaming,,0,['CompletionCreateParamsBase'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\completion_create_params.py,CompletionCreateParamsStreaming,,0,['CompletionCreateParamsBase'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\image_block_param.py,Source,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\image_block_param.py,ImageBlockParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\input_json_delta.py,InputJSONDelta,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\message.py,Message,,0,['BaseModel'],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\message_create_params.py,MessageCreateParamsBase,,0,['TypedDict'],11,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\message_create_params.py,MessageCreateParamsNonStreaming,,0,['MessageCreateParamsBase'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\message_create_params.py,MessageCreateParamsStreaming,,0,['MessageCreateParamsBase'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\message_delta_usage.py,MessageDeltaUsage,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\message_param.py,MessageParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\metadata_param.py,MetadataParam,,0,['TypedDict'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\raw_content_block_delta_event.py,RawContentBlockDeltaEvent,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\raw_content_block_start_event.py,RawContentBlockStartEvent,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\raw_content_block_stop_event.py,RawContentBlockStopEvent,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\raw_message_delta_event.py,Delta,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\raw_message_delta_event.py,RawMessageDeltaEvent,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\raw_message_start_event.py,RawMessageStartEvent,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\raw_message_stop_event.py,RawMessageStopEvent,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\text_block.py,TextBlock,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\text_block_param.py,TextBlockParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\text_delta.py,TextDelta,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\tool_choice_any_param.py,ToolChoiceAnyParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\tool_choice_auto_param.py,ToolChoiceAutoParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\tool_choice_tool_param.py,ToolChoiceToolParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\tool_param.py,InputSchemaTyped,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\tool_param.py,ToolParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\tool_result_block_param.py,ToolResultBlockParam,,0,['TypedDict'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\tool_use_block.py,ToolUseBlock,,0,['BaseModel'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\tool_use_block_param.py,ToolUseBlockParam,,0,['TypedDict'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\usage.py,Usage,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_decoders\jsonl.py,JSONLDecoder,"A decoder for [JSON Lines](https://jsonlines.org) format.

This class provides an iterator over a byte-iterator that parses each JSON Line
into a given type.",4,['Generic[_T]'],1,"['__init__', '__decode__', '__next__', '__iter__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\_decoders\jsonl.py,AsyncJSONLDecoder,"A decoder for [JSON Lines](https://jsonlines.org) format.

This class provides an async iterator over a byte-iterator that parses each JSON Line
into a given type.",1,['Generic[_T]'],1,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\bedrock\_stream.py,BedrockStream,,1,['Stream[_T]'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\bedrock\_stream.py,AsyncBedrockStream,,1,['AsyncStream[_T]'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\bedrock\_stream_decoder.py,AWSEventStreamDecoder,,3,[],0,"['__init__', 'iter_bytes', '_parse_message_from_event']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\streaming\_prompt_caching_beta_types.py,MessageStopEvent,,0,['RawMessageStopEvent'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\streaming\_types.py,TextEvent,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\streaming\_types.py,InputJsonEvent,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\streaming\_types.py,MessageStopEvent,,0,['RawMessageStopEvent'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\streaming\_types.py,ContentBlockStopEvent,,0,['RawContentBlockStopEvent'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\lib\_extras\_common.py,MissingDependencyError,,1,['AnthropicError'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_base64_pdf_block_param.py,BetaBase64PDFBlockParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_base64_pdf_source_param.py,BetaBase64PDFSourceParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_cache_control_ephemeral_param.py,BetaCacheControlEphemeralParam,,0,['TypedDict'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_image_block_param.py,Source,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_image_block_param.py,BetaImageBlockParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_input_json_delta.py,BetaInputJSONDelta,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_message.py,BetaMessage,,0,['BaseModel'],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_message_delta_usage.py,BetaMessageDeltaUsage,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_message_param.py,BetaMessageParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_message_tokens_count.py,BetaMessageTokensCount,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_metadata_param.py,BetaMetadataParam,,0,['TypedDict'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_raw_content_block_delta_event.py,BetaRawContentBlockDeltaEvent,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_raw_content_block_start_event.py,BetaRawContentBlockStartEvent,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_raw_content_block_stop_event.py,BetaRawContentBlockStopEvent,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_raw_message_delta_event.py,Delta,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_raw_message_delta_event.py,BetaRawMessageDeltaEvent,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_raw_message_start_event.py,BetaRawMessageStartEvent,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_raw_message_stop_event.py,BetaRawMessageStopEvent,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_text_block.py,BetaTextBlock,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_text_block_param.py,BetaTextBlockParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_text_delta.py,BetaTextDelta,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_bash_20241022_param.py,BetaToolBash20241022Param,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_choice_any_param.py,BetaToolChoiceAnyParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_choice_auto_param.py,BetaToolChoiceAutoParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_choice_tool_param.py,BetaToolChoiceToolParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_computer_use_20241022_param.py,BetaToolComputerUse20241022Param,,0,['TypedDict'],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_param.py,InputSchemaTyped,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_param.py,BetaToolParam,,0,['TypedDict'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_result_block_param.py,BetaToolResultBlockParam,,0,['TypedDict'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_text_editor_20241022_param.py,BetaToolTextEditor20241022Param,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_use_block.py,BetaToolUseBlock,,0,['BaseModel'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_tool_use_block_param.py,BetaToolUseBlockParam,,0,['TypedDict'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\beta_usage.py,BetaUsage,,0,['BaseModel'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\message_count_tokens_params.py,MessageCountTokensParams,,0,['TypedDict'],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\message_create_params.py,MessageCreateParamsBase,,0,['TypedDict'],11,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\message_create_params.py,MessageCreateParamsNonStreaming,,0,['MessageCreateParamsBase'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\message_create_params.py,MessageCreateParamsStreaming,,0,['MessageCreateParamsBase'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\batch_create_params.py,BatchCreateParams,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\batch_create_params.py,Request,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\batch_list_params.py,BatchListParams,,0,['TypedDict'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\beta_message_batch.py,BetaMessageBatch,,0,['BaseModel'],10,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\beta_message_batch_canceled_result.py,BetaMessageBatchCanceledResult,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\beta_message_batch_errored_result.py,BetaMessageBatchErroredResult,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\beta_message_batch_expired_result.py,BetaMessageBatchExpiredResult,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\beta_message_batch_individual_response.py,BetaMessageBatchIndividualResponse,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\beta_message_batch_request_counts.py,BetaMessageBatchRequestCounts,,0,['BaseModel'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\messages\beta_message_batch_succeeded_result.py,BetaMessageBatchSucceededResult,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\message_create_params.py,MessageCreateParamsBase,,0,['TypedDict'],11,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\message_create_params.py,MessageCreateParamsNonStreaming,,0,['MessageCreateParamsBase'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\message_create_params.py,MessageCreateParamsStreaming,,0,['MessageCreateParamsBase'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_cache_control_ephemeral_param.py,PromptCachingBetaCacheControlEphemeralParam,,0,['TypedDict'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_image_block_param.py,Source,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_image_block_param.py,PromptCachingBetaImageBlockParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_message.py,PromptCachingBetaMessage,,0,['BaseModel'],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_message_param.py,PromptCachingBetaMessageParam,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_text_block_param.py,PromptCachingBetaTextBlockParam,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_tool_param.py,InputSchemaTyped,,0,['TypedDict'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_tool_param.py,PromptCachingBetaToolParam,,0,['TypedDict'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_tool_result_block_param.py,PromptCachingBetaToolResultBlockParam,,0,['TypedDict'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_tool_use_block_param.py,PromptCachingBetaToolUseBlockParam,,0,['TypedDict'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\prompt_caching_beta_usage.py,PromptCachingBetaUsage,,0,['BaseModel'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anthropic\types\beta\prompt_caching\raw_prompt_caching_beta_message_start_event.py,RawPromptCachingBetaMessageStartEvent,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,UnreliableObjectReceiveStream,"An interface for receiving objects.

This interface makes no guarantees that the received messages arrive in the order in
which they were sent, or that no messages are missed.

Asynchronously iterating over objects of this type will yield objects matching the
given type parameter.",1,"['Generic[T_co]', 'AsyncResource', 'TypedAttributeProvider']",0,['__aiter__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,UnreliableObjectSendStream,"An interface for sending objects.

This interface makes no guarantees that the messages sent will reach the
recipient(s) in the same order in which they were sent, or at all.",0,"['Generic[T_contra]', 'AsyncResource', 'TypedAttributeProvider']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,UnreliableObjectStream,"A bidirectional message stream which does not guarantee the order or reliability of
message delivery.",0,"['UnreliableObjectReceiveStream[T_Item]', 'UnreliableObjectSendStream[T_Item]']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,ObjectReceiveStream,"A receive message stream which guarantees that messages are received in the same
order in which they were sent, and that no messages are missed.",0,['UnreliableObjectReceiveStream[T_co]'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,ObjectSendStream,"A send message stream which guarantees that messages are delivered in the same order
in which they were sent, without missing any messages in the middle.",0,['UnreliableObjectSendStream[T_contra]'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,ObjectStream,"A bidirectional message stream which guarantees the order and reliability of message
delivery.",0,"['ObjectReceiveStream[T_Item]', 'ObjectSendStream[T_Item]', 'UnreliableObjectStream[T_Item]']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,ByteReceiveStream,"An interface for receiving bytes from a single peer.

Iterating this byte stream will yield a byte string of arbitrary length, but no more
than 65536 bytes.",1,"['AsyncResource', 'TypedAttributeProvider']",0,['__aiter__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,ByteSendStream,An interface for sending bytes to a single peer.,0,"['AsyncResource', 'TypedAttributeProvider']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,ByteStream,A bidirectional byte stream.,0,"['ByteReceiveStream', 'ByteSendStream']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\abc\_streams.py,Listener,An interface for objects that let you accept incoming connections.,0,"['Generic[T_co]', 'AsyncResource', 'TypedAttributeProvider']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\streams\memory.py,MemoryObjectStreamStatistics,,0,['NamedTuple'],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\streams\memory.py,MemoryObjectItemReceiver,,1,['Generic[T_Item]'],2,['__repr__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\streams\memory.py,MemoryObjectStreamState,,1,['Generic[T_Item]'],6,['statistics'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\streams\memory.py,MemoryObjectReceiveStream,,8,"['Generic[T_co]', 'ObjectReceiveStream[T_co]']",2,"['__post_init__', 'receive_nowait', 'clone', 'close', 'statistics', '__enter__', '__exit__', '__del__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\streams\memory.py,MemoryObjectSendStream,,8,"['Generic[T_contra]', 'ObjectSendStream[T_contra]']",2,"['__post_init__', 'send_nowait', 'clone', 'close', 'statistics', '__enter__', '__exit__', '__del__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,BrokenResourceError,"Raised when trying to use a resource that has been rendered unusable due to external
causes (e.g. a send stream whose peer has disconnected).",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,BrokenWorkerProcess,"Raised by :func:`run_sync_in_process` if the worker process terminates abruptly or
otherwise misbehaves.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,BusyResourceError,"Raised when two tasks are trying to read from or write to the same resource
concurrently.",1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,ClosedResourceError,Raised when trying to use a resource that has been closed.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,DelimiterNotFound,"Raised during
:meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_until` if the
maximum number of bytes has been read without the delimiter being found.",1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,EndOfStream,Raised when trying to read from a stream that has been closed from the other end.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,IncompleteRead,"Raised during
:meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_exactly` or
:meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_until` if the
connection is closed before the requested amount of bytes has been read.",1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,TypedAttributeLookupError,"Raised by :meth:`~anyio.TypedAttributeProvider.extra` when the given typed attribute
is not found and no default value has been given.",0,['LookupError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_exceptions.py,WouldBlock,Raised by ``X_nowait`` functions if ``X()`` would block.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\anyio\_core\_streams.py,create_memory_object_stream,"Create a memory object stream.

The stream's item type can be annotated like
:func:`create_memory_object_stream[T_Item]`.

:param max_buffer_size: number of items held in the buffer until ``send()`` starts
    blocking
:param item_type: old way of marking the streams with the right generic type for
    static typing (does nothing on AnyIO 4)

    .. deprecated:: 4.0
      Use ``create_memory_object_stream[YourItemType](...)`` instead.
:return: a tuple of (send stream, receive stream)",1,"['tuple[MemoryObjectSendStream[T_Item], MemoryObjectReceiveStream[T_Item]]']",0,['__new__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\bandit\core\node_visitor.py,BanditNodeVisitor,,15,[],0,"['__init__', 'visit_ClassDef', 'visit_FunctionDef', 'visit_Call', 'visit_Import', 'visit_ImportFrom', 'visit_Constant', 'visit_Str', 'visit_Bytes', 'pre_visit', 'visit', 'post_visit', 'generic_visit', 'update_scores', 'process']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\bandit\core\utils.py,InvalidModulePath,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\bandit\core\utils.py,ConfigError,Raised when the config file fails validation.,1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\bandit\core\utils.py,ProfileNotFound,Raised when chosen profile cannot be found.,1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\bandit\plugins\django_xss.py,DeepAssignation,,3,[],0,"['__init__', 'is_assigned_in', 'is_assigned']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\colorama\tests\ansi_test.py,AnsiTest,,5,['TestCase'],0,"['setUp', 'tearDown', 'testForeAttributes', 'testBackAttributes', 'testStyleAttributes']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\colorama\tests\isatty_test.py,IsattyTest,,7,['TestCase'],0,"['test_TTY', 'test_nonTTY', 'test_withPycharm', 'test_withPycharmTTYOverride', 'test_withPycharmNonTTYOverride', 'test_withPycharmNoneOverride', 'test_withPycharmStreamWrapped']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\colorama\tests\utils.py,StreamTTY,,1,['StringIO'],0,['isatty'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\colorama\tests\utils.py,StreamNonTTY,,1,['StringIO'],0,['isatty'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\fsspec\implementations\cache_metadata.py,CacheMetadata,"Cache metadata.

All reading and writing of cache metadata is performed by this class,
accessing the cached files and blocks is not.

Metadata is stored in a single file per storage directory in JSON format.
For backward compatibility, also reads metadata stored in pickle format
which is converted to JSON when next saved.",11,[],0,"['__init__', '_load', '_save', '_scan_locations', 'check_file', 'clear_expired', 'load', 'on_close_cached_file', 'pop_file', 'save', 'update_file']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\functorch\dim\reference.py,isin,,2,[],0,"['__contains__', 'index']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\functorch\dim\reference.py,llist,,0,"['isin', 'list']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\functorch\dim\reference.py,ltuple,,0,"['isin', 'tuple']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\functorch\dim\reference.py,dim_tracker,,3,[],0,"['__init__', 'record', '__getitem__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\fail_clearing_run_switches.py,RunCallable,,1,[],0,['__del__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\fail_clearing_run_switches.py,G,,1,['greenlet.greenlet'],0,['__getattribute__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\test_contextvars.py,ContextVarsTests,,12,['TestCase'],0,"['_new_ctx_run', '_increment', '_test_context', 'test_context_propagated_by_context_run', 'test_context_propagated_by_setting_attribute', 'test_context_not_propagated', 'test_context_shared', 'test_break_ctxvars', 'test_not_broken_if_using_attribute_instead_of_context_run', 'test_context_assignment_while_running', 'test_context_assignment_different_thread', 'test_context_assignment_wrong_type']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\test_contextvars.py,NoContextVarsTests,,1,['TestCase'],0,['test_contextvars_errors'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\test_extension_interface.py,CAPITests,,11,['TestCase'],0,"['test_switch', 'test_switch_kwargs', 'test_setparent', 'test_getcurrent', 'test_new_greenlet', 'test_raise_greenlet_dead', 'test_raise_greenlet_error', 'test_throw', 'test_non_traceback_param', 'test_instance_of_wrong_type', 'test_not_throwable']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\test_greenlet_trash.py,TestTrashCanReEnter,,2,['unittest.TestCase'],0,"['test_it', 'check_it']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\test_stack_saved.py,Test,,1,['TestCase'],0,['test_stack_saved'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\test_throw.py,ThrowTests,,7,['TestCase'],0,"['test_class', 'test_val', 'test_kill', 'test_throw_goes_to_original_parent', 'test_non_traceback_param', 'test_instance_of_wrong_type', 'test_not_throwable']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\test_version.py,VersionTests,,1,['NonLeakingTestCase'],0,['test_version'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\greenlet\tests\test_weakref.py,WeakRefTests,,3,['TestCase'],0,"['test_dead_weakref', 'test_inactive_weakref', 'test_dealloc_weakref']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\tests\test_against_stdlib_http.py,SingleMindedRequestHandler,,1,['SimpleHTTPRequestHandler'],0,['translate_path'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\h11\tests\test_against_stdlib_http.py,H11RequestHandler,,1,['socketserver.BaseRequestHandler'],0,['handle'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_async\connection.py,AsyncHTTPConnection,,8,['AsyncConnectionInterface'],0,"['__init__', 'can_handle_request', 'is_available', 'has_expired', 'is_idle', 'is_closed', 'info', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_async\http_proxy.py,AsyncHTTPProxy,A connection pool that sends requests via an HTTP proxy.,2,['AsyncConnectionPool'],0,"['__init__', 'create_connection']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_async\http_proxy.py,AsyncForwardHTTPConnection,,8,['AsyncConnectionInterface'],0,"['__init__', 'can_handle_request', 'info', 'is_available', 'has_expired', 'is_idle', 'is_closed', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_async\http_proxy.py,AsyncTunnelHTTPConnection,,8,['AsyncConnectionInterface'],0,"['__init__', 'can_handle_request', 'info', 'is_available', 'has_expired', 'is_idle', 'is_closed', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_async\interfaces.py,AsyncRequestInterface,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_async\interfaces.py,AsyncConnectionInterface,,6,['AsyncRequestInterface'],0,"['info', 'can_handle_request', 'is_available', 'has_expired', 'is_idle', 'is_closed']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_async\socks_proxy.py,AsyncSOCKSProxy,A connection pool that sends requests via an HTTP proxy.,2,['AsyncConnectionPool'],0,"['__init__', 'create_connection']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_async\socks_proxy.py,AsyncSocks5Connection,,8,['AsyncConnectionInterface'],0,"['__init__', 'can_handle_request', 'is_available', 'has_expired', 'is_idle', 'is_closed', 'info', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\anyio.py,AnyIOStream,,2,['AsyncNetworkStream'],0,"['__init__', 'get_extra_info']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\anyio.py,AnyIOBackend,,0,['AsyncNetworkBackend'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\auto.py,AutoBackend,,0,['AsyncNetworkBackend'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\base.py,NetworkStream,,5,[],0,"['read', 'write', 'close', 'start_tls', 'get_extra_info']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\base.py,NetworkBackend,,3,[],0,"['connect_tcp', 'connect_unix_socket', 'sleep']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\base.py,AsyncNetworkStream,,1,[],0,['get_extra_info'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\base.py,AsyncNetworkBackend,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\mock.py,MockSSLObject,,2,[],0,"['__init__', 'selected_alpn_protocol']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\mock.py,MockStream,,7,['NetworkStream'],0,"['__init__', 'read', 'write', 'close', 'start_tls', 'get_extra_info', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\mock.py,MockBackend,,4,['NetworkBackend'],0,"['__init__', 'connect_tcp', 'connect_unix_socket', 'sleep']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\mock.py,AsyncMockStream,,3,['AsyncNetworkStream'],0,"['__init__', 'get_extra_info', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\mock.py,AsyncMockBackend,,1,['AsyncNetworkBackend'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\trio.py,TrioStream,,3,['AsyncNetworkStream'],0,"['__init__', 'get_extra_info', '_get_socket_stream']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_backends\trio.py,TrioBackend,,0,['AsyncNetworkBackend'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_sync\connection.py,HTTPConnection,,13,['ConnectionInterface'],0,"['__init__', 'handle_request', '_connect', 'can_handle_request', 'close', 'is_available', 'has_expired', 'is_idle', 'is_closed', 'info', '__repr__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_sync\http_proxy.py,HTTPProxy,A connection pool that sends requests via an HTTP proxy.,2,['ConnectionPool'],0,"['__init__', 'create_connection']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_sync\http_proxy.py,ForwardHTTPConnection,,10,['ConnectionInterface'],0,"['__init__', 'handle_request', 'can_handle_request', 'close', 'info', 'is_available', 'has_expired', 'is_idle', 'is_closed', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_sync\http_proxy.py,TunnelHTTPConnection,,10,['ConnectionInterface'],0,"['__init__', 'handle_request', 'can_handle_request', 'close', 'info', 'is_available', 'has_expired', 'is_idle', 'is_closed', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_sync\socks_proxy.py,SOCKSProxy,A connection pool that sends requests via an HTTP proxy.,2,['ConnectionPool'],0,"['__init__', 'create_connection']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpcore\_sync\socks_proxy.py,Socks5Connection,,10,['ConnectionInterface'],0,"['__init__', 'handle_request', 'can_handle_request', 'close', 'is_available', 'has_expired', 'is_idle', 'is_closed', 'info', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\asgi.py,ASGIResponseStream,,1,['AsyncByteStream'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\asgi.py,ASGITransport,"A custom AsyncTransport that handles sending requests directly to an ASGI app.

```python
transport = httpx.ASGITransport(
    app=app,
    root_path=""/submount"",
    client=(""1.2.3.4"", 123)
)
client = httpx.AsyncClient(transport=transport)
```

Arguments:

* `app` - The ASGI application.
* `raise_app_exceptions` - Boolean indicating if exceptions in the application
   should be raised. Default to `True`. Can be set to `False` for use cases
   such as testing the content of a client 500 response.
* `root_path` - The root path on which the ASGI application should be mounted.
* `client` - A two-tuple indicating the client IP and port of incoming requests.
```",1,['AsyncBaseTransport'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\base.py,BaseTransport,,4,[],0,"['__enter__', '__exit__', 'handle_request', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\base.py,AsyncBaseTransport,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\default.py,ResponseStream,,3,['SyncByteStream'],0,"['__init__', '__iter__', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\default.py,HTTPTransport,,5,['BaseTransport'],0,"['__init__', '__enter__', '__exit__', 'handle_request', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\default.py,AsyncResponseStream,,1,['AsyncByteStream'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\default.py,AsyncHTTPTransport,,1,['AsyncBaseTransport'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\mock.py,MockTransport,,2,"['AsyncBaseTransport', 'BaseTransport']",0,"['__init__', 'handle_request']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\wsgi.py,WSGIByteStream,,3,['SyncByteStream'],0,"['__init__', '__iter__', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\httpx\_transports\wsgi.py,WSGITransport,"A custom transport that handles sending requests directly to an WSGI app.
The simplest way to use this functionality is to use the `app` argument.

```
client = httpx.Client(app=app)
```

Alternatively, you can setup the transport instance explicitly.
This allows you to include any additional configuration arguments specific
to the WSGITransport class:

```
transport = httpx.WSGITransport(
    app=app,
    script_name=""/submount"",
    remote_addr=""1.2.3.4""
)
client = httpx.Client(transport=transport)
```

Arguments:

* `app` - The WSGI application.
* `raise_app_exceptions` - Boolean indicating if exceptions in the application
   should be raised. Default to `True`. Can be set to `False` for use cases
   such as testing the content of a client 500 response.
* `script_name` - The root path on which the WSGI application should be mounted.
* `remote_addr` - A string indicating the client IP of incoming requests.
```",2,['BaseTransport'],0,"['__init__', 'handle_request']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\agents\tools.py,InvalidTool,Tool that is run when invalid tool name is encountered by agent.,1,['BaseTool'],2,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\callbacks\streaming_aiter_final_only.py,AsyncFinalIteratorCallbackHandler,"Callback handler that returns an async iterator.
Only the final output of the agent will be iterated.",3,['AsyncIteratorCallbackHandler'],0,"['append_to_last_tokens', 'check_if_answer_reached', '__init__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\callbacks\streaming_stdout_final_only.py,FinalStreamingStdOutCallbackHandler,"Callback handler for streaming in agents.
Only works with agents using LLMs that support streaming.

Only the final output of the agent will be streamed.",5,['StreamingStdOutCallbackHandler'],0,"['append_to_last_tokens', 'check_if_answer_reached', '__init__', 'on_llm_start', 'on_llm_new_token']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\memory\chat_memory.py,BaseChatMemory,"Abstract base class for chat memory.

**ATTENTION** This abstraction was created prior to when chat models had
    native tool calling capabilities.
    It does **NOT** support native tool calling capabilities for chat models and
    will fail SILENTLY if used with a chat model that has native tool calling.

DO NOT USE THIS ABSTRACTION FOR NEW CODE.",3,"['BaseMemory', 'ABC']",4,"['_get_input_output', 'save_context', 'clear']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\retrievers\merger_retriever.py,MergerRetriever,Retriever that merges the results of multiple retrievers.,2,['BaseRetriever'],1,"['_get_relevant_documents', 'merge_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\retrievers\parent_document_retriever.py,ParentDocumentRetriever,"Retrieve small chunks then retrieve their parent documents.

When splitting documents for retrieval, there are often conflicting desires:

1. You may want to have small documents, so that their embeddings can most
    accurately reflect their meaning. If too long, then the embeddings can
    lose meaning.
2. You want to have long enough documents that the context of each chunk is
    retained.

The ParentDocumentRetriever strikes that balance by splitting and storing
small chunks of data. During retrieval, it first fetches the small chunks
but then looks up the parent ids for those chunks and returns those larger
documents.

Note that ""parent document"" refers to the document that a small chunk
originated from. This can either be the whole raw document OR a larger
chunk.

Examples:

    .. code-block:: python

        from langchain_chroma import Chroma
        from langchain_community.embeddings import OpenAIEmbeddings
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        from langchain.storage import InMemoryStore

        # This text splitter is used to create the parent documents
        parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)
        # This text splitter is used to create the child documents
        # It should create documents smaller than the parent
        child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)
        # The vectorstore to use to index the child chunks
        vectorstore = Chroma(embedding_function=OpenAIEmbeddings())
        # The storage layer for the parent documents
        store = InMemoryStore()

        # Initialize the retriever
        retriever = ParentDocumentRetriever(
            vectorstore=vectorstore,
            docstore=store,
            child_splitter=child_splitter,
            parent_splitter=parent_splitter,
        )",2,['MultiVectorRetriever'],3,"['_split_docs_for_adding', 'add_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\runnables\hub.py,HubRunnable,An instance of a runnable stored in the LangChain Hub.,1,"['RunnableBindingBase[Input, Output]']",1,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\runnables\openai_functions.py,OpenAIFunction,A function description for ChatOpenAI,0,['TypedDict'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\runnables\openai_functions.py,OpenAIFunctionsRouter,A runnable that routes to the selected function.,1,"['RunnableBindingBase[BaseMessage, Any]']",1,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\storage\encoder_backed.py,EncoderBackedStore,"Wraps a store with key and value encoders/decoders.

Examples that uses JSON for encoding/decoding:

.. code-block:: python

    import json

    def key_encoder(key: int) -> str:
        return json.dumps(key)

    def value_serializer(value: float) -> str:
        return json.dumps(value)

    def value_deserializer(serialized_value: str) -> float:
        return json.loads(serialized_value)

    # Create an instance of the abstract store
    abstract_store = MyCustomStore()

    # Create an instance of the encoder-backed store
    store = EncoderBackedStore(
        store=abstract_store,
        key_encoder=key_encoder,
        value_serializer=value_serializer,
        value_deserializer=value_deserializer
    )

    # Use the encoder-backed store methods
    store.mset([(1, 3.14), (2, 2.718)])
    values = store.mget([1, 2])  # Retrieves [3.14, 2.718]
    store.mdelete([1, 2])  # Deletes the keys 1 and 2",5,"['BaseStore[K, V]']",0,"['__init__', 'mget', 'mset', 'mdelete', 'yield_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\storage\file_system.py,LocalFileStore,"BaseStore interface that works on the local file system.

Examples:
    Create a LocalFileStore instance and perform operations on it:

    .. code-block:: python

        from langchain.storage import LocalFileStore

        # Instantiate the LocalFileStore with the root path
        file_store = LocalFileStore(""/path/to/root"")

        # Set values for keys
        file_store.mset([(""key1"", b""value1""), (""key2"", b""value2"")])

        # Get values for keys
        values = file_store.mget([""key1"", ""key2""])  # Returns [b""value1"", b""value2""]

        # Delete keys
        file_store.mdelete([""key1""])

        # Iterate over keys
        for key in file_store.yield_keys():
            print(key)  # noqa: T201",7,['ByteStore'],0,"['__init__', '_get_full_path', '_mkdir_for_store', 'mget', 'mset', 'mdelete', 'yield_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\callbacks\tracers\logging.py,LoggingCallbackHandler,Tracer that logs via the input Logger.,2,['FunctionCallbackHandler'],1,"['__init__', 'on_text']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\constitutional_ai\models.py,ConstitutionalPrinciple,Class for a constitutional principle.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\flare\prompts.py,FinishedOutputParser,Output parser that checks if the output is finished.,1,"['BaseOutputParser[Tuple[str, bool]]']",1,['parse'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\natbot\crawler.py,ElementInViewPort,A typed dictionary containing information about elements in the viewport.,0,['TypedDict'],10,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\natbot\crawler.py,Crawler,"A crawler for web pages.

**Security Note**: This is an implementation of a crawler that uses a browser via
    Playwright.

    This crawler can be used to load arbitrary webpages INCLUDING content
    from the local file system.

    Control access to who can submit crawling requests and what network access
    the crawler has.

    Make sure to scope permissions to the minimal permissions necessary for
    the application.

    See https://python.langchain.com/docs/security for more information.",7,[],0,"['__init__', 'go_to_page', 'scroll', 'click', 'type', 'enter', 'crawl']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\openai_functions\citation_fuzzy_match.py,FactWithEvidence,"Class representing a single statement.

Each fact has a body and a list of sources.
If there are multiple facts make sure to break them apart
such that each one only uses a set of sources that are relevant to it.",2,['BaseModel'],2,"['_get_span', 'get_spans']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\openai_functions\citation_fuzzy_match.py,QuestionAnswer,"A question and its answer as a list of facts each one should have a source.
each sentence contains a body and a list of sources.",0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\openai_functions\qa_with_structure.py,AnswerWithSources,"An answer to the question, with sources.",0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\qa_with_sources\loading.py,LoadingCallable,Interface for loading the combine documents chain.,1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\query_constructor\parser.py,ISO8601Date,A date in ISO 8601 format (YYYY-MM-DD).,0,['TypedDict'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\query_constructor\parser.py,ISO8601DateTime,A datetime in ISO 8601 format (YYYY-MM-DDTHH:MM:SS).,0,['TypedDict'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\query_constructor\parser.py,QueryTransformer,Transform a query string into an intermediate representation.,13,['Transformer'],0,"['__init__', 'program', 'func_call', '_match_func_name', 'args', 'false', 'true', 'list', 'int', 'float', 'date', 'datetime', 'string']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\question_answering\chain.py,LoadingCallable,Interface for loading the combine documents chain.,1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\sql_database\query.py,SQLInput,Input for a SQL Chain.,0,['TypedDict'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\sql_database\query.py,SQLInputWithTables,Input for a SQL Chain.,0,['TypedDict'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\chains\summarize\chain.py,LoadingCallable,Interface for loading the combine documents chain.,1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain\smith\evaluation\progress.py,ProgressBarCallback,A simple progress bar for the console.,11,['base_callbacks.BaseCallbackHandler'],0,"['__init__', 'increment', '_print_bar', 'on_chain_error', 'on_chain_end', 'on_retriever_error', 'on_retriever_end', 'on_llm_error', 'on_llm_end', 'on_tool_error', 'on_tool_end']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\azure_ai_services.py,AzureAiServicesToolkit,Toolkit for Azure AI Services.,1,['BaseToolkit'],0,['get_tools'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\azure_cognitive_services.py,AzureCognitiveServicesToolkit,Toolkit for Azure Cognitive Services.,1,['BaseToolkit'],0,['get_tools'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\argilla_callback.py,ArgillaCallbackHandler,"Callback Handler that logs into Argilla.

Args:
    dataset_name: name of the `FeedbackDataset` in Argilla. Note that it must
        exist in advance. If you need help on how to create a `FeedbackDataset` in
        Argilla, please visit
        https://docs.argilla.io/en/latest/tutorials_and_integrations/integrations/use_argilla_callback_in_langchain.html.
    workspace_name: name of the workspace in Argilla where the specified
        `FeedbackDataset` lives in. Defaults to `None`, which means that the
        default workspace will be used.
    api_url: URL of the Argilla Server that we want to use, and where the
        `FeedbackDataset` lives in. Defaults to `None`, which means that either
        `ARGILLA_API_URL` environment variable or the default will be used.
    api_key: API Key to connect to the Argilla Server. Defaults to `None`, which
        means that either `ARGILLA_API_KEY` environment variable or the default
        will be used.

Raises:
    ImportError: if the `argilla` package is not installed.
    ConnectionError: if the connection to Argilla fails.
    FileNotFoundError: if the `FeedbackDataset` retrieval from Argilla fails.

Examples:
    >>> from langchain_community.llms import OpenAI
    >>> from langchain_community.callbacks import ArgillaCallbackHandler
    >>> argilla_callback = ArgillaCallbackHandler(
    ...     dataset_name=""my-dataset"",
    ...     workspace_name=""my-workspace"",
    ...     api_url=""http://localhost:6900"",
    ...     api_key=""argilla.apikey"",
    ... )
    >>> llm = OpenAI(
    ...     temperature=0,
    ...     callbacks=[argilla_callback],
    ...     verbose=True,
    ...     openai_api_key=""API_KEY_HERE"",
    ... )
    >>> llm.generate([
    ...     ""What is the best NLP-annotation tool out there? (no bias at all)"",
    ... ])
    ""Argilla, no doubt about it.""",14,['BaseCallbackHandler'],4,"['__init__', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_agent_action', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\arize_callback.py,ArizeCallbackHandler,Callback Handler that logs to Arize.,14,['BaseCallbackHandler'],0,"['__init__', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_agent_action', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\comet_ml_callback.py,CometCallbackHandler,"Callback Handler that logs to Comet.

Parameters:
    job_type (str): The type of comet_ml task such as ""inference"",
        ""testing"" or ""qc""
    project_name (str): The comet_ml project name
    tags (list): Tags to add to the task
    task_name (str): Name of the comet_ml task
    visualize (bool): Whether to visualize the run.
    complexity_metrics (bool): Whether to log complexity metrics
    stream_logs (bool): Whether to stream callback actions to Comet

This handler will utilize the associated callback method and formats
the input of each callback function with metadata regarding the state of LLM run,
and adds the response to the list of records for both the {method}_records and
action. It then logs the response to Comet.",26,"['BaseMetadataCallbackHandler', 'BaseCallbackHandler']",0,"['__init__', '_init_resp', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish', 'on_agent_action', '_get_complexity_metrics', '_get_custom_metrics', 'flush_tracker', '_log_stream', '_log_model', '_log_session', '_log_text_metrics', '_log_visualizations', '_reset', '_create_session_analysis_dataframe', '_get_llm_parameters']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\confident_callback.py,DeepEvalCallbackHandler,"Callback Handler that logs into deepeval.

Args:
    implementation_name: name of the `implementation` in deepeval
    metrics: A list of metrics

Raises:
    ImportError: if the `deepeval` package is not installed.

Examples:
    >>> from langchain_community.llms import OpenAI
    >>> from langchain_community.callbacks import DeepEvalCallbackHandler
    >>> from deepeval.metrics import AnswerRelevancy
    >>> metric = AnswerRelevancy(minimum_score=0.3)
    >>> deepeval_callback = DeepEvalCallbackHandler(
    ...     implementation_name=""exampleImplementation"",
    ...     metrics=[metric],
    ... )
    >>> llm = OpenAI(
    ...     temperature=0,
    ...     callbacks=[deepeval_callback],
    ...     verbose=True,
    ...     openai_api_key=""API_KEY_HERE"",
    ... )
    >>> llm.generate([
    ...     ""What is the best evaluation tool out there? (no bias at all)"",
    ... ])
    ""Deepeval, no doubt about it.""",14,['BaseCallbackHandler'],3,"['__init__', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_agent_action', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\context_callback.py,ContextCallbackHandler,"Callback Handler that records transcripts to the Context service.

 (https://context.ai).

Keyword Args:
    token (optional): The token with which to authenticate requests to Context.
        Visit https://with.context.ai/settings to generate a token.
        If not provided, the value of the `CONTEXT_TOKEN` environment
        variable will be used.

Raises:
    ImportError: if the `context-python` package is not installed.

Chat Example:
    >>> from langchain_community.llms import ChatOpenAI
    >>> from langchain_community.callbacks import ContextCallbackHandler
    >>> context_callback = ContextCallbackHandler(
    ...     token=""<CONTEXT_TOKEN_HERE>"",
    ... )
    >>> chat = ChatOpenAI(
    ...     temperature=0,
    ...     headers={""user_id"": ""123""},
    ...     callbacks=[context_callback],
    ...     openai_api_key=""API_KEY_HERE"",
    ... )
    >>> messages = [
    ...     SystemMessage(content=""You translate English to French.""),
    ...     HumanMessage(content=""I love programming with LangChain.""),
    ... ]
    >>> chat.invoke(messages)

Chain Example:
    >>> from langchain.chains import LLMChain
    >>> from langchain_community.chat_models import ChatOpenAI
    >>> from langchain_community.callbacks import ContextCallbackHandler
    >>> context_callback = ContextCallbackHandler(
    ...     token=""<CONTEXT_TOKEN_HERE>"",
    ... )
    >>> human_message_prompt = HumanMessagePromptTemplate(
    ...     prompt=PromptTemplate(
    ...         template=""What is a good name for a company that makes {product}?"",
    ...         input_variables=[""product""],
    ...    ),
    ... )
    >>> chat_prompt_template = ChatPromptTemplate.from_messages(
    ...   [human_message_prompt]
    ... )
    >>> callback = ContextCallbackHandler(token)
    >>> # Note: the same callback object must be shared between the
    ...   LLM and the chain.
    >>> chat = ChatOpenAI(temperature=0.9, callbacks=[callback])
    >>> chain = LLMChain(
    ...   llm=chat,
    ...   prompt=chat_prompt_template,
    ...   callbacks=[callback]
    ... )
    >>> chain.run(""colorful socks"")",6,['BaseCallbackHandler'],0,"['__init__', 'on_chat_model_start', 'on_llm_end', 'on_chain_start', 'on_chain_end', '_log_conversation']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\flyte_callback.py,FlyteCallbackHandler,Callback handler that is used within a Flyte task.,14,"['BaseMetadataCallbackHandler', 'BaseCallbackHandler']",0,"['__init__', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish', 'on_agent_action']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\human.py,HumanRejectedException,Exception to raise when a person manually review and rejects a value.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\human.py,HumanApprovalCallbackHandler,Callback for manually validating values.,2,['BaseCallbackHandler'],1,"['__init__', 'on_tool_start']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\human.py,AsyncHumanApprovalCallbackHandler,Asynchronous callback for manually validating values.,1,['AsyncCallbackHandler'],1,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\infino_callback.py,InfinoCallbackHandler,Callback Handler that logs to Infino.,16,['BaseCallbackHandler'],0,"['__init__', '_send_to_infino', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_agent_action', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish', 'on_chat_model_start']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\llmonitor_callback.py,UserContextManager,Context manager for LLMonitor user context.,3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\llmonitor_callback.py,LLMonitorCallbackHandler,"Callback Handler for LLMonitor`.

#### Parameters:
    - `app_id`: The app id of the app you want to report to. Defaults to
    `None`, which means that `LLMONITOR_APP_ID` will be used.
    - `api_url`: The url of the LLMonitor API. Defaults to `None`,
    which means that either `LLMONITOR_API_URL` environment variable
    or `https://app.llmonitor.com` will be used.

#### Raises:
    - `ValueError`: if `app_id` is not provided either as an
    argument or as an environment variable.
    - `ConnectionError`: if the connection to the API fails.


#### Example:
```python
from langchain_community.llms import OpenAI
from langchain_community.callbacks import LLMonitorCallbackHandler

llmonitor_callback = LLMonitorCallbackHandler()
llm = OpenAI(callbacks=[llmonitor_callback],
             metadata={""userId"": ""user-123""})
llm.invoke(""Hello, how are you?"")
```",13,['BaseCallbackHandler'],5,"['__init__', 'on_llm_start', 'on_chat_model_start', 'on_llm_end', 'on_tool_start', 'on_tool_end', 'on_chain_start', 'on_chain_end', 'on_agent_action', 'on_agent_finish', 'on_chain_error', 'on_tool_error', 'on_llm_error']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\mlflow_callback.py,MlflowLogger,"Callback Handler that logs metrics and artifacts to mlflow server.

Parameters:
    name (str): Name of the run.
    experiment (str): Name of the experiment.
    tags (dict): Tags to be attached for the run.
    tracking_uri (str): MLflow tracking server uri.

This handler implements the helper functions to initialize,
log metrics and artifacts to the mlflow server.",11,[],0,"['__init__', 'start_run', 'finish_run', 'metric', 'metrics', 'jsonf', 'table', 'html', 'text', 'artifact', 'langchain_artifact']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\mlflow_callback.py,MlflowCallbackHandler,"Callback Handler that logs metrics and artifacts to mlflow server.

Parameters:
    name (str): Name of the run.
    experiment (str): Name of the experiment.
    tags (dict): Tags to be attached for the run.
    tracking_uri (str): MLflow tracking server uri.

This handler will utilize the associated callback method called and formats
the input of each callback function with metadata regarding the state of LLM run,
and adds the response to the list of records for both the {method}_records and
action. It then logs the response to mlflow server.",21,"['BaseMetadataCallbackHandler', 'BaseCallbackHandler']",0,"['__init__', '_reset', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish', 'on_agent_action', 'on_retriever_start', 'on_retriever_end', 'on_retriever_error', '_create_session_analysis_df', '_contain_llm_records', 'flush_tracker']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\promptlayer_callback.py,PromptLayerCallbackHandler,Callback handler for promptlayer.,6,['BaseCallbackHandler'],0,"['__init__', 'on_chat_model_start', 'on_llm_start', 'on_llm_end', '_convert_message_to_dict', '_create_message_dicts']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\sagemaker_callback.py,SageMakerCallbackHandler,"Callback Handler that logs prompt artifacts and metrics to SageMaker Experiments.

Parameters:
    run (sagemaker.experiments.run.Run): Run object where the experiment is logged.",17,['BaseCallbackHandler'],0,"['__init__', '_reset', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish', 'on_agent_action', 'jsonf', 'flush_tracker']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\trubrics_callback.py,TrubricsCallbackHandler,"Callback handler for Trubrics.

Args:
    project: a trubrics project, default project is ""default""
    email: a trubrics account email, can equally be set in env variables
    password: a trubrics account password, can equally be set in env variables
    **kwargs: all other kwargs are parsed and set to trubrics prompt variables,
        or added to the `metadata` dict",4,['BaseCallbackHandler'],0,"['__init__', 'on_llm_start', 'on_chat_model_start', 'on_llm_end']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\upstash_ratelimit_callback.py,UpstashRatelimitError,"Upstash Ratelimit Error

Raised when the rate limit is reached in `UpstashRatelimitHandler`",1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\upstash_ratelimit_callback.py,UpstashRatelimitHandler,"Callback to handle rate limiting based on the number of requests
or the number of tokens in the input.

It uses Upstash Ratelimit to track the ratelimit which utilizes
Upstash Redis to track the state.

Should not be passed to the chain when initialising the chain.
This is because the handler has a state which should be fresh
every time invoke is called. Instead, initialise and pass a handler
every time you invoke.",5,['BaseCallbackHandler'],2,"['__init__', 'on_chain_start', 'on_llm_start', 'on_llm_end', 'reset']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\uptrain_callback.py,UpTrainDataSchema,"The UpTrain data schema for tracking evaluation results.

Args:
    project_name (str): The project name to be shown in UpTrain dashboard.

Attributes:
    project_name (str): The project name to be shown in UpTrain dashboard.
    uptrain_results (DefaultDict[str, Any]): Dictionary to store evaluation results.
    eval_types (Set[str]): Set to store the types of evaluations.
    query (str): Query for the RAG evaluation.
    context (str): Context for the RAG evaluation.
    response (str): Response for the RAG evaluation.
    old_context (List[str]): Old context nodes for Context Conciseness evaluation.
    new_context (List[str]): New context nodes for Context Conciseness evaluation.
    context_conciseness_run_id (str): Run ID for Context Conciseness evaluation.
    multi_queries (List[str]): List of multi queries for Multi Query evaluation.
    multi_query_run_id (str): Run ID for Multi Query evaluation.
    multi_query_daugher_run_id (str): Run ID for Multi Query daughter evaluation.",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\uptrain_callback.py,UpTrainCallbackHandler,"Callback Handler that logs evaluation results to uptrain and the console.

Args:
    project_name (str): The project name to be shown in UpTrain dashboard.
    key_type (str): Type of key to use. Must be 'uptrain' or 'openai'.
    api_key (str): API key for the UpTrain or OpenAI API.
    (This key is required to perform evaluations using GPT.)

Raises:
    ValueError: If the key type is invalid.
    ImportError: If the `uptrain` package is not installed.",6,['BaseCallbackHandler'],0,"['__init__', 'uptrain_evaluate', 'on_llm_end', 'on_chain_start', 'on_retriever_start', 'on_retriever_end']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\wandb_callback.py,WandbCallbackHandler,"Callback Handler that logs to Weights and Biases.

Parameters:
    job_type (str): The type of job.
    project (str): The project to log to.
    entity (str): The entity to log to.
    tags (list): The tags to log.
    group (str): The group to log to.
    name (str): The name of the run.
    notes (str): The notes to log.
    visualize (bool): Whether to visualize the run.
    complexity_metrics (bool): Whether to log complexity metrics.
    stream_logs (bool): Whether to stream callback actions to W&B

This handler will utilize the associated callback method called and formats
the input of each callback function with metadata regarding the state of LLM run,
and adds the response to the list of records for both the {method}_records and
action. It then logs the response using the run.log() method to Weights and Biases.",17,"['BaseMetadataCallbackHandler', 'BaseCallbackHandler']",0,"['__init__', '_init_resp', 'on_llm_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish', 'on_agent_action', '_create_session_analysis_df', 'flush_tracker']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chat_loaders\facebook_messenger.py,SingleFileFacebookMessengerChatLoader,"Load `Facebook Messenger` chat data from a single file.

Args:
    path (Union[Path, str]): The path to the chat file.",2,['BaseChatLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chat_loaders\facebook_messenger.py,FolderFacebookMessengerChatLoader,"Load `Facebook Messenger` chat data from a folder.

Args:
    path (Union[str, Path]): The path to the directory
        containing the chat files.",2,['BaseChatLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chat_loaders\gmail.py,GMailLoader,"Load data from `GMail`.

There are many ways you could want to load data from GMail.
This loader is currently fairly opinionated in how to do so.
The way it does it is it first looks for all messages that you have sent.
It then looks for messages where you are responding to a previous email.
It then fetches that previous email, and creates a training example
of that email, followed by your email.

Note that there are clear limitations here. For example,
all examples created are only looking at the previous email for context.

To use:

- Set up a Google Developer Account:
    Go to the Google Developer Console, create a project,
    and enable the Gmail API for that project.
    This will give you a credentials.json file that you'll need later.",2,['BaseChatLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chat_message_histories\cosmos_db.py,CosmosDBChatMessageHistory,Chat message history backed by Azure CosmosDB.,8,['BaseChatMessageHistory'],0,"['__init__', 'prepare_cosmos', '__enter__', '__exit__', 'load_messages', 'add_message', 'upsert_messages', 'clear']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chat_message_histories\firestore.py,FirestoreChatMessageHistory,Chat message history backed by Google Firestore.,6,['BaseChatMessageHistory'],0,"['__init__', 'prepare_firestore', 'load_messages', 'add_message', 'upsert_messages', 'clear']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chat_models\gigachat.py,GigaChat,"`GigaChat` large language models API.

To use, you should pass login and password to access GigaChat API or use token.

Example:
    .. code-block:: python

        from langchain_community.chat_models import GigaChat
        giga = GigaChat(credentials=..., scope=..., verify_ssl_certs=False)",4,"['_BaseGigaChat', 'BaseChatModel']",0,"['_build_payload', '_create_chat_result', '_generate', '_stream']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chat_models\yandex.py,ChatYandexGPT,"YandexGPT large language models.

There are two authentication options for the service account
with the ``ai.languageModels.user`` role:
    - You can specify the token in a constructor parameter `iam_token`
    or in an environment variable `YC_IAM_TOKEN`.
    - You can specify the key in a constructor parameter `api_key`
    or in an environment variable `YC_API_KEY`.

Example:
    .. code-block:: python

        from langchain_community.chat_models import ChatYandexGPT
        chat_model = ChatYandexGPT(iam_token=""t1.9eu..."")",1,"['_BaseYandexGPT', 'BaseChatModel']",0,['_generate'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\cross_encoders\fake.py,FakeCrossEncoder,Fake cross encoder model.,1,"['BaseCrossEncoder', 'BaseModel']",0,['score'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\docstore\arbitrary_fn.py,DocstoreFn,"Docstore via arbitrary lookup function.

This is useful when:
 * it's expensive to construct an InMemoryDocstore/dict
 * you retrieve documents from remote sources
 * you just want to reuse existing objects",2,['Docstore'],0,"['__init__', 'search']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\docstore\in_memory.py,InMemoryDocstore,Simple in memory docstore in the form of a dict.,4,"['Docstore', 'AddableMixin']",0,"['__init__', 'add', 'delete', 'search']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\docstore\wikipedia.py,Wikipedia,Wikipedia API.,2,['Docstore'],0,"['__init__', 'search']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_compressors\openvino_rerank.py,RerankRequest,Request for reranking.,1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_compressors\openvino_rerank.py,OpenVINOReranker,OpenVINO rerank models.,4,['BaseDocumentCompressor'],5,"['__init__', 'rerank', 'compress_documents', 'save_model']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\acreom.py,AcreomLoader,Load `acreom` vault from a directory.,5,['BaseLoader'],1,"['__init__', '_parse_front_matter', '_remove_front_matter', '_process_acreom_content', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\airbyte_json.py,AirbyteJSONLoader,Load local `Airbyte` json files.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\airtable.py,AirtableLoader,Load the `Airtable` tables.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\arcgis_loader.py,ArcGISLoader,Load records from an ArcGIS FeatureLayer.,3,['BaseLoader'],0,"['__init__', '_get_layer_properties', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\arxiv.py,ArxivLoader,"Load a query result from `Arxiv`.
The loader converts the original PDF format into the text.

Setup:
    Install ``arxiv`` and ``PyMuPDF`` packages.
    ``PyMuPDF`` transforms PDF files downloaded from the arxiv.org site
    into the text format.

    .. code-block:: bash

        pip install -U arxiv pymupdf


Instantiate:
    .. code-block:: python

        from langchain_community.document_loaders import ArxivLoader

        loader = ArxivLoader(
            query=""reasoning"",
            # load_max_docs=2,
            # load_all_available_meta=False
        )

Load:
    .. code-block:: python

        docs = loader.load()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python
        Understanding the Reasoning Ability of Language Models
        From the Perspective of Reasoning Paths Aggre
        {
            'Published': '2024-02-29',
            'Title': 'Understanding the Reasoning Ability of Language Models From the
                    Perspective of Reasoning Paths Aggregation',
            'Authors': 'Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan,
                    Wenhu Chen, William Yang Wang',
            'Summary': 'Pre-trained language models (LMs) are able to perform complex reasoning
                    without explicit fine-tuning...'
        }


Lazy load:
    .. code-block:: python

        docs = []
        docs_lazy = loader.lazy_load()

        # async variant:
        # docs_lazy = await loader.alazy_load()

        for doc in docs_lazy:
            docs.append(doc)
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Understanding the Reasoning Ability of Language Models
        From the Perspective of Reasoning Paths Aggre
        {
            'Published': '2024-02-29',
            'Title': 'Understanding the Reasoning Ability of Language Models From the
                    Perspective of Reasoning Paths Aggregation',
            'Authors': 'Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan,
                    Wenhu Chen, William Yang Wang',
            'Summary': 'Pre-trained language models (LMs) are able to perform complex reasoning
                    without explicit fine-tuning...'
        }

Async load:
    .. code-block:: python

        docs = await loader.aload()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Understanding the Reasoning Ability of Language Models
        From the Perspective of Reasoning Paths Aggre
        {
            'Published': '2024-02-29',
            'Title': 'Understanding the Reasoning Ability of Language Models From the
                    Perspective of Reasoning Paths Aggregation',
            'Authors': 'Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan,
                    Wenhu Chen, William Yang Wang',
            'Summary': 'Pre-trained language models (LMs) are able to perform complex reasoning
                    without explicit fine-tuning...'
        }

Use summaries of articles as docs:
    .. code-block:: python

        from langchain_community.document_loaders import ArxivLoader

        loader = ArxivLoader(
            query=""reasoning""
        )

        docs = loader.get_summaries_as_docs()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Pre-trained language models (LMs) are able to perform complex reasoning
        without explicit fine-tuning
        {
            'Entry ID': 'http://arxiv.org/abs/2402.03268v2',
            'Published': datetime.date(2024, 2, 29),
            'Title': 'Understanding the Reasoning Ability of Language Models From the
                    Perspective of Reasoning Paths Aggregation',
            'Authors': 'Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan,
                    Wenhu Chen, William Yang Wang'
        }",3,['BaseLoader'],0,"['__init__', 'lazy_load', 'get_summaries_as_docs']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\astradb.py,AstraDBLoader,,2,['BaseLoader'],0,"['__init__', 'lazy_load']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\athena.py,AthenaLoader,"Load documents from `AWS Athena`.

Each document represents one row of the result.
- By default, all columns are written into the `page_content` of the document
and none into the `metadata` of the document.
- If `metadata_columns` are provided then these columns are written
into the `metadata` of the document while the rest of the columns
are written into the `page_content` of the document.

To authenticate, the AWS client uses this method to automatically load credentials:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html

If a specific credential profile should be used, you must pass
the name of the profile from the ~/.aws/credentials file that is to be used.

Make sure the credentials / roles used have the required policies to
access the Amazon Textract service.",7,['BaseLoader'],0,"['__init__', '_execute_query', '_remove_suffix', '_remove_prefix', '_get_result_set', '_get_columns', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\azlyrics.py,AZLyricsLoader,Load `AZLyrics` webpages.,1,['WebBaseLoader'],0,['load'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\azure_ai_data.py,AzureAIDataLoader,Load from Azure AI Data.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\azure_blob_storage_container.py,AzureBlobStorageContainerLoader,Load from `Azure Blob Storage` container.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\azure_blob_storage_file.py,AzureBlobStorageFileLoader,Load from `Azure Blob Storage` files.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\baiducloud_bos_directory.py,BaiduBOSDirectoryLoader,Load from `Baidu BOS directory`.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\baiducloud_bos_file.py,BaiduBOSFileLoader,Load from `Baidu Cloud BOS` file.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\bibtex.py,BibtexLoader,"Load a `bibtex` file.

Each document represents one entry from the bibtex file.

If a PDF file is present in the `file` bibtex field, the original PDF
is loaded into the document text. If no such file entry is present,
the `abstract` field is used instead.",3,['BaseLoader'],0,"['__init__', '_load_entry', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\bigquery.py,BigQueryLoader,"Load from the Google Cloud Platform `BigQuery`.

Each document represents one row of the result. The `page_content_columns`
are written into the `page_content` of the document. The `metadata_columns`
are written into the `metadata` of the document. By default, all columns
are written into the `page_content` and none into the `metadata`.",2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\bilibili.py,BiliBiliLoader,Load fetching transcripts from BiliBili videos.,3,['BaseLoader'],0,"['__init__', 'load', '_get_bilibili_subs_and_info']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\blackboard.py,BlackboardLoader,"Load a `Blackboard` course.

This loader is not compatible with all Blackboard courses. It is only
compatible with courses that use the new Blackboard interface.
To use this loader, you must have the BbRouter cookie. You can get this
cookie by logging into the course and then copying the value of the
BbRouter cookie from the browser's developer tools.

Example:
    .. code-block:: python

        from langchain_community.document_loaders import BlackboardLoader

        loader = BlackboardLoader(
            blackboard_course_url=""https://blackboard.example.com/webapps/blackboard/execute/announcement?method=search&context=course_entry&course_id=_123456_1"",
            bbrouter=""expires:12345..."",
        )
        documents = loader.load()",12,['WebBaseLoader'],0,"['__init__', 'check_bs4', 'load', '_get_folder_path', '_get_documents', '_get_attachments', '_download_attachments', '_load_documents', '_get_paths', 'download', 'parse_filename', '_parse_filename_from_url']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\brave_search.py,BraveSearchLoader,Load with `Brave Search` engine.,3,['BaseLoader'],0,"['__init__', 'load', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\browserbase.py,BrowserbaseLoader,"Load pre-rendered web pages using a headless browser hosted on Browserbase.

Depends on `browserbase` package.
Get your API key from https://browserbase.com",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\browserless.py,BrowserlessLoader,Load webpages with `Browserless` /content endpoint.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\cassandra.py,CassandraLoader,,2,['BaseLoader'],0,"['__init__', 'lazy_load']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\chatgpt.py,ChatGPTLoader,Load conversations from exported `ChatGPT` data.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\chromium.py,AsyncChromiumLoader,"Scrape HTML pages from URLs using a
headless instance of the Chromium.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\college_confidential.py,CollegeConfidentialLoader,Load `College Confidential` webpages.,1,['WebBaseLoader'],0,['load'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\conllu.py,CoNLLULoader,Load `CoNLL-U` files.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\couchbase.py,CouchbaseLoader,"Load documents from `Couchbase`.

Each document represents one row of the result. The `page_content_fields` are
written into the `page_content`of the document. The `metadata_fields` are written
into the `metadata` of the document. By default, all columns are written into
the `page_content` and none into the `metadata`.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\csv_loader.py,CSVLoader,"Load a `CSV` file into a list of Documents.

Each document represents one row of the CSV file. Every row is converted
into a key/value pair and outputted to a new line in the document's
page_content.

The source for each document loaded from csv is set to the value of the
`file_path` argument for all documents by default.
You can override this by setting the `source_column` argument to the
name of a column in the CSV file.
The source of each document will then be set to the value of the column
with the name specified in `source_column`.

Output Example:
    .. code-block:: txt

        column1: value1
        column2: value2
        column3: value3

Instantiate:
    .. code-block:: python

        from langchain_community.document_loaders import CSVLoader

        loader = CSVLoader(file_path='./hw_200.csv',
            csv_args={
            'delimiter': ',',
            'quotechar': '""',
            'fieldnames': ['Index', 'Height', 'Weight']
        })

Load:
    .. code-block:: python

        docs = loader.load()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Index: Index
        Height: Height(Inches)""
        Weight: ""Weight(Pounds)""
        {'source': './hw_200.csv', 'row': 0}

Async load:
    .. code-block:: python

        docs = await loader.aload()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Index: Index
        Height: Height(Inches)""
        Weight: ""Weight(Pounds)""
        {'source': './hw_200.csv', 'row': 0}

Lazy load:
    .. code-block:: python

        docs = []
        docs_lazy = loader.lazy_load()

        # async variant:
        # docs_lazy = await loader.alazy_load()

        for doc in docs_lazy:
            docs.append(doc)
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Index: Index
        Height: Height(Inches)""
        Weight: ""Weight(Pounds)""
        {'source': './hw_200.csv', 'row': 0}",3,['BaseLoader'],0,"['__init__', 'lazy_load', '__read_file']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\csv_loader.py,UnstructuredCSVLoader,"Load `CSV` files using `Unstructured`.

Like other
Unstructured loaders, UnstructuredCSVLoader can be used in both
""single"" and ""elements"" mode. If you use the loader in ""elements""
mode, the CSV file will be a single Unstructured Table element.
If you use the loader in ""elements"" mode, an HTML representation
of the table will be available in the ""text_as_html"" key in the
document metadata.

Examples
--------
from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader

loader = UnstructuredCSVLoader(""stanley-cups.csv"", mode=""elements"")
docs = loader.load()",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\cube_semantic.py,CubeSemanticLoader,"Load `Cube semantic layer` metadata.

Args:
    cube_api_url: REST API endpoint.
        Use the REST API of your Cube's deployment.
        Please find out more information here:
        https://cube.dev/docs/http-api/rest#configuration-base-path
    cube_api_token: Cube API token.
        Authentication tokens are generated based on your Cube's API secret.
        Please find out more information here:
        https://cube.dev/docs/security#generating-json-web-tokens-jwt
    load_dimension_values: Whether to load dimension values for every string
        dimension or not.
    dimension_values_limit: Maximum number of dimension values to load.
    dimension_values_max_retries: Maximum number of retries to load dimension
        values.
    dimension_values_retry_delay: Delay between retries to load dimension values.",3,['BaseLoader'],0,"['__init__', '_get_dimension_values', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\datadog_logs.py,DatadogLogsLoader,"Load `Datadog` logs.

Logs are written into the `page_content` and into the `metadata`.",3,['BaseLoader'],0,"['__init__', 'parse_log', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\dataframe.py,BaseDataFrameLoader,,2,['BaseLoader'],0,"['__init__', 'lazy_load']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\dataframe.py,DataFrameLoader,Load `Pandas` DataFrame.,1,['BaseDataFrameLoader'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\diffbot.py,DiffbotLoader,Load `Diffbot` json file.,4,['BaseLoader'],0,"['__init__', '_diffbot_api_url', '_get_diffbot_data', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\directory.py,DirectoryLoader,Load from a directory.,5,['BaseLoader'],0,"['__init__', 'load', 'lazy_load', '_lazy_load_file_to_non_generator', '_lazy_load_file']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\discord.py,DiscordChatLoader,Load `Discord` chat logs.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\docusaurus.py,DocusaurusLoader,"Load from Docusaurus Documentation.

It leverages the SitemapLoader to loop through the generated pages of a
Docusaurus Documentation website and extracts the content by looking for specific
HTML tags. By default, the parser searches for the main content of the Docusaurus
page, which is normally the <article>. You can also define your own
custom HTML tags by providing them as a list, for example: [""div"", "".main"", ""a""].",2,['SitemapLoader'],0,"['__init__', '_parsing_function']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\doc_intelligence.py,AzureAIDocumentIntelligenceLoader,Load a PDF with Azure Document Intelligence.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\duckdb_loader.py,DuckDBLoader,"Load from `DuckDB`.

Each document represents one row of the result. The `page_content_columns`
are written into the `page_content` of the document. The `metadata_columns`
are written into the `metadata` of the document. By default, all columns
are written into the `page_content` and none into the `metadata`.",2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\email.py,UnstructuredEmailLoader,"Load email files using `Unstructured`.

Works with both
.eml and .msg files. You can process attachments in addition to the
e-mail message itself by passing process_attachments=True into the
constructor for the loader. By default, attachments will be processed
with the unstructured partition function. If you already know the document
types of the attachments, you can specify another partitioning function
with the attachment partitioner kwarg.

Example
-------
from langchain_community.document_loaders import UnstructuredEmailLoader

loader = UnstructuredEmailLoader(""example_data/fake-email.eml"", mode=""elements"")
loader.load()

Example
-------
from langchain_community.document_loaders import UnstructuredEmailLoader

loader = UnstructuredEmailLoader(
    ""example_data/fake-email-attachment.eml"",
    mode=""elements"",
    process_attachments=True,
)
loader.load()",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\email.py,OutlookMessageLoader,"Loads Outlook Message files using extract_msg.

https://github.com/TeamMsgExtractor/msg-extractor",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\epub.py,UnstructuredEPubLoader,"Load `EPub` files using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredEPubLoader

loader = UnstructuredEPubLoader(
    ""example.epub"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-epub",1,['UnstructuredFileLoader'],0,['_get_elements'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\etherscan.py,EtherscanLoader,"Load transactions from `Ethereum` mainnet.

The Loader use Etherscan API to interact with Ethereum mainnet.

ETHERSCAN_API_KEY environment variable must be set use this loader.",8,['BaseLoader'],0,"['__init__', 'lazy_load', 'getNormTx', 'getEthBalance', 'getInternalTx', 'getERC20Tx', 'getERC721Tx', 'getERC1155Tx']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\excel.py,UnstructuredExcelLoader,"Load Microsoft Excel files using `Unstructured`.

Like other
Unstructured loaders, UnstructuredExcelLoader can be used in both
""single"" and ""elements"" mode. If you use the loader in ""elements""
mode, each sheet in the Excel file will be an Unstructured Table
element. If you use the loader in ""single"" mode, an
HTML representation of the table will be available in the
""text_as_html"" key in the document metadata.

Examples
--------
from langchain_community.document_loaders.excel import UnstructuredExcelLoader

loader = UnstructuredExcelLoader(""stanley-cups.xlsx"", mode=""elements"")
docs = loader.load()",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\facebook_chat.py,FacebookChatLoader,Load `Facebook Chat` messages directory dump.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\fauna.py,FaunaLoader,"Load from `FaunaDB`.

Attributes:
    query (str): The FQL query string to execute.
    page_content_field (str): The field that contains the content of each page.
    secret (str): The secret key for authenticating to FaunaDB.
    metadata_fields (Optional[Sequence[str]]):
        Optional list of field names to include in metadata.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\figma.py,FigmaFileLoader,Load `Figma` file.,4,['BaseLoader'],0,"['__init__', '_construct_figma_api_url', '_get_figma_file', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\firecrawl.py,FireCrawlLoader,"FireCrawlLoader document loader integration

Setup:
    Install ``firecrawl-py``,``langchain_community`` and set environment variable ``FIRECRAWL_API_KEY``.

    .. code-block:: bash

        pip install -U firecrawl-py langchain_community
        export FIRECRAWL_API_KEY=""your-api-key""

Instantiate:
    .. code-block:: python

        from langchain_community.document_loaders import FireCrawlLoader

        loader = FireCrawlLoader(
            url = ""https://firecrawl.dev"",
            mode = ""crawl""
            # other params = ...
        )

Lazy load:
    .. code-block:: python

        docs = []
        docs_lazy = loader.lazy_load()

        # async variant:
        # docs_lazy = await loader.alazy_load()

        for doc in docs_lazy:
            docs.append(doc)
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Introducing [Smart Crawl!](https://www.firecrawl.dev/smart-crawl)
         Join the waitlist to turn any web
        {'ogUrl': 'https://www.firecrawl.dev/', 'title': 'Home - Firecrawl', 'robots': 'follow, index', 'ogImage': 'https://www.firecrawl.dev/og.png?123', 'ogTitle': 'Firecrawl', 'sitemap': {'lastmod': '2024-08-12T00:28:16.681Z', 'changefreq': 'weekly'}, 'keywords': 'Firecrawl,Markdown,Data,Mendable,Langchain', 'sourceURL': 'https://www.firecrawl.dev/', 'ogSiteName': 'Firecrawl', 'description': 'Firecrawl crawls and converts any website into clean markdown.', 'ogDescription': 'Turn any website into LLM-ready data.', 'pageStatusCode': 200, 'ogLocaleAlternate': []}

Async load:
    .. code-block:: python

        docs = await loader.aload()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Introducing [Smart Crawl!](https://www.firecrawl.dev/smart-crawl)
         Join the waitlist to turn any web
        {'ogUrl': 'https://www.firecrawl.dev/', 'title': 'Home - Firecrawl', 'robots': 'follow, index', 'ogImage': 'https://www.firecrawl.dev/og.png?123', 'ogTitle': 'Firecrawl', 'sitemap': {'lastmod': '2024-08-12T00:28:16.681Z', 'changefreq': 'weekly'}, 'keywords': 'Firecrawl,Markdown,Data,Mendable,Langchain', 'sourceURL': 'https://www.firecrawl.dev/', 'ogSiteName': 'Firecrawl', 'description': 'Firecrawl crawls and converts any website into clean markdown.', 'ogDescription': 'Turn any website into LLM-ready data.', 'pageStatusCode': 200, 'ogLocaleAlternate': []}",4,['BaseLoader'],0,"['legacy_crawler_options_adapter', 'legacy_scrape_options_adapter', '__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\gcs_directory.py,GCSDirectoryLoader,Load from GCS directory.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\gcs_file.py,GCSFileLoader,Load from GCS file.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\geodataframe.py,GeoDataFrameLoader,Load `geopandas` Dataframe.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\git.py,GitLoader,"Load `Git` repository files.

The Repository can be local on disk available at `repo_path`,
or remote at `clone_url` that will be cloned to `repo_path`.
Currently, supports only text files.

Each document represents one file in the repository. The `path` points to
the local Git repository, and the `branch` specifies the branch to load
files from. By default, it loads from the `main` branch.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\gitbook.py,GitbookLoader,"Load `GitBook` data.

1. load from either a single page, or
2. load all (relative) paths in the navbar.",4,['WebBaseLoader'],0,"['__init__', 'lazy_load', '_get_document', '_get_paths']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\glue_catalog.py,GlueCatalogLoader,"Load table schemas from AWS Glue.

This loader fetches the schema of each table within a specified AWS Glue database.
The schema details include column names and their data types, similar to pandas
dtype representation.

AWS credentials are automatically loaded using boto3, following the standard AWS
method:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html

If a specific AWS profile is required, it can be specified and will be used to
establish the session.",5,['BaseLoader'],0,"['__init__', '_initialize_glue_client', '_fetch_tables', '_fetch_table_schema', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\google_speech_to_text.py,GoogleSpeechToTextLoader,"Loader for Google Cloud Speech-to-Text audio transcripts.

It uses the Google Cloud Speech-to-Text API to transcribe audio files
and loads the transcribed text into one or more Documents,
depending on the specified format.

To use, you should have the ``google-cloud-speech`` python package installed.

Audio files can be specified via a Google Cloud Storage uri or a local file path.

For a detailed explanation of Google Cloud Speech-to-Text, refer to the product
documentation.
https://cloud.google.com/speech-to-text",2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\gutenberg.py,GutenbergLoader,Load from `Gutenberg.org`.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\helpers.py,FileEncoding,File encoding as the NamedTuple.,0,['NamedTuple'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\hn.py,HNLoader,"Load `Hacker News` data.

It loads data from either main page results or the comments page.",3,['WebBaseLoader'],0,"['load', 'load_comments', 'load_results']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\html.py,UnstructuredHTMLLoader,"Load `HTML` files using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredHTMLLoader

loader = UnstructuredHTMLLoader(
    ""example.html"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-html",1,['UnstructuredFileLoader'],0,['_get_elements'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\html_bs.py,BSHTMLLoader,"__ModuleName__ document loader integration

Setup:
    Install ``langchain-community`` and ``bs4``.

    .. code-block:: bash

        pip install -U langchain-community bs4

Instantiate:
    .. code-block:: python

        from langchain_community.document_loaders import BSHTMLLoader

        loader = BSHTMLLoader(
            file_path=""./example_data/fake-content.html"",
        )

Lazy load:
    .. code-block:: python

        docs = []
        docs_lazy = loader.lazy_load()

        # async variant:
        # docs_lazy = await loader.alazy_load()

        for doc in docs_lazy:
            docs.append(doc)
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python


        Test Title


        My First Heading
        My first paragraph.



        {'source': './example_data/fake-content.html', 'title': 'Test Title'}

Async load:
    .. code-block:: python

        docs = await loader.aload()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python



        Test Title


        My First Heading
        My first paragraph.



        {'source': './example_data/fake-content.html', 'title': 'Test Title'}",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\hugging_face_dataset.py,HuggingFaceDatasetLoader,Load from `Hugging Face Hub` datasets.,3,['BaseLoader'],0,"['__init__', 'lazy_load', 'parse_obj']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\hugging_face_model.py,HuggingFaceModelLoader,"Load model information from `Hugging Face Hub`, including README content.

This loader interfaces with the Hugging Face Models API to fetch and load
model metadata and README files.
The API allows you to search and filter models based on specific criteria
such as model tags, authors, and more.

API URL: https://huggingface.co/api/models
DOC URL: https://huggingface.co/docs/hub/en/api

Examples:

    .. code-block:: python

        from langchain_community.document_loaders import HuggingFaceModelLoader

        # Initialize the loader with search criteria
        loader = HuggingFaceModelLoader(search=""bert"", limit=10)

        # Load models
        documents = loader.load()

        # Iterate through the fetched documents
        for doc in documents:
            print(doc.page_content)  # README content of the model
            print(doc.metadata)      # Metadata of the model",4,['BaseLoader'],2,"['__init__', 'fetch_models', 'fetch_readme_content', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\image.py,UnstructuredImageLoader,"Load `PNG` and `JPG` files using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredImageLoader

loader = UnstructuredImageLoader(
    ""example.png"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-image",1,['UnstructuredFileLoader'],0,['_get_elements'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\image_captions.py,ImageCaptionLoader,"Load image captions.

By default, the loader utilizes the pre-trained
Salesforce BLIP image captioning model.
https://huggingface.co/Salesforce/blip-image-captioning-base",3,['BaseLoader'],0,"['__init__', 'load', '_get_captions_and_metadata']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\imsdb.py,IMSDbLoader,Load `IMSDb` webpages.,1,['WebBaseLoader'],0,['load'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\iugu.py,IuguLoader,Load from `IUGU`.,4,['BaseLoader'],0,"['__init__', '_make_request', '_get_resource', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\joplin.py,JoplinLoader,"Load notes from `Joplin`.

In order to use this loader, you need to have Joplin running with the
Web Clipper enabled (look for ""Web Clipper"" in the app settings).

To get the access token, you need to go to the Web Clipper options and
under ""Advanced Options"" you will find the access token.

You can find more information about the Web Clipper service here:
https://joplinapp.org/clipper/",6,['BaseLoader'],0,"['__init__', '_get_notes', '_get_folder', '_get_tags', '_convert_date', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\json_loader.py,JSONLoader,"Load a `JSON` file using a `jq` schema.

Setup:
    .. code-block:: bash

        pip install -U jq

Instantiate:
    .. code-block:: python

        from langchain_community.document_loaders import JSONLoader
        import json
        from pathlib import Path

        file_path='./sample_quiz.json'
        data = json.loads(Path(file_path).read_text())
        loader = JSONLoader(
                 file_path=file_path,
                 jq_schema='.quiz',
                 text_content=False)

Load:
    .. code-block:: python

        docs = loader.load()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        {""sport"": {""q1"": {""question"": ""Which one is correct team name in
        NBA?"", ""options"": [""New York Bulls""
        {'source': '/sample_quiz
        .json', 'seq_num': 1}

Async load:
    .. code-block:: python

        docs = await loader.aload()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        {""sport"": {""q1"": {""question"": ""Which one is correct team name in
        NBA?"", ""options"": [""New York Bulls""
        {'source': '/sample_quizg
        .json', 'seq_num': 1}

Lazy load:
    .. code-block:: python

        docs = []
        docs_lazy = loader.lazy_load()

        # async variant:
        # docs_lazy = await loader.alazy_load()

        for doc in docs_lazy:
            docs.append(doc)
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        {""sport"": {""q1"": {""question"": ""Which one is correct team name in
        NBA?"", ""options"": [""New York Bulls""
        {'source': '/sample_quiz
        .json', 'seq_num': 1}",7,['BaseLoader'],0,"['__init__', 'lazy_load', '_parse', '_get_text', '_get_metadata', '_validate_content_key', '_validate_metadata_func']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\kinetica_loader.py,KineticaLoader,"Load from `Kinetica` API.

Each document represents one row of the result. The `page_content_columns`
are written into the `page_content` of the document. The `metadata_columns`
are written into the `metadata` of the document. By default, all columns
are written into the `page_content` and none into the `metadata`.",5,['BaseLoader'],0,"['__init__', '_execute_query', '_get_columns', 'lazy_load', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\lakefs.py,LakeFSClient,Client for lakeFS.,3,[],0,"['__init__', 'ls_objects', 'is_presign_supported']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\lakefs.py,LakeFSLoader,Load from `lakeFS`.,6,['BaseLoader'],3,"['__init__', 'set_path', 'set_ref', 'set_repo', 'load', '__validate_instance']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\lakefs.py,UnstructuredLakeFSLoader,Load from `lakeFS` as unstructured data.,3,['UnstructuredBaseLoader'],0,"['__init__', '_get_metadata', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\larksuite.py,LarkSuiteDocLoader,Load from `LarkSuite` (`FeiShu`).,3,['BaseLoader'],0,"['__init__', '_get_larksuite_api_json_data', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\larksuite.py,LarkSuiteWikiLoader,Load from `LarkSuite` (`FeiShu`) wiki.,2,['LarkSuiteDocLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\markdown.py,UnstructuredMarkdownLoader,"Load `Markdown` files using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Setup:
    Install ``langchain-community``.

    .. code-block:: bash

        pip install -U langchain-community

Instantiate:
    .. code-block:: python

        from langchain_community.document_loaders import UnstructuredMarkdownLoader

        loader = UnstructuredMarkdownLoader(
            ""./example_data/example.md"",
            mode=""elements"",
            strategy=""fast"",
        )

Lazy load:
    .. code-block:: python

        docs = []
        docs_lazy = loader.lazy_load()

        # async variant:
        # docs_lazy = await loader.alazy_load()

        for doc in docs_lazy:
            docs.append(doc)
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Sample Markdown Document
        {'source': './example_data/example.md', 'category_depth': 0, 'last_modified': '2024-08-14T15:04:18', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './example_data', 'filename': 'example.md', 'category': 'Title', 'element_id': '3d0b313864598e704aa26c728ecb61e5'}


Async load:
    .. code-block:: python

        docs = await loader.aload()
        print(docs[0].page_content[:100])
        print(docs[0].metadata)

    .. code-block:: python

        Sample Markdown Document
        {'source': './example_data/example.md', 'category_depth': 0, 'last_modified': '2024-08-14T15:04:18', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './example_data', 'filename': 'example.md', 'category': 'Title', 'element_id': '3d0b313864598e704aa26c728ecb61e5'}

References
----------
https://unstructured-io.github.io/unstructured/core/partition.html#partition-md",1,['UnstructuredFileLoader'],0,['_get_elements'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\mastodon.py,MastodonTootsLoader,Load the `Mastodon` 'toots'.,3,['BaseLoader'],0,"['__init__', 'lazy_load', '_format_toots']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\mediawikidump.py,MWDumpLoader,"Load `MediaWiki` dump from an `XML` file.

Example:
    .. code-block:: python

        from langchain_text_splitters import RecursiveCharacterTextSplitter
        from langchain_community.document_loaders import MWDumpLoader

        loader = MWDumpLoader(
            file_path=""myWiki.xml"",
            encoding=""utf8""
        )
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=0
        )
        texts = text_splitter.split_documents(docs)


:param file_path: XML local file path
:type file_path: str
:param encoding: Charset encoding, defaults to ""utf8""
:type encoding: str, optional
:param namespaces: The namespace of pages you want to parse.
    See https://www.mediawiki.org/wiki/Help:Namespaces#Localisation
    for a list of all common namespaces
:type namespaces: List[int],optional
:param skip_redirects: TR=rue to skip pages that redirect to other pages,
    False to keep them. False by default
:type skip_redirects: bool, optional
:param stop_on_error: False to skip over pages that cause parsing errors,
    True to stop. True by default
:type stop_on_error: bool, optional",4,['BaseLoader'],0,"['__init__', '_load_dump_file', '_load_single_page_from_dump', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\merge.py,MergedDataLoader,Merge documents from a list of loaders,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\mhtml.py,MHTMLLoader,Parse `MHTML` files with `BeautifulSoup`.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\mintbase.py,MintbaseDocumentLoader,"Load elements from a blockchain smart contract.

The supported blockchains are: Near mainnet, Near testnet.

If no BlockchainType is specified, the default is Near mainnet.

The Loader uses the Mintbase API to interact with the blockchain.
MB_API_KEY environment variable must be set to use this loader.

The API returns 100 NFTs per request and can be paginated using the
startToken parameter.

If get_all_tokens is set to True, the loader will get all tokens
on the contract.  Note that for contracts with a large number of tokens,
this may take a long time (e.g. 10k tokens is 100 requests).
Default value is false for this reason.

The max_execution_time (sec) can be set to limit the execution time
of the loader.

Future versions of this loader can:
    - Support additional Mintbase APIs (e.g. getTokens, etc.)

Example:
    .. code-block:: python

        contractAddress = ""nft.yearofchef.near""  # Year of chef contract address
        blockchainLoader = MintbaseDocumentLoader(
            contract_address=contractAddress, blockchain_type=""mainnet"",api_key=""omni-site""
        )",3,['BaseLoader'],0,"['__init__', 'load', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\modern_treasury.py,ModernTreasuryLoader,Load from `Modern Treasury`.,4,['BaseLoader'],0,"['__init__', '_make_request', '_get_resource', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\mongodb.py,MongodbLoader,Load MongoDB documents.,4,['BaseLoader'],0,"['__init__', 'load', '_construct_projection', '_extract_fields']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\news.py,NewsURLLoader,"Load news articles from URLs using `Unstructured`.

Args:
    urls: URLs to load. Each is loaded into its own document.
    text_mode: If True, extract text from URL and use that for page content.
        Otherwise, extract raw HTML.
    nlp: If True, perform NLP on the extracted contents, like providing a summary
        and extracting keywords.
    continue_on_failure: If True, continue loading documents even if
        loading fails for a particular URL.
    show_progress_bar: If True, use tqdm to show a loading progress bar. Requires
        tqdm to be installed, ``pip install tqdm``.
    **newspaper_kwargs: Any additional named arguments to pass to
        newspaper.Article().

Example:
    .. code-block:: python

        from langchain_community.document_loaders import NewsURLLoader

        loader = NewsURLLoader(
            urls=[""<url-1>"", ""<url-2>""],
        )
        docs = loader.load()

Newspaper reference:
    https://newspaper.readthedocs.io/en/latest/",3,['BaseLoader'],0,"['__init__', 'load', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\notebook.py,NotebookLoader,Load `Jupyter notebook` (.ipynb) files.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\notion.py,NotionDirectoryLoader,Load `Notion directory` dump.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\notiondb.py,NotionDBLoader,"Load from `Notion DB`.

Reads content from pages within a Notion Database.
Args:
    integration_token (str): Notion integration token.
    database_id (str): Notion database id.
    request_timeout_sec (int): Timeout for Notion requests in seconds.
        Defaults to 10.
    filter_object (Dict[str, Any]): Filter object used to limit returned
        entries based on specified criteria.
        E.g.: {
            ""timestamp"": ""last_edited_time"",
            ""last_edited_time"": {
                ""on_or_after"": ""2024-02-07""
            }
        } -> will only return entries that were last edited
            on or after 2024-02-07
        Notion docs: https://developers.notion.com/reference/post-database-query-filter
        Defaults to None, which will return ALL entries.",6,['BaseLoader'],0,"['__init__', 'load', '_retrieve_page_summaries', 'load_page', '_load_blocks', '_request']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\nuclia.py,NucliaLoader,Load from any file type using `Nuclia Understanding API`.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\obsidian.py,ObsidianLoader,Load `Obsidian` files from directory.,9,['BaseLoader'],6,"['__init__', '_replace_template_var', '_restore_template_vars', '_parse_front_matter', '_to_langchain_compatible_metadata', '_parse_document_tags', '_parse_dataview_fields', '_remove_front_matter', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\obs_directory.py,OBSDirectoryLoader,Load from `Huawei OBS directory`.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\obs_file.py,OBSFileLoader,Load from the `Huawei OBS file`.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\odt.py,UnstructuredODTLoader,"Load `OpenOffice ODT` files using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredODTLoader

loader = UnstructuredODTLoader(
    ""example.odt"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-odt",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\open_city_data.py,OpenCityDataLoader,Load from `Open City`.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\oracleadb_loader.py,OracleAutonomousDatabaseLoader,"Load from oracle adb

Autonomous Database connection can be made by either connection_string
or tns name. wallet_location and wallet_password are required
for TLS connection.
Each document will represent one row of the query result.
Columns are written into the `page_content` and 'metadata' in
constructor is written into 'metadata' of document,
by default, the 'metadata' is None.",4,['BaseLoader'],0,"['__init__', '_set_dsn', '_run_query', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\org_mode.py,UnstructuredOrgModeLoader,"Load `Org-Mode` files using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredOrgModeLoader

loader = UnstructuredOrgModeLoader(
    ""example.org"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-org",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\polars_dataframe.py,PolarsDataFrameLoader,Load `Polars` DataFrame.,2,['BaseDataFrameLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\powerpoint.py,UnstructuredPowerPointLoader,"Load `Microsoft PowerPoint` files using `Unstructured`.

Works with both .ppt and .pptx files.
You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredPowerPointLoader

loader = UnstructuredPowerPointLoader(
    ""example.pptx"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-pptx",1,['UnstructuredFileLoader'],0,['_get_elements'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\psychic.py,PsychicLoader,Load from `Psychic.dev`.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\pubmed.py,PubMedLoader,"Load from the `PubMed` biomedical library.

Attributes:
    query: The query to be passed to the PubMed API.
    load_max_docs: The maximum number of documents to load.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\pyspark_dataframe.py,PySparkDataFrameLoader,Load `PySpark` DataFrames.,4,['BaseLoader'],0,"['__init__', 'get_num_rows', 'lazy_load', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\python.py,PythonLoader,"Load `Python` files, respecting any non-default encoding if specified.",1,['TextLoader'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\readthedocs.py,ReadTheDocsLoader,Load `ReadTheDocs` documentation directory.,3,['BaseLoader'],0,"['__init__', 'lazy_load', '_clean_data']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\recursive_url_loader.py,RecursiveUrlLoader,"Recursively load all child links from a root URL.

    **Security Note**: This loader is a crawler that will start crawling
        at a given URL and then expand to crawl child links recursively.

        Web crawlers should generally NOT be deployed with network access
        to any internal servers.

        Control access to who can submit crawling requests and what network access
        the crawler has.

        While crawling, the crawler may encounter malicious URLs that would lead to a
        server-side request forgery (SSRF) attack.

        To mitigate risks, the crawler by default will only load URLs from the same
        domain as the start URL (controlled via prevent_outside named argument).

        This will mitigate the risk of SSRF attacks, but will not eliminate it.

        For example, if crawling a host which hosts several sites:

        https://some_host/alice_site/
        https://some_host/bob_site/

        A malicious URL on Alice's site could cause the crawler to make a malicious
        GET request to an endpoint on Bob's site. Both sites are hosted on the
        same host, so such a request would not be prevented by default.

        See https://python.langchain.com/docs/security/

    Setup:

        This class has no required additional dependencies. You can optionally install
        ``beautifulsoup4`` for richer default metadata extraction:

        .. code-block:: bash

            pip install -U beautifulsoup4

    Instantiate:
        .. code-block:: python

            from langchain_community.document_loaders import RecursiveUrlLoader

            loader = RecursiveUrlLoader(
                ""https://docs.python.org/3.9/"",
                # max_depth=2,
                # use_async=False,
                # extractor=None,
                # metadata_extractor=None,
                # exclude_dirs=(),
                # timeout=10,
                # check_response_status=True,
                # continue_on_failure=True,
                # prevent_outside=True,
                # base_url=None,
                # ...
            )

    Lazy load:
        .. code-block:: python

            docs = []
            docs_lazy = loader.lazy_load()

            # async variant:
            # docs_lazy = await loader.alazy_load()

            for doc in docs_lazy:
                docs.append(doc)
            print(docs[0].page_content[:100])
            print(docs[0].metadata)

        .. code-block:: python

            <!DOCTYPE html>

            <html xmlns=""http://www.w3.org/1999/xhtml"">
            <head>
                <meta charset=""utf-8"" /><
            {'source': 'https://docs.python.org/3.9/', 'content_type': 'text/html', 'title': '3.9.19 Documentation', 'language': None}

    Async load:
        .. code-block:: python

            docs = await loader.aload()
            print(docs[0].page_content[:100])
            print(docs[0].metadata)

        .. code-block:: python

            <!DOCTYPE html>

            <html xmlns=""http://www.w3.org/1999/xhtml"">
            <head>
                <meta charset=""utf-8"" /><
            {'source': 'https://docs.python.org/3.9/', 'content_type': 'text/html', 'title': '3.9.19 Documentation', 'language': None}

    Content parsing / extraction:
        By default the loader sets the raw HTML from each link as the Document page
        content. To parse this HTML into a more human/LLM-friendly format you can pass
        in a custom ``extractor`` method:

            .. code-block:: python

                # This example uses `beautifulsoup4` and `lxml`
                import re
                from bs4 import BeautifulSoup

                def bs4_extractor(html: str) -> str:
                    soup = BeautifulSoup(html, ""lxml"")
                    return re.sub(r""

+"", ""

"", soup.text).strip()

                loader = RecursiveUrlLoader(
                    ""https://docs.python.org/3.9/"",
                    extractor=bs4_extractor,
                )
                print(loader.load()[0].page_content[:200])


            .. code-block:: python

                3.9.19 Documentation

                Download
                Download these documents
                Docs by version

                Python 3.13 (in development)
                Python 3.12 (stable)
                Python 3.11 (security-fixes)
                Python 3.10 (security-fixes)
                Python 3.9 (securit

    Metadata extraction:
        Similarly to content extraction, you can specify a metadata extraction function
        to customize how Document metadata is extracted from the HTTP response.

        .. code-block:: python

            import aiohttp
            import requests
            from typing import Union

            def simple_metadata_extractor(
                raw_html: str, url: str, response: Union[requests.Response, aiohttp.ClientResponse]
            ) -> dict:
                content_type = getattr(response, ""headers"").get(""Content-Type"", """")
                return {""source"": url, ""content_type"": content_type}

            loader = RecursiveUrlLoader(
                ""https://docs.python.org/3.9/"",
                metadata_extractor=simple_metadata_extractor,
            )
            loader.load()[0].metadata

        .. code-block:: python

            {'source': 'https://docs.python.org/3.9/', 'content_type': 'text/html'}

    Filtering URLs:
        You may not always want to pull every URL from a website. There are four parameters
        that allow us to control what URLs we pull recursively. First, we can set the
        ``prevent_outside`` parameter to prevent URLs outside of the ``base_url`` from
        being pulled. Note that the ``base_url`` does not need to be the same as the URL we
        pass in, as shown below. We can also use ``link_regex`` and ``exclude_dirs`` to be
        more specific with the URLs that we select. In this example, we only pull websites
        from the python docs, which contain the string ""index"" somewhere and are not
        located in the FAQ section of the website.

        .. code-block:: python

            loader = RecursiveUrlLoader(
                ""https://docs.python.org/3.9/"",
                prevent_outside=True,
                base_url=""https://docs.python.org"",
                link_regex=r'<a\s+(?:[^>]*?\s+)?href=""([^""]*(?=index)[^""]*)""',
                exclude_dirs=['https://docs.python.org/3.9/faq']
            )
            docs = loader.load()

        .. code-block:: python

            ['https://docs.python.org/3.9/',
            'https://docs.python.org/3.9/py-modindex.html',
            'https://docs.python.org/3.9/genindex.html',
            'https://docs.python.org/3.9/tutorial/index.html',
            'https://docs.python.org/3.9/using/index.html',
            'https://docs.python.org/3.9/extending/index.html',
            'https://docs.python.org/3.9/installing/index.html',
            'https://docs.python.org/3.9/library/index.html',
            'https://docs.python.org/3.9/c-api/index.html',
            'https://docs.python.org/3.9/howto/index.html',
            'https://docs.python.org/3.9/distributing/index.html',
            'https://docs.python.org/3.9/reference/index.html',
            'https://docs.python.org/3.9/whatsnew/index.html']

    ",3,['BaseLoader'],0,"['__init__', '_get_child_links_recursive', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\reddit.py,RedditPostsLoader,"Load `Reddit` posts.

Read posts on a subreddit.
First, you need to go to
https://www.reddit.com/prefs/apps/
and create your application",4,['BaseLoader'],0,"['__init__', 'load', '_subreddit_posts_loader', '_user_posts_loader']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\roam.py,RoamLoader,Load `Roam` files from a directory.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\rocksetdb.py,ColumnNotFoundError,Column not found error.,1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\rocksetdb.py,RocksetLoader,"Load from a `Rockset` database.

To use, you should have the `rockset` python package installed.

Example:
    .. code-block:: python

        # This code will load 3 records from the ""langchain_demo""
        # collection as Documents, with the `text` column used as
        # the content

        from langchain_community.document_loaders import RocksetLoader
        from rockset import RocksetClient, Regions, models

        loader = RocksetLoader(
            RocksetClient(Regions.usw2a1, ""<api key>""),
            models.QueryRequestSql(
                query=""select * from langchain_demo limit 3""
            ),
            [""text""]
        )
    )",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\rst.py,UnstructuredRSTLoader,"Load `RST` files using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredRSTLoader

loader = UnstructuredRSTLoader(
    ""example.rst"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-rst",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\rtf.py,UnstructuredRTFLoader,"Load `RTF` files using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredRTFLoader

loader = UnstructuredRTFLoader(
    ""example.rtf"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-rtf",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\s3_directory.py,S3DirectoryLoader,Load from `Amazon AWS S3` directory.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\s3_file.py,S3FileLoader,Load from `Amazon AWS S3` file.,3,['UnstructuredBaseLoader'],0,"['__init__', '_get_elements', '_get_metadata']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\scrapfly.py,ScrapflyLoader,"Turn a url to llm accessible markdown with `Scrapfly.io`.

For further details, visit: https://scrapfly.io/docs/sdk/python",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\scrapingant.py,ScrapingAntLoader,"Turn an url to LLM accessible markdown with `ScrapingAnt`.

For further details, visit: https://docs.scrapingant.com/python-client",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\sitemap.py,SitemapLoader,"Load a sitemap and its URLs.

**Security Note**: This loader can be used to load all URLs specified in a sitemap.
    If a malicious actor gets access to the sitemap, they could force
    the server to load URLs from other domains by modifying the sitemap.
    This could lead to server-side request forgery (SSRF) attacks; e.g.,
    with the attacker forcing the server to load URLs from internal
    service endpoints that are not publicly accessible. While the attacker
    may not immediately gain access to this data, this data could leak
    into downstream systems (e.g., data loader is used to load data for indexing).

    This loader is a crawler and web crawlers should generally NOT be deployed
    with network access to any internal servers.

    Control access to who can submit crawling requests and what network access
    the crawler has.

    By default, the loader will only load URLs from the same domain as the sitemap
    if the site map is not a local file. This can be disabled by setting
    restrict_to_same_domain to False (not recommended).

    If the site map is a local file, no such risk mitigation is applied by default.

    Use the filter URLs argument to limit which URLs can be loaded.

    See https://python.langchain.com/docs/security",3,['WebBaseLoader'],0,"['__init__', 'parse_sitemap', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\snowflake_loader.py,SnowflakeLoader,"Load from `Snowflake` API.

Each document represents one row of the result. The `page_content_columns`
are written into the `page_content` of the document. The `metadata_columns`
are written into the `metadata` of the document. By default, all columns
are written into the `page_content` and none into the `metadata`.",4,['BaseLoader'],0,"['__init__', '_execute_query', '_get_columns', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\spider.py,SpiderLoader,"Load web pages as Documents using Spider AI.

Must have the Python package `spider-client` installed and a Spider API key.
See https://spider.cloud for more.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\spreedly.py,SpreedlyLoader,Load from `Spreedly` API.,4,['BaseLoader'],0,"['__init__', '_make_request', '_get_resource', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\srt.py,SRTLoader,Load `.srt` (subtitle) files.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\stripe.py,StripeLoader,Load from `Stripe` API.,4,['BaseLoader'],0,"['__init__', '_make_request', '_get_resource', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\surrealdb.py,SurrealDBLoader,Load SurrealDB documents.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\telegram.py,TelegramChatFileLoader,Load from `Telegram chat` dump.,2,['BaseLoader'],0,"['__init__', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\telegram.py,TelegramChatApiLoader,Load `Telegram` chat json directory dump.,4,['BaseLoader'],0,"['__init__', '_get_message_threads', '_combine_message_texts', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\tencent_cos_directory.py,TencentCOSDirectoryLoader,Load from `Tencent Cloud COS` directory.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\tencent_cos_file.py,TencentCOSFileLoader,Load from `Tencent Cloud COS` file.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\tensorflow_datasets.py,TensorflowDatasetLoader,"Load from `TensorFlow Dataset`.

Attributes:
    dataset_name: the name of the dataset to load
    split_name: the name of the split to load.
    load_max_docs: a limit to the number of loaded documents. Defaults to 100.
    sample_to_document_function: a function that converts a dataset sample
      into a Document

Example:
    .. code-block:: python

        from langchain_community.document_loaders import TensorflowDatasetLoader

        def mlqaen_example_to_document(example: dict) -> Document:
            return Document(
                page_content=decode_to_str(example[""context""]),
                metadata={
                    ""id"": decode_to_str(example[""id""]),
                    ""title"": decode_to_str(example[""title""]),
                    ""question"": decode_to_str(example[""question""]),
                    ""answer"": decode_to_str(example[""answers""][""text""][0]),
                },
            )

        tsds_client = TensorflowDatasetLoader(
                dataset_name=""mlqa/en"",
                split_name=""test"",
                load_max_docs=100,
                sample_to_document_function=mlqaen_example_to_document,
            )",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\text.py,TextLoader,"Load text file.


Args:
    file_path: Path to the file to load.

    encoding: File encoding to use. If `None`, the file will be loaded
    with the default system encoding.

    autodetect_encoding: Whether to try to autodetect the file encoding
        if the specified encoding fails.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\tidb.py,TiDBLoader,Load documents from TiDB.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\tomarkdown.py,ToMarkdownLoader,Load `HTML` using `2markdown API`.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\toml.py,TomlLoader,"Load `TOML` files.

It can load a single source file or several files in a single
directory.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\tsv.py,UnstructuredTSVLoader,"Load `TSV` files using `Unstructured`.

Like other
Unstructured loaders, UnstructuredTSVLoader can be used in both
""single"" and ""elements"" mode. If you use the loader in ""elements""
mode, the TSV file will be a single Unstructured Table element.
If you use the loader in ""elements"" mode, an HTML representation
of the table will be available in the ""text_as_html"" key in the
document metadata.

Examples
--------
from langchain_community.document_loaders.tsv import UnstructuredTSVLoader

loader = UnstructuredTSVLoader(""stanley-cups.tsv"", mode=""elements"")
docs = loader.load()",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\url.py,UnstructuredURLLoader,"Load files from remote URLs using `Unstructured`.

Use the unstructured partition function to detect the MIME type
and route the file to the appropriate partitioner.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredURLLoader

loader = UnstructuredURLLoader(
    urls=[""<url-1>"", ""<url-2>""], mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition",6,['BaseLoader'],0,"['__init__', '_validate_mode', '__is_headers_available_for_html', '__is_headers_available_for_non_html', '__is_non_html_available', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\url_selenium.py,SeleniumURLLoader,"Load `HTML` pages with `Selenium` and parse with `Unstructured`.

This is useful for loading pages that require javascript to render.

Attributes:
    urls (List[str]): List of URLs to load.
    continue_on_failure (bool): If True, continue loading other URLs on failure.
    browser (str): The browser to use, either 'chrome' or 'firefox'.
    binary_location (Optional[str]): The location of the browser binary.
    executable_path (Optional[str]): The path to the browser executable.
    headless (bool): If True, the browser will run in headless mode.
    arguments [List[str]]: List of arguments to pass to the browser.",4,['BaseLoader'],0,"['__init__', '_get_driver', '_build_metadata', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\whatsapp_chat.py,WhatsAppChatLoader,Load `WhatsApp` messages text file.,2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\wikipedia.py,WikipediaLoader,"Load from `Wikipedia`.

The hard limit on the length of the query is 300 for now.

Each wiki page represents one Document.",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\xml.py,UnstructuredXMLLoader,"Load `XML` file using `Unstructured`.

You can run the loader in one of two modes: ""single"" and ""elements"".
If you use ""single"" mode, the document will be returned as a single
langchain Document object. If you use ""elements"" mode, the unstructured
library will split the document into elements such as Title and NarrativeText.
You can pass in additional unstructured kwargs after mode to apply
different unstructured settings.

Examples
--------
from langchain_community.document_loaders import UnstructuredXMLLoader

loader = UnstructuredXMLLoader(
    ""example.xml"", mode=""elements"", strategy=""fast"",
)
docs = loader.load()

References
----------
https://unstructured-io.github.io/unstructured/bricks.html#partition-xml",2,['UnstructuredFileLoader'],0,"['__init__', '_get_elements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\xorbits.py,XorbitsLoader,Load `Xorbits` DataFrame.,1,['BaseDataFrameLoader'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_transformers\doctran_text_extract.py,DoctranPropertyExtractor,"Extract properties from text documents using doctran.

Arguments:
    properties: A list of the properties to extract.
    openai_api_key: OpenAI API key. Can also be specified via environment variable
        ``OPENAI_API_KEY``.

Example:
    .. code-block:: python

        from langchain_community.document_transformers import DoctranPropertyExtractor

        properties = [
            {
                ""name"": ""category"",
                ""description"": ""What type of email this is."",
                ""type"": ""string"",
                ""enum"": [""update"", ""action_item"", ""customer_feedback"", ""announcement"", ""other""],
                ""required"": True,
            },
            {
                ""name"": ""mentions"",
                ""description"": ""A list of all people mentioned in this email."",
                ""type"": ""array"",
                ""items"": {
                    ""name"": ""full_name"",
                    ""description"": ""The full name of the person mentioned."",
                    ""type"": ""string"",
                },
                ""required"": True,
            },
            {
                ""name"": ""eli5"",
                ""description"": ""Explain this email to me like I'm 5 years old."",
                ""type"": ""string"",
                ""required"": True,
            },
        ]

        # Pass in openai_api_key or set env var OPENAI_API_KEY
        property_extractor = DoctranPropertyExtractor(properties)
        transformed_document = await qa_transformer.atransform_documents(documents)",2,['BaseDocumentTransformer'],0,"['__init__', 'transform_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_transformers\doctran_text_qa.py,DoctranQATransformer,"Extract QA from text documents using doctran.

Arguments:
    openai_api_key: OpenAI API key. Can also be specified via environment variable
        ``OPENAI_API_KEY``.

Example:
    .. code-block:: python

        from langchain_community.document_transformers import DoctranQATransformer

        # Pass in openai_api_key or set env var OPENAI_API_KEY
        qa_transformer = DoctranQATransformer()
        transformed_document = await qa_transformer.atransform_documents(documents)",2,['BaseDocumentTransformer'],0,"['__init__', 'transform_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_transformers\doctran_text_translate.py,DoctranTextTranslator,"Translate text documents using doctran.

Arguments:
    openai_api_key: OpenAI API key. Can also be specified via environment variable
    ``OPENAI_API_KEY``.
    language: The language to translate *to*.

Example:
    .. code-block:: python

    from langchain_community.document_transformers import DoctranTextTranslator

    # Pass in openai_api_key or set env var OPENAI_API_KEY
    qa_translator = DoctranTextTranslator(language=""spanish"")
    translated_document = await qa_translator.atransform_documents(documents)",2,['BaseDocumentTransformer'],0,"['__init__', 'transform_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_transformers\google_translate.py,GoogleTranslateTransformer,Translate text documents using Google Cloud Translation.,2,['BaseDocumentTransformer'],0,"['__init__', 'transform_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_transformers\html2text.py,Html2TextTransformer,"Replace occurrences of a particular search pattern with a replacement string

Arguments:
    ignore_links: Whether links should be ignored; defaults to True.
    ignore_images: Whether images should be ignored; defaults to True.

Example:
    .. code-block:: python
        from langchain_community.document_transformers import Html2TextTransformer
        html2text = Html2TextTransformer()
        docs_transform = html2text.transform_documents(docs)",2,['BaseDocumentTransformer'],0,"['__init__', 'transform_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_transformers\markdownify.py,MarkdownifyTransformer,"Converts HTML documents to Markdown format with customizable options for handling
links, images, other tags and heading styles using the markdownify library.

Arguments:
    strip: A list of tags to strip. This option can't be used with the convert option.
    convert: A list of tags to convert. This option can't be used with the strip option.
    autolinks: A boolean indicating whether the ""automatic link"" style should be used when a a tag's contents match its href. Defaults to True.
    heading_style: Defines how headings should be converted. Accepted values are ATX, ATX_CLOSED, SETEXT, and UNDERLINED (which is an alias for SETEXT). Defaults to ATX.
    kwargs: Additional options to pass to markdownify.

Example:
    .. code-block:: python
        from langchain_community.document_transformers import MarkdownifyTransformer
        markdownify = MarkdownifyTransformer()
        docs_transform = markdownify.transform_documents(docs)

More configuration options can be found at the markdownify GitHub page:
https://github.com/matthewwithanm/python-markdownify",2,['BaseDocumentTransformer'],0,"['__init__', 'transform_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_transformers\nuclia_text_transform.py,NucliaTextTransformer,"Nuclia Text Transformer.

The Nuclia Understanding API splits into paragraphs and sentences,
identifies entities, provides a summary of the text and generates
embeddings for all sentences.",2,['BaseDocumentTransformer'],0,"['__init__', 'transform_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_transformers\openai_functions.py,OpenAIMetadataTagger,"Extract metadata tags from document contents using OpenAI functions.

    Example:
        .. code-block:: python

                from langchain_community.chat_models import ChatOpenAI
                from langchain_community.document_transformers import OpenAIMetadataTagger
                from langchain_core.documents import Document

                schema = {
                    ""properties"": {
                        ""movie_title"": { ""type"": ""string"" },
                        ""critic"": { ""type"": ""string"" },
                        ""tone"": {
                            ""type"": ""string"",
                            ""enum"": [""positive"", ""negative""]
                        },
                        ""rating"": {
                            ""type"": ""integer"",
                            ""description"": ""The number of stars the critic rated the movie""
                        }
                    },
                    ""required"": [""movie_title"", ""critic"", ""tone""]
                }

                # Must be an OpenAI model that supports functions
                llm = ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613"")
                tagging_chain = create_tagging_chain(schema, llm)
                document_transformer = OpenAIMetadataTagger(tagging_chain=tagging_chain)
                original_documents = [
                    Document(page_content=""Review of The Bee Movie
By Roger Ebert

This is the greatest movie ever made. 4 out of 5 stars.""),
                    Document(page_content=""Review of The Godfather
By Anonymous

This movie was super boring. 1 out of 5 stars."", metadata={""reliable"": False}),
                ]

                enhanced_documents = document_transformer.transform_documents(original_documents)
    ",1,"['BaseDocumentTransformer', 'BaseModel']",1,['transform_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\embeddings\fake.py,FakeEmbeddings,Fake embedding model.,3,"['Embeddings', 'BaseModel']",1,"['_get_embedding', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\embeddings\fake.py,DeterministicFakeEmbedding,"Fake embedding model that always returns
the same embedding vector for the same text.",4,"['Embeddings', 'BaseModel']",1,"['_get_embedding', '_get_seed', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\embeddings\javelin_ai_gateway.py,JavelinAIGatewayEmbeddings,"Javelin AI Gateway embeddings.

To use, you should have the ``javelin_sdk`` python package installed.
For more information, see https://docs.getjavelin.io

Example:
    .. code-block:: python

        from langchain_community.embeddings import JavelinAIGatewayEmbeddings

        embeddings = JavelinAIGatewayEmbeddings(
            gateway_uri=""<javelin-ai-gateway-uri>"",
            route=""<your-javelin-gateway-embeddings-route>""
        )",4,"['Embeddings', 'BaseModel']",4,"['__init__', '_query', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\embeddings\llamafile.py,LlamafileEmbeddings,"Llamafile lets you distribute and run large language models with a
single file.

To get started, see: https://github.com/Mozilla-Ocho/llamafile

To use this class, you will need to first:

1. Download a llamafile.
2. Make the downloaded file executable: `chmod +x path/to/model.llamafile`
3. Start the llamafile in server mode with embeddings enabled:

    `./path/to/model.llamafile --server --nobrowser --embedding`

Example:
    .. code-block:: python

        from langchain_community.embeddings import LlamafileEmbeddings
        embedder = LlamafileEmbeddings()
        doc_embeddings = embedder.embed_documents(
            [
                ""Alpha is the first letter of the Greek alphabet"",
                ""Beta is the second letter of the Greek alphabet"",
            ]
        )
        query_embedding = embedder.embed_query(
            ""What is the second letter of the Greek alphabet""
        )",3,"['BaseModel', 'Embeddings']",2,"['_embed', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\embeddings\mlflow_gateway.py,MlflowAIGatewayEmbeddings,"MLflow AI Gateway embeddings.

To use, you should have the ``mlflow[gateway]`` python package installed.
For more information, see https://mlflow.org/docs/latest/gateway/index.html.

Example:
    .. code-block:: python

        from langchain_community.embeddings import MlflowAIGatewayEmbeddings

        embeddings = MlflowAIGatewayEmbeddings(
            gateway_uri=""<your-mlflow-ai-gateway-uri>"",
            route=""<your-mlflow-ai-gateway-embeddings-route>""
        )",4,"['Embeddings', 'BaseModel']",2,"['__init__', '_query', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\embeddings\self_hosted_hugging_face.py,SelfHostedHuggingFaceEmbeddings,"HuggingFace embedding models on self-hosted remote hardware.

Supported hardware includes auto-launched instances on AWS, GCP, Azure,
and Lambda, as well as servers specified
by IP address and SSH credentials (such as on-prem, or another cloud
like Paperspace, Coreweave, etc.).

To use, you should have the ``runhouse`` python package installed.

Example:
    .. code-block:: python

        from langchain_community.embeddings import SelfHostedHuggingFaceEmbeddings
        import runhouse as rh
        model_id = ""sentence-transformers/all-mpnet-base-v2""
        gpu = rh.cluster(name=""rh-a10x"", instance_type=""A100:1"")
        hf = SelfHostedHuggingFaceEmbeddings(model_id=model_id, hardware=gpu)",1,['SelfHostedEmbeddings'],7,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\embeddings\self_hosted_hugging_face.py,SelfHostedHuggingFaceInstructEmbeddings,"HuggingFace InstructEmbedding models on self-hosted remote hardware.

Supported hardware includes auto-launched instances on AWS, GCP, Azure,
and Lambda, as well as servers specified
by IP address and SSH credentials (such as on-prem, or another
cloud like Paperspace, Coreweave, etc.).

To use, you should have the ``runhouse`` python package installed.

Example:
    .. code-block:: python

        from langchain_community.embeddings import SelfHostedHuggingFaceInstructEmbeddings
        import runhouse as rh
        model_name = ""hkunlp/instructor-large""
        gpu = rh.cluster(name='rh-a10x', instance_type='A100:1')
        hf = SelfHostedHuggingFaceInstructEmbeddings(
            model_name=model_name, hardware=gpu)",3,['SelfHostedHuggingFaceEmbeddings'],4,"['__init__', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\embeddings\xinference.py,XinferenceEmbeddings,"Xinference embedding models.

To use, you should have the xinference library installed:

.. code-block:: bash

    pip install xinference

If you're simply using the services provided by Xinference, you can utilize the xinference_client package:

.. code-block:: bash

    pip install xinference_client

Check out: https://github.com/xorbitsai/inference
To run, you need to start a Xinference supervisor on one server and Xinference workers on the other servers.

Example:
    To start a local instance of Xinference, run

    .. code-block:: bash

       $ xinference

    You can also deploy Xinference in a distributed cluster. Here are the steps:

    Starting the supervisor:

    .. code-block:: bash

       $ xinference-supervisor

    If you're simply using the services provided by Xinference, you can utilize the xinference_client package:

    .. code-block:: bash

        pip install xinference_client

    Starting the worker:

    .. code-block:: bash

       $ xinference-worker

Then, launch a model using command line interface (CLI).

Example:

.. code-block:: bash

   $ xinference launch -n orca -s 3 -q q4_0

It will return a model UID. Then you can use Xinference Embedding with LangChain.

Example:

.. code-block:: python

    from langchain_community.embeddings import XinferenceEmbeddings

    xinference = XinferenceEmbeddings(
        server_url=""http://0.0.0.0:9997"",
        model_uid = {model_uid} # replace model_uid with the model UID return from launching the model
    )",3,['Embeddings'],3,"['__init__', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graphs\graph_document.py,Node,"Represents a node in a graph with associated properties.

Attributes:
    id (Union[str, int]): A unique identifier for the node.
    type (str): The type or label of the node, default is ""Node"".
    properties (dict): Additional properties and metadata associated with the node.",0,['Serializable'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graphs\graph_document.py,Relationship,"Represents a directed relationship between two nodes in a graph.

Attributes:
    source (Node): The source node of the relationship.
    target (Node): The target node of the relationship.
    type (str): The type of the relationship.
    properties (dict): Additional properties associated with the relationship.",0,['Serializable'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graphs\graph_document.py,GraphDocument,"Represents a graph document consisting of nodes and relationships.

Attributes:
    nodes (List[Node]): A list of nodes in the graph.
    relationships (List[Relationship]): A list of relationships in the graph.
    source (Document): The document from which the graph information is derived.",0,['Serializable'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graphs\index_creator.py,GraphIndexCreator,Functionality to create graph index.,1,['BaseModel'],2,['from_text'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graphs\memgraph_graph.py,MemgraphGraph,"Memgraph wrapper for graph operations.

*Security note*: Make sure that the database connection uses credentials
    that are narrowly-scoped to only include necessary permissions.
    Failure to do so may result in data corruption or loss, since the calling
    code may attempt commands that would result in deletion, mutation
    of data if appropriately prompted or reading sensitive data if such
    data is present in the database.
    The best way to guard against such negative outcomes is to (as appropriate)
    limit the permissions granted to the credentials used with this tool.

    See https://python.langchain.com/docs/security for more information.",2,['Neo4jGraph'],0,"['__init__', 'refresh_schema']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\mmr_helper.py,_Candidate,,2,[],5,"['__post_init__', 'update_redundancy']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\mmr_helper.py,MmrHelper,"Helper for executing an MMR traversal query.

Args:
    query_embedding: The embedding of the query to use for scoring.
    lambda_mult: Number between 0 and 1 that determines the degree
        of diversity among the results with 0 corresponding to maximum
        diversity and 1 to minimum diversity. Defaults to 0.5.
    score_threshold: Only documents with a score greater than or equal
        this threshold will be chosen. Defaults to -infinity.",6,[],14,"['__init__', 'candidate_ids', '_already_selected_embeddings', '_pop_candidate', 'pop_best', 'add_candidates']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\indexes\_document_manager.py,MongoDocumentManager,A MongoDB based implementation of the document manager.,7,['RecordManager'],0,"['__init__', 'create_schema', 'update', 'get_time', 'exists', 'list_keys', 'delete_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\arxiv.py,ArxivRetriever,"`Arxiv` retriever.

Setup:
    Install ``arxiv``:

    .. code-block:: bash

        pip install -U arxiv

Key init args:
    load_max_docs: int
        maximum number of documents to load
    get_ful_documents: bool
        whether to return full document text or snippets

Instantiate:
    .. code-block:: python

        from langchain_community.retrievers import ArxivRetriever

        retriever = ArxivRetriever(
            load_max_docs=2,
            get_ful_documents=True,
        )

Usage:
    .. code-block:: python

        docs = retriever.invoke(""What is the ImageBind model?"")
        docs[0].metadata

    .. code-block:: none

        {'Entry ID': 'http://arxiv.org/abs/2305.05665v2',
        'Published': datetime.date(2023, 5, 31),
        'Title': 'ImageBind: One Embedding Space To Bind Them All',
        'Authors': 'Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra'}

Use within a chain:
    .. code-block:: python

        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.runnables import RunnablePassthrough
        from langchain_openai import ChatOpenAI

        prompt = ChatPromptTemplate.from_template(
            """"""Answer the question based only on the context provided.

        Context: {context}

        Question: {question}""""""
        )

        llm = ChatOpenAI(model=""gpt-3.5-turbo-0125"")

        def format_docs(docs):
            return ""\n\n"".join(doc.page_content for doc in docs)

        chain = (
            {""context"": retriever | format_docs, ""question"": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        chain.invoke(""What is the ImageBind model?"")

    .. code-block:: none

         'The ImageBind model is an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data...'",1,"['BaseRetriever', 'ArxivAPIWrapper']",1,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\asknews.py,AskNewsRetriever,AskNews retriever.,2,['BaseRetriever'],11,"['_get_relevant_documents', '_extract_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\breebs.py,BreebsRetriever,"A retriever class for `Breebs`.

See https://www.breebs.com/ for more info.
Args:
    breeb_key: The key to trigger the breeb
    (specialized knowledge pill on a specific topic).

To retrieve the list of all available Breebs : you can call https://breebs.promptbreeders.com/web/listbreebs",2,['BaseRetriever'],2,"['__init__', '_get_relevant_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\chaindesk.py,ChaindeskRetriever,`Chaindesk API` retriever.,2,['BaseRetriever'],3,"['__init__', '_get_relevant_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\databerry.py,DataberryRetriever,`Databerry API` retriever.,1,['BaseRetriever'],3,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\dria_index.py,DriaRetriever,`Dria` retriever using the DriaAPIWrapper.,4,['BaseRetriever'],1,"['__init__', 'create_knowledge_base', 'add_texts', '_get_relevant_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\llama_index.py,LlamaIndexRetriever,"`LlamaIndex` retriever.

It is used for the question-answering with sources over
an LlamaIndex data structure.",1,['BaseRetriever'],2,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\llama_index.py,LlamaIndexGraphRetriever,"`LlamaIndex` graph data structure retriever.

It is used for question-answering with sources over an LlamaIndex
graph data structure.",1,['BaseRetriever'],2,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\outline.py,OutlineRetriever,"Retriever for Outline API.

It wraps run() to get_relevant_documents().
It uses all OutlineAPIWrapper arguments without any change.",1,"['BaseRetriever', 'OutlineAPIWrapper']",0,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\pubmed.py,PubMedRetriever,"`PubMed API` retriever.

It wraps load() to get_relevant_documents().
It uses all PubMedAPIWrapper arguments without any change.",1,"['BaseRetriever', 'PubMedAPIWrapper']",0,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\rememberizer.py,RememberizerRetriever,"`Rememberizer` retriever.

It wraps load() to get_relevant_documents().
It uses all RememberizerAPIWrapper arguments without any change.",1,"['BaseRetriever', 'RememberizerAPIWrapper']",0,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\remote_retriever.py,RemoteLangChainRetriever,`LangChain API` retriever.,1,['BaseRetriever'],6,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\wikipedia.py,WikipediaRetriever,"`Wikipedia API` retriever.

Setup:
    Install the ``wikipedia`` dependency:

    .. code-block:: bash

        pip install -U wikipedia

Instantiate:
    .. code-block:: python

        from langchain_community.retrievers import WikipediaRetriever

        retriever = WikipediaRetriever()

Usage:
    .. code-block:: python

        docs = retriever.invoke(""TOKYO GHOUL"")
        print(docs[0].page_content[:100])

    .. code-block:: none

        Tokyo Ghoul (Japanese: 東京喰種（トーキョーグール）, Hepburn: Tōkyō Gūru) is a Japanese dark fantasy

Use within a chain:
    .. code-block:: python

        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.runnables import RunnablePassthrough
        from langchain_openai import ChatOpenAI

        prompt = ChatPromptTemplate.from_template(
            """"""Answer the question based only on the context provided.

        Context: {context}

        Question: {question}""""""
        )

        llm = ChatOpenAI(model=""gpt-3.5-turbo-0125"")

        def format_docs(docs):
            return ""\n\n"".join(doc.page_content for doc in docs)

        chain = (
            {""context"": retriever | format_docs, ""question"": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        chain.invoke(
            ""Who is the main character in `Tokyo Ghoul` and does he transform into a ghoul?""
        )

    .. code-block:: none

         'The main character in Tokyo Ghoul is Ken Kaneki, who transforms into a ghoul after receiving an organ transplant from a ghoul named Rize.'",1,"['BaseRetriever', 'WikipediaAPIWrapper']",0,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\retrievers\you.py,YouRetriever,"You.com Search API retriever.

It wraps results() to get_relevant_documents
It uses all YouSearchAPIWrapper arguments without any change.",1,"['BaseRetriever', 'YouSearchAPIWrapper']",0,['_get_relevant_documents'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\storage\cassandra.py,CassandraByteStore,"A ByteStore implementation using Cassandra as the backend.

Parameters:
    table: The name of the table to use.
    session: A Cassandra session object. If not provided, it will be resolved
        from the cassio config.
    keyspace: The keyspace to use. If not provided, it will be resolved
        from the cassio config.
    setup_mode: The setup mode to use. Default is SYNC  (SetupMode.SYNC).",9,['ByteStore'],0,"['__init__', 'ensure_db_setup', 'get_select_statement', 'get_insert_statement', 'get_delete_statement', 'mget', 'mset', 'mdelete', 'yield_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\storage\mongodb.py,MongoDBByteStore,"BaseStore implementation using MongoDB as the underlying store.

Examples:
    Create a MongoDBByteStore instance and perform operations on it:

    .. code-block:: python

        # Instantiate the MongoDBByteStore with a MongoDB connection
        from langchain.storage import MongoDBByteStore

        mongo_conn_str = ""mongodb://localhost:27017/""
        mongodb_store = MongoDBBytesStore(mongo_conn_str, db_name=""test-db"",
                                     collection_name=""test-collection"")

        # Set values for keys
        mongodb_store.mset([(""key1"", ""hello""), (""key2"", ""workd"")])

        # Get values for keys
        values = mongodb_store.mget([""key1"", ""key2""])
        # [bytes1, bytes1]

        # Iterate over keys
        for key in mongodb_store.yield_keys():
            print(key)

        # Delete keys
        mongodb_store.mdelete([""key1"", ""key2""])",5,"['BaseStore[str, bytes]']",0,"['__init__', 'mget', 'mset', 'mdelete', 'yield_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\storage\mongodb.py,MongoDBStore,"BaseStore implementation using MongoDB as the underlying store.

Examples:
    Create a MongoDBStore instance and perform operations on it:

    .. code-block:: python

        # Instantiate the MongoDBStore with a MongoDB connection
        from langchain.storage import MongoDBStore

        mongo_conn_str = ""mongodb://localhost:27017/""
        mongodb_store = MongoDBStore(mongo_conn_str, db_name=""test-db"",
                                     collection_name=""test-collection"")

        # Set values for keys
        doc1 = Document(...)
        doc2 = Document(...)
        mongodb_store.mset([(""key1"", doc1), (""key2"", doc2)])

        # Get values for keys
        values = mongodb_store.mget([""key1"", ""key2""])
        # [doc1, doc2]

        # Iterate over keys
        for key in mongodb_store.yield_keys():
            print(key)

        # Delete keys
        mongodb_store.mdelete([""key1"", ""key2""])",5,"['BaseStore[str, Document]']",0,"['__init__', 'mget', 'mset', 'mdelete', 'yield_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\storage\redis.py,RedisStore,"BaseStore implementation using Redis as the underlying store.

Examples:
    Create a RedisStore instance and perform operations on it:

    .. code-block:: python

        # Instantiate the RedisStore with a Redis connection
        from langchain_community.storage import RedisStore
        from langchain_community.utilities.redis import get_client

        client = get_client('redis://localhost:6379')
        redis_store = RedisStore(client=client)

        # Set values for keys
        redis_store.mset([(""key1"", b""value1""), (""key2"", b""value2"")])

        # Get values for keys
        values = redis_store.mget([""key1"", ""key2""])
        # [b""value1"", b""value2""]

        # Delete keys
        redis_store.mdelete([""key1""])

        # Iterate over keys
        for key in redis_store.yield_keys():
            print(key)  # noqa: T201",6,['ByteStore'],0,"['__init__', '_get_prefixed_key', 'mget', 'mset', 'mdelete', 'yield_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\storage\upstash_redis.py,_UpstashRedisStore,BaseStore implementation using Upstash Redis as the underlying store.,6,"['BaseStore[str, str]']",0,"['__init__', '_get_prefixed_key', 'mget', 'mset', 'mdelete', 'yield_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\storage\upstash_redis.py,UpstashRedisStore,"BaseStore implementation using Upstash Redis
as the underlying store to store strings.

Deprecated in favor of the more generic UpstashRedisByteStore.",0,['_UpstashRedisStore'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\storage\upstash_redis.py,UpstashRedisByteStore,"BaseStore implementation using Upstash Redis
as the underlying store to store raw bytes.",5,['ByteStore'],0,"['__init__', 'mget', 'mset', 'mdelete', 'yield_keys']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ifttt.py,IFTTTWebhook,"IFTTT Webhook.

Args:
    name: name of the tool
    description: description of the tool
    url: url to hit with the json event.",1,['BaseTool'],1,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utilities\brave_search.py,BraveSearchWrapper,Wrapper around the Brave search engine.,3,['BaseModel'],3,"['run', 'download_documents', '_search_request']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utilities\dria_index.py,DriaAPIWrapper,"Wrapper around Dria API.

This wrapper facilitates interactions with Dria's vector search
and retrieval services, including creating knowledge bases, inserting data,
and fetching search results.

Attributes:
    api_key: Your API key for accessing Dria.
    contract_id: The contract ID of the knowledge base to interact with.
    top_n: Number of top results to fetch for a search.",6,[],0,"['__init__', 'create_knowledge_base', 'insert_data', 'search', 'query_with_vector', 'run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utilities\jina_search.py,JinaSearchAPIWrapper,Wrapper around the Jina search engine.,3,['BaseModel'],1,"['run', 'download_documents', '_search_request']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utilities\mojeek_search.py,MojeekSearchAPIWrapper,,2,['BaseModel'],3,"['run', '_search']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utilities\nasa.py,NasaAPIWrapper,Wrapper for NASA API.,5,['BaseModel'],0,"['get_media', 'get_media_metadata_manifest', 'get_media_metadata_location', 'get_video_captions_location', 'run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utilities\oracleai.py,OracleSummary,"Get Summary
Args:
    conn: Oracle Connection,
    params: Summary parameters,
    proxy: Proxy",2,[],0,"['__init__', 'get_summary']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utilities\redis.py,TokenEscaper,Escape punctuation within an input string.,2,[],1,"['__init__', 'escape']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utils\ernie_functions.py,FunctionDescription,Representation of a callable function to the Ernie API.,0,['TypedDict'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\utils\ernie_functions.py,ToolDescription,Representation of a callable function to the Ernie API.,0,['TypedDict'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\cogniswitch\toolkit.py,CogniswitchToolkit,"Toolkit for CogniSwitch.

Use the toolkit to get all the tools present in the Cogniswitch and
use them to interact with your knowledge.

Parameters:
    cs_token: str. The Cogniswitch token.
    OAI_token: str. The OpenAI API token.
    apiKey: str. The Cogniswitch OAuth token.",1,['BaseToolkit'],3,['get_tools'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\json\toolkit.py,JsonToolkit,"Toolkit for interacting with a JSON spec.

Parameters:
    spec: The JSON spec.",1,['BaseToolkit'],1,['get_tools'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\openapi\planner.py,RequestsGetToolWithParsing,Requests GET tool with LLM-instructed extraction of truncated responses.,1,"['BaseRequestsTool', 'BaseTool']",4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\openapi\planner.py,RequestsPostToolWithParsing,Requests POST tool with LLM-instructed extraction of truncated responses.,1,"['BaseRequestsTool', 'BaseTool']",4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\openapi\planner.py,RequestsPatchToolWithParsing,Requests PATCH tool with LLM-instructed extraction of truncated responses.,1,"['BaseRequestsTool', 'BaseTool']",4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\openapi\planner.py,RequestsPutToolWithParsing,Requests PUT tool with LLM-instructed extraction of truncated responses.,1,"['BaseRequestsTool', 'BaseTool']",4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\openapi\planner.py,RequestsDeleteToolWithParsing,Tool that sends a DELETE request and parses the response.,1,"['BaseRequestsTool', 'BaseTool']",4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\agent_toolkits\openapi\spec.py,ReducedOpenAPISpec,"A reduced OpenAPI spec.

This is a quick and dirty representation for OpenAPI specs.

Parameters:
    servers: The servers in the spec.
    description: The description of the spec.
    endpoints: The endpoints in the spec.",0,[],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\tracers\comet.py,CometTracer,Comet Tracer.,15,['BaseTracer'],0,"['__init__', '_initialize_comet_modules', '_persist_run', '_process_start_trace', '_process_end_trace', 'flush', '_on_llm_start', '_on_llm_end', '_on_llm_error', '_on_chain_start', '_on_chain_end', '_on_chain_error', '_on_tool_start', '_on_tool_end', '_on_tool_error']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\tracers\wandb.py,WandbRunArgs,Arguments for the WandbTracer.,0,['TypedDict'],24,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\callbacks\tracers\wandb.py,WandbTracer,"Callback Handler that logs to Weights and Biases.

This handler will log the model architecture and run traces to Weights and Biases.
This will ensure that all LangChain activity is logged to W&B.",6,['BaseTracer'],2,"['__init__', 'finish', '_ensure_run', 'process_model_dict', '_log_trace_from_run', '_persist_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,AuthContext,Class for an authorization context.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,SemanticEntities,Class for a semantic entity filter.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,SemanticTopics,Class for a semantic topic filter.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,SemanticContext,Class for a semantic context.,1,['BaseModel'],2,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,ChainInput,Input for PebbloRetrievalQA chain.,1,['BaseModel'],3,['dict'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,Runtime,"OS, language details",0,['BaseModel'],10,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,Framework,Langchain framework details,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,Model,,0,['BaseModel'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,PkgInfo,,0,['BaseModel'],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,VectorDB,,0,['BaseModel'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,ChainInfo,,0,['BaseModel'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,App,,0,['BaseModel'],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,Context,,0,['BaseModel'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,Prompt,,0,['BaseModel'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\chains\pebblo_retrieval\models.py,Qa,,0,['BaseModel'],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\blob_loaders\file_system.py,FileSystemBlobLoader,"Load blobs in the local file system.

Example:

.. code-block:: python

    from langchain_community.document_loaders.blob_loaders import FileSystemBlobLoader
    loader = FileSystemBlobLoader(""/path/to/directory"")
    for blob in loader.yield_blobs():
        print(blob)  # noqa: T201",4,['BlobLoader'],0,"['__init__', 'yield_blobs', '_yield_paths', 'count_matching_files']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\blob_loaders\youtube_audio.py,YoutubeAudioLoader,Load YouTube urls as audio file(s).,2,['BlobLoader'],0,"['__init__', 'yield_blobs']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\docai.py,DocAIParsingResults,Dataclass to store Document AI parsing results.,0,[],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\docai.py,DocAIParser,"`Google Cloud Document AI` parser.

For a detailed explanation of Document AI, refer to the product documentation.
https://cloud.google.com/document-ai/docs/overview",9,['BaseBlobParser'],0,"['__init__', 'lazy_parse', 'online_process', 'batch_parse', 'parse_from_results', 'operations_from_names', 'is_running', 'docai_parse', 'get_results']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\doc_intelligence.py,AzureAIDocumentIntelligenceParser,"Loads a PDF with Azure Document Intelligence
(formerly Forms Recognizer).",5,['BaseBlobParser'],0,"['__init__', '_generate_docs_page', '_generate_docs_single', 'lazy_parse', 'parse_url']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\generic.py,MimeTypeBasedParser,"Parser that uses `mime`-types to parse a blob.

This parser is useful for simple pipelines where the mime-type is sufficient
to determine how to parse a blob.

To use, configure handlers based on mime-types and pass them to the initializer.

Example:

    .. code-block:: python

        from langchain_community.document_loaders.parsers.generic import MimeTypeBasedParser

        parser = MimeTypeBasedParser(
            handlers={
                ""application/pdf"": ...,
            },
            fallback_parser=...,
        )",2,['BaseBlobParser'],0,"['__init__', 'lazy_parse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\grobid.py,ServerUnavailableException,Exception raised when the Grobid server is unavailable.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\grobid.py,GrobidParser,Load  article `PDF` files using `Grobid`.,3,['BaseBlobParser'],0,"['__init__', 'process_xml', 'lazy_parse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\msword.py,MsWordParser,Parse the Microsoft Word documents from a blob.,1,['BaseBlobParser'],0,['lazy_parse'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\pdf.py,PyPDFParser,Load `PDF` using `pypdf`,3,['BaseBlobParser'],0,"['__init__', 'lazy_parse', '_extract_images_from_page']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\pdf.py,PDFMinerParser,Parse `PDF` using `PDFMiner`.,3,['BaseBlobParser'],0,"['__init__', 'lazy_parse', '_extract_images_from_page']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\pdf.py,PyMuPDFParser,Parse `PDF` using `PyMuPDF`.,5,['BaseBlobParser'],0,"['__init__', 'lazy_parse', '_get_page_content', '_extract_metadata', '_extract_images_from_page']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\pdf.py,PyPDFium2Parser,Parse `PDF` with `PyPDFium2`.,3,['BaseBlobParser'],0,"['__init__', 'lazy_parse', '_extract_images_from_page']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\pdf.py,PDFPlumberParser,Parse `PDF` with `PDFPlumber`.,4,['BaseBlobParser'],0,"['__init__', 'lazy_parse', '_process_page_content', '_extract_images_from_page']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\pdf.py,AmazonTextractPDFParser,"Send `PDF` files to `Amazon Textract` and parse them.

For parsing multi-page PDFs, they have to reside on S3.

The AmazonTextractPDFLoader calls the
[Amazon Textract Service](https://aws.amazon.com/textract/)
to convert PDFs into a Document structure.
Single and multi-page documents are supported with up to 3000 pages
and 512 MB of size.

For the call to be successful an AWS account is required,
similar to the
[AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)
requirements.

Besides the AWS configuration, it is very similar to the other PDF
loaders, while also supporting JPEG, PNG and TIFF and non-native
PDF formats.

```python
from langchain_community.document_loaders import AmazonTextractPDFLoader
loader=AmazonTextractPDFLoader(""example_data/alejandro_rosalez_sample-small.jpeg"")
documents = loader.load()
```

One feature is the linearization of the output.
When using the features LAYOUT, FORMS or TABLES together with Textract

```python
from langchain_community.document_loaders import AmazonTextractPDFLoader
# you can mix and match each of the features
loader=AmazonTextractPDFLoader(
    ""example_data/alejandro_rosalez_sample-small.jpeg"",
    textract_features=[""TABLES"", ""LAYOUT""])
documents = loader.load()
```

it will generate output that formats the text in reading order and
try to output the information in a tabular structure or
output the key/value pairs with a colon (key: value).
This helps most LLMs to achieve better accuracy when
processing these texts.",2,['BaseBlobParser'],0,"['__init__', 'lazy_parse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\pdf.py,DocumentIntelligenceParser,"Loads a PDF with Azure Document Intelligence
(formerly Form Recognizer) and chunks at character level.",3,['BaseBlobParser'],0,"['__init__', '_generate_docs', 'lazy_parse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\txt.py,TextParser,Parser for text blobs.,1,['BaseBlobParser'],0,['lazy_parse'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\vsdx.py,VsdxParser,Parser for vsdx files.,4,"['BaseBlobParser', 'ABC']",0,"['parse', 'lazy_parse', 'get_pages_content', 'get_relationships']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\html\bs4.py,BS4HTMLParser,Parse HTML files using `Beautiful Soup`.,2,['BaseBlobParser'],0,"['__init__', 'lazy_parse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\c.py,CSegmenter,Code segmenter for C.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\cobol.py,CobolSegmenter,Code segmenter for `COBOL`.,7,['CodeSegmenter'],3,"['__init__', 'is_valid', '_extract_code', '_is_relevant_code', '_process_lines', 'extract_functions_classes', 'simplify_code']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\cpp.py,CPPSegmenter,Code segmenter for C++.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\csharp.py,CSharpSegmenter,Code segmenter for C#.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\elixir.py,ElixirSegmenter,Code segmenter for Elixir.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\go.py,GoSegmenter,Code segmenter for Go.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\java.py,JavaSegmenter,Code segmenter for Java.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\javascript.py,JavaScriptSegmenter,Code segmenter for JavaScript.,5,['CodeSegmenter'],0,"['__init__', 'is_valid', '_extract_code', 'extract_functions_classes', 'simplify_code']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\kotlin.py,KotlinSegmenter,Code segmenter for Kotlin.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\language_parser.py,LanguageParser,"Parse using the respective programming language syntax.

Each top-level function and class in the code is loaded into separate documents.
Furthermore, an extra document is generated, containing the remaining top-level code
that excludes the already segmented functions and classes.

This approach can potentially improve the accuracy of QA models over source code.

The supported languages for code parsing are:

- C: ""c"" (*)
- C++: ""cpp"" (*)
- C#: ""csharp"" (*)
- COBOL: ""cobol""
- Elixir: ""elixir""
- Go: ""go"" (*)
- Java: ""java"" (*)
- JavaScript: ""js"" (requires package `esprima`)
- Kotlin: ""kotlin"" (*)
- Lua: ""lua"" (*)
- Perl: ""perl"" (*)
- Python: ""python""
- Ruby: ""ruby"" (*)
- Rust: ""rust"" (*)
- Scala: ""scala"" (*)
- TypeScript: ""ts"" (*)

Items marked with (*) require the packages `tree_sitter` and
`tree_sitter_languages`. It is straightforward to add support for additional
languages using `tree_sitter`, although this currently requires modifying LangChain.

The language used for parsing can be configured, along with the minimum number of
lines required to activate the splitting based on syntax.

If a language is not explicitly specified, `LanguageParser` will infer one from
filename extensions, if present.

Examples:

   .. code-block:: python

        from langchain_community.document_loaders.generic import GenericLoader
        from langchain_community.document_loaders.parsers import LanguageParser

        loader = GenericLoader.from_filesystem(
            ""./code"",
            glob=""**/*"",
            suffixes=["".py"", "".js""],
            parser=LanguageParser()
        )
        docs = loader.load()

    Example instantiations to manually select the language:

    .. code-block:: python


        loader = GenericLoader.from_filesystem(
            ""./code"",
            glob=""**/*"",
            suffixes=["".py""],
            parser=LanguageParser(language=""python"")
        )

    Example instantiations to set number of lines threshold:

    .. code-block:: python

        loader = GenericLoader.from_filesystem(
            ""./code"",
            glob=""**/*"",
            suffixes=["".py""],
            parser=LanguageParser(parser_threshold=200)
        )",2,['BaseBlobParser'],0,"['__init__', 'lazy_parse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\lua.py,LuaSegmenter,Code segmenter for Lua.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\perl.py,PerlSegmenter,Code segmenter for Perl.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\php.py,PHPSegmenter,Code segmenter for PHP.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\python.py,PythonSegmenter,Code segmenter for `Python`.,5,['CodeSegmenter'],0,"['__init__', 'is_valid', '_extract_code', 'extract_functions_classes', 'simplify_code']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\ruby.py,RubySegmenter,Code segmenter for Ruby.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\rust.py,RustSegmenter,Code segmenter for Rust.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\scala.py,ScalaSegmenter,Code segmenter for Scala.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\document_loaders\parsers\language\typescript.py,TypeScriptSegmenter,Code segmenter for TypeScript.,3,['TreeSitterSegmenter'],0,"['get_language', 'get_chunk_query', 'make_line_comment']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\extractors\gliner_link_extractor.py,GLiNERLinkExtractor,"Link documents with common named entities using `GLiNER`_.

`GLiNER`_ is a Named Entity Recognition (NER) model capable of identifying any
entity type using a bidirectional transformer encoder (BERT-like).

The ``GLiNERLinkExtractor`` uses GLiNER to create links between documents that
have named entities in common.

Example::

    extractor = GLiNERLinkExtractor(
        labels=[""Person"", ""Award"", ""Date"", ""Competitions"", ""Teams""]
    )
    results = extractor.extract_one(""some long text..."")

.. _GLiNER: https://github.com/urchade/GLiNER

.. seealso::

        - :mod:`How to use a graph vector store <langchain_community.graph_vectorstores>`
        - :class:`How to create links between documents <langchain_community.graph_vectorstores.links.Link>`

How to link Documents on common named entities
==============================================

Preliminaries
-------------

Install the ``gliner`` package:

.. code-block:: bash

    pip install -q langchain_community gliner

Usage
-----

We load the ``state_of_the_union.txt`` file, chunk it, then for each chunk we
extract named entity links and add them to the chunk.

Using extract_one()
^^^^^^^^^^^^^^^^^^^

We can use :meth:`extract_one` on a document to get the links and add the links
to the document metadata with
:meth:`~langchain_community.graph_vectorstores.links.add_links`::

    from langchain_community.document_loaders import TextLoader
    from langchain_community.graph_vectorstores import CassandraGraphVectorStore
    from langchain_community.graph_vectorstores.extractors import GLiNERLinkExtractor
    from langchain_community.graph_vectorstores.links import add_links
    from langchain_text_splitters import CharacterTextSplitter

    loader = TextLoader(""state_of_the_union.txt"")
    raw_documents = loader.load()

    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    documents = text_splitter.split_documents(raw_documents)

    ner_extractor = GLiNERLinkExtractor([""Person"", ""Topic""])
    for document in documents:
        links = ner_extractor.extract_one(document)
        add_links(document, links)

    print(documents[0].metadata)

.. code-block:: output

    {'source': 'state_of_the_union.txt', 'links': [Link(kind='entity:Person', direction='bidir', tag='President Zelenskyy'), Link(kind='entity:Person', direction='bidir', tag='Vladimir Putin')]}

Using LinkExtractorTransformer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using the :class:`~langchain_community.graph_vectorstores.extractors.link_extractor_transformer.LinkExtractorTransformer`,
we can simplify the link extraction::

    from langchain_community.document_loaders import TextLoader
    from langchain_community.graph_vectorstores.extractors import (
        GLiNERLinkExtractor,
        LinkExtractorTransformer,
    )
    from langchain_text_splitters import CharacterTextSplitter

    loader = TextLoader(""state_of_the_union.txt"")
    raw_documents = loader.load()

    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    documents = text_splitter.split_documents(raw_documents)

    ner_extractor = GLiNERLinkExtractor([""Person"", ""Topic""])
    transformer = LinkExtractorTransformer([ner_extractor])
    documents = transformer.transform_documents(documents)

    print(documents[0].metadata)

.. code-block:: output

    {'source': 'state_of_the_union.txt', 'links': [Link(kind='entity:Person', direction='bidir', tag='President Zelenskyy'), Link(kind='entity:Person', direction='bidir', tag='Vladimir Putin')]}

The documents with named entity links can then be added to a :class:`~langchain_community.graph_vectorstores.base.GraphVectorStore`::

    from langchain_community.graph_vectorstores import CassandraGraphVectorStore

    store = CassandraGraphVectorStore.from_documents(documents=documents, embedding=...)

Args:
    labels: List of kinds of entities to extract.
    kind: Kind of links to produce with this extractor.
    model: GLiNER model to use.
    extract_kwargs: Keyword arguments to pass to GLiNER.",3,['LinkExtractor[GLiNERInput]'],0,"['__init__', 'extract_one', 'extract_many']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\extractors\hierarchy_link_extractor.py,HierarchyLinkExtractor,,3,['LinkExtractor[HierarchyInput]'],0,"['__init__', 'as_document_extractor', 'extract_one']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\extractors\html_link_extractor.py,HtmlInput,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\extractors\html_link_extractor.py,HtmlLinkExtractor,,3,['LinkExtractor[HtmlInput]'],0,"['__init__', 'as_document_extractor', 'extract_one']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\extractors\keybert_link_extractor.py,KeybertLinkExtractor,,3,['LinkExtractor[KeybertInput]'],0,"['__init__', 'extract_one', 'extract_many']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\extractors\link_extractor_adapter.py,LinkExtractorAdapter,,3,['LinkExtractor[InputT]'],0,"['__init__', 'extract_one', 'extract_many']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\graph_vectorstores\extractors\link_extractor_transformer.py,LinkExtractorTransformer,"DocumentTransformer for applying one or more LinkExtractors.

Example:
    .. code-block:: python

        extract_links = LinkExtractorTransformer([
            HtmlLinkExtractor().as_document_extractor(),
        ])
        extract_links.transform_documents(docs)",2,['BaseDocumentTransformer'],0,"['__init__', 'transform_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ainetwork\owner.py,RuleSchema,Schema for owner operations.,0,['BaseModel'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ainetwork\owner.py,AINOwnerOps,Tool for owner operations.,0,['AINBaseTool'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ainetwork\rule.py,RuleSchema,Schema for owner operations.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ainetwork\rule.py,AINRuleOps,Tool for owner operations.,0,['AINBaseTool'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ainetwork\transfer.py,TransferSchema,Schema for transfer operations.,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ainetwork\transfer.py,AINTransfer,Tool for transfer operations.,0,['AINBaseTool'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ainetwork\value.py,ValueSchema,Schema for value operations.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ainetwork\value.py,AINValueOps,Tool for value operations.,0,['AINBaseTool'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\amadeus\base.py,AmadeusBaseTool,Base Tool for Amadeus.,0,['BaseTool'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\amadeus\flight_search.py,FlightSearchSchema,Schema for the AmadeusFlightSearch tool.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\amadeus\flight_search.py,AmadeusFlightSearch,Tool for searching for a single flight between two airports.,1,['AmadeusBaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\arxiv\tool.py,ArxivInput,Input for the Arxiv tool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\arxiv\tool.py,ArxivQueryRun,Tool that searches the Arxiv API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\asknews\tool.py,SearchInput,Input for the AskNews Search tool.,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\asknews\tool.py,AskNewsSearch,Tool that searches the AskNews API.,1,['BaseTool'],5,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\audio\huggingface_text_to_speech_inference.py,HuggingFaceTextToSpeechModelInference,"HuggingFace Text-to-Speech Model Inference.

Requirements:
    - Environment variable ``HUGGINGFACE_API_KEY`` must be set,
      or passed as a named parameter to the constructor.",2,['BaseTool'],10,"['__init__', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\bing_search\tool.py,BingSearchRun,Tool that queries the Bing search API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\bing_search\tool.py,BingSearchResults,"Bing Search tool.

Setup:
    Install ``langchain-community`` and set environment variable ``BING_SUBSCRIPTION_KEY``.

    .. code-block:: bash

        pip install -U langchain-community
        export BING_SUBSCRIPTION_KEY=""your-api-key""

Instantiation:
    .. code-block:: python

        from langchain_community.tools.bing_search import BingSearchResults
        from langchain_community.utilities import BingSearchAPIWrapper

        api_wrapper = BingSearchAPIWrapper()
        tool = BingSearchResults(api_wrapper=api_wrapper)

Invocation with args:
    .. code-block:: python

        tool.invoke({""query"": ""what is the weather in SF?""})

    .. code-block:: python

        ""[{'snippet': '<b>San Francisco, CA</b> <b>Weather</b> Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days.', 'title': 'San Francisco, CA Weather Forecast | AccuWeather', 'link': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629'}, {'snippet': 'Tropical Storm Ernesto Forms; Fire <b>Weather</b> Concerns in the Great Basin: Hot Temperatures Return to the South-Central U.S. ... <b>San Francisco CA</b> 37.77°N 122.41°W (Elev. 131 ft) Last Update: 2:21 pm PDT Aug 12, 2024. Forecast Valid: 6pm PDT Aug 12, 2024-6pm PDT Aug 19, 2024 .', 'title': 'National Weather Service', 'link': 'https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA'}, {'snippet': 'Current <b>weather</b> <b>in San Francisco, CA</b>. Check current conditions <b>in San Francisco, CA</b> with radar, hourly, and more.', 'title': 'San Francisco, CA Current Weather | AccuWeather', 'link': 'https://www.accuweather.com/en/us/san-francisco/94103/current-weather/347629'}, {'snippet': 'Everything you need to know about today&#39;s <b>weather</b> <b>in San Francisco, CA</b>. High/Low, Precipitation Chances, Sunrise/Sunset, and today&#39;s Temperature History.', 'title': 'Weather Today for San Francisco, CA | AccuWeather', 'link': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-today/347629'}]""

Invocation with ToolCall:

    .. code-block:: python

        tool.invoke({""args"": {""query"":""what is the weather in SF?""}, ""id"": ""1"", ""name"": tool.name, ""type"": ""tool_call""})

    .. code-block:: python

        ToolMessage(
            content=""[{'snippet': 'Get the latest <b>weather</b> forecast for <b>San Francisco, CA</b>, including temperature, RealFeel, and chance of precipitation. Find out how the <b>weather</b> will affect your plans and activities in the city of ...', 'title': 'San Francisco, CA Weather Forecast | AccuWeather', 'link': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629'}, {'snippet': 'Radar. Be prepared with the most accurate 10-day forecast for <b>San Francisco, CA</b> with highs, lows, chance of precipitation from The <b>Weather</b> Channel and <b>Weather</b>.com.', 'title': '10-Day Weather Forecast for San Francisco, CA - The Weather Channel', 'link': 'https://weather.com/weather/tenday/l/San+Francisco+CA+USCA0987:1:US'}, {'snippet': 'Tropical Storm Ernesto Forms; Fire <b>Weather</b> Concerns in the Great Basin: Hot Temperatures Return to the South-Central U.S. ... <b>San Francisco CA</b> 37.77°N 122.41°W (Elev. 131 ft) Last Update: 2:21 pm PDT Aug 12, 2024. Forecast Valid: 6pm PDT Aug 12, 2024-6pm PDT Aug 19, 2024 .', 'title': 'National Weather Service', 'link': 'https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA'}, {'snippet': 'Current <b>weather</b> <b>in San Francisco, CA</b>. Check current conditions <b>in San Francisco, CA</b> with radar, hourly, and more.', 'title': 'San Francisco, CA Current Weather | AccuWeather', 'link': 'https://www.accuweather.com/en/us/san-francisco/94103/current-weather/347629'}]"",
            artifact=[{'snippet': 'Get the latest <b>weather</b> forecast for <b>San Francisco, CA</b>, including temperature, RealFeel, and chance of precipitation. Find out how the <b>weather</b> will affect your plans and activities in the city of ...', 'title': 'San Francisco, CA Weather Forecast | AccuWeather', 'link': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629'}, {'snippet': 'Radar. Be prepared with the most accurate 10-day forecast for <b>San Francisco, CA</b> with highs, lows, chance of precipitation from The <b>Weather</b> Channel and <b>Weather</b>.com.', 'title': '10-Day Weather Forecast for San Francisco, CA - The Weather Channel', 'link': 'https://weather.com/weather/tenday/l/San+Francisco+CA+USCA0987:1:US'}, {'snippet': 'Tropical Storm Ernesto Forms; Fire <b>Weather</b> Concerns in the Great Basin: Hot Temperatures Return to the South-Central U.S. ... <b>San Francisco CA</b> 37.77°N 122.41°W (Elev. 131 ft) Last Update: 2:21 pm PDT Aug 12, 2024. Forecast Valid: 6pm PDT Aug 12, 2024-6pm PDT Aug 19, 2024 .', 'title': 'National Weather Service', 'link': 'https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA'}, {'snippet': 'Current <b>weather</b> <b>in San Francisco, CA</b>. Check current conditions <b>in San Francisco, CA</b> with radar, hourly, and more.', 'title': 'San Francisco, CA Current Weather | AccuWeather', 'link': 'https://www.accuweather.com/en/us/san-francisco/94103/current-weather/347629'}],
            name='bing_search_results_json',
            tool_call_id='1'
        )",1,['BaseTool'],5,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\clickup\tool.py,ClickupAction,Tool that queries the  Clickup API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\cogniswitch\tool.py,CogniswitchKnowledgeRequest,"Tool that uses the Cogniswitch service to answer questions.

name: str = ""cogniswitch_knowledge_request""
description: str = (
    ""A wrapper around cogniswitch service to answer the question
    from the knowledge base.""
    ""Input should be a search query.""
)",2,['BaseTool'],6,"['_run', 'answer_cs']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\cogniswitch\tool.py,CogniswitchKnowledgeStatus,"Tool that uses the Cogniswitch services to get the
 status of the document or url uploaded.

name: str = ""cogniswitch_knowledge_status""
description: str = (
    ""A wrapper around cogniswitch services to know the status of
     the document uploaded from a url or a file. ""
    ""Input should be a file name or the url link""
)",2,['BaseTool'],6,"['_run', 'knowledge_status']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\cogniswitch\tool.py,CogniswitchKnowledgeSourceFile,"Tool that uses the Cogniswitch services to store data from file.

name: str = ""cogniswitch_knowledge_source_file""
description: str = (
    ""This calls the CogniSwitch services to analyze & store data from a file.
    If the input looks like a file path, assign that string value to file key.
    Assign document name & description only if provided in input.""
)",2,['BaseTool'],6,"['_run', 'store_data']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\cogniswitch\tool.py,CogniswitchKnowledgeSourceURL,"Tool that uses the Cogniswitch services to store data from a URL.

name: str = ""cogniswitch_knowledge_source_url""
description: str = (
    ""This calls the CogniSwitch services to analyze & store data from a url.
    the URL is provided in input, assign that value to the url key.
    Assign document name & description only if provided in input""
)",2,['BaseTool'],6,"['_run', 'store_data']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\connery\models.py,Validation,Connery Action parameter validation model.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\connery\models.py,Parameter,Connery Action parameter model.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\connery\models.py,Action,Connery Action model.,0,['BaseModel'],8,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\databricks\_execution.py,ParameterizedStatement,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\databricks\_execution.py,FunctionExecutionResult,"Result of executing a function.
We always use a string to present the result value for AI model to consume.",1,[],4,['to_json'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\dataforseo_api_search\tool.py,DataForSeoAPISearchRun,Tool that queries the DataForSeo Google search API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\dataforseo_api_search\tool.py,DataForSeoAPISearchResults,"Tool that queries the DataForSeo Google Search API
and get back json.",1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\dataherald\tool.py,DataheraldTextToSQLInput,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\dataherald\tool.py,DataheraldTextToSQL,Tool that queries using the Dataherald SDK.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ddg_search\tool.py,DDGInput,Input for the DuckDuckGo search tool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ddg_search\tool.py,DuckDuckGoSearchRun,"DuckDuckGo tool.

Setup:
    Install ``duckduckgo-search`` and ``langchain-community``.

    .. code-block:: bash

        pip install -U duckduckgo-search langchain-community

Instantiation:
    .. code-block:: python

        from langchain_community.tools import DuckDuckGoSearchResults

        tool = DuckDuckGoSearchResults()

Invocation with args:
    .. code-block:: python

        tool.invoke(""Obama"")

    .. code-block:: python

        '[snippet: Users on X have been widely comparing the boost of support felt for Kamala Harris' campaign to Barack Obama's in 2008., title: Surging Support For Kamala Harris Compared To Obama-Era Energy, link: https://www.msn.com/en-us/news/politics/surging-support-for-kamala-harris-compared-to-obama-era-energy/ar-BB1qzdC0, date: 2024-07-24T18:27:01+00:00, source: Newsweek on MSN.com], [snippet: Harris tried to emulate Obama's coalition in 2020 and failed. She may have a better shot at reaching young, Black, and Latino voters this time around., title: Harris May Follow Obama's Path to the White House After All, link: https://www.msn.com/en-us/news/politics/harris-may-follow-obama-s-path-to-the-white-house-after-all/ar-BB1qv9d4, date: 2024-07-23T22:42:00+00:00, source: Intelligencer on MSN.com], [snippet: The Republican presidential candidate said in an interview on Fox News that he ""wouldn't be worried"" about Michelle Obama running., title: Donald Trump Responds to Michelle Obama Threat, link: https://www.msn.com/en-us/news/politics/donald-trump-responds-to-michelle-obama-threat/ar-BB1qqtu5, date: 2024-07-22T18:26:00+00:00, source: Newsweek on MSN.com], [snippet: H eading into the weekend at his vacation home in Rehoboth Beach, Del., President Biden was reportedly stewing over Barack Obama's role in the orchestrated campaign to force him, title: Opinion | Barack Obama Strikes Again, link: https://www.msn.com/en-us/news/politics/opinion-barack-obama-strikes-again/ar-BB1qrfiy, date: 2024-07-22T21:28:00+00:00, source: The Wall Street Journal on MSN.com]'

Invocation with ToolCall:

    .. code-block:: python

        tool.invoke({""args"": {""query"":""Obama""}, ""id"": ""1"", ""name"": tool.name, ""type"": ""tool_call""})

    .. code-block:: python

        ToolMessage(content=""[snippet: Biden, Obama and the Clintons Will Speak at the Democratic Convention. The president, two of his predecessors and the party's 2016 nominee are said to be planning speeches at the party's ..., title: Biden, Obama and the Clintons Will Speak at the Democratic Convention ..., link: https://www.nytimes.com/2024/08/12/us/politics/dnc-speakers-biden-obama-clinton.html], [snippet: Barack Obama—with his wife, Michelle—being sworn in as the 44th president of the United States, January 20, 2009. Key events in the life of Barack Obama. Barack Obama (born August 4, 1961, Honolulu, Hawaii, U.S.) is the 44th president of the United States (2009-17) and the first African American to hold the office., title: Barack Obama | Biography, Parents, Education, Presidency, Books ..., link: https://www.britannica.com/biography/Barack-Obama], [snippet: Former President Barack Obama released a letter about President Biden's decision to drop out of the 2024 presidential race. Notably, Obama did not name or endorse Vice President Kamala Harris., title: Read Obama's full statement on Biden dropping out - CBS News, link: https://www.cbsnews.com/news/barack-obama-biden-dropping-out-2024-presidential-race-full-statement/], [snippet: Many of the marquee names in Democratic politics began quickly lining up behind Vice President Kamala Harris on Sunday, but one towering presence in the party held back: Barack Obama. The former ..., title: Why Obama Hasn't Endorsed Harris - The New York Times, link: https://www.nytimes.com/2024/07/21/us/politics/why-obama-hasnt-endorsed-harris.html]"", name='duckduckgo_results_json', tool_call_id='1')",1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\ddg_search\tool.py,DuckDuckGoSearchResults,"Tool that queries the DuckDuckGo search API and
returns the results in `output_format`.",1,['BaseTool'],10,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\image_explicitcontent.py,ExplicitImageInput,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\image_explicitcontent.py,EdenAiExplicitImageTool,"Tool that queries the Eden AI Explicit image detection.

for api reference check edenai documentation:
https://docs.edenai.co/reference/image_explicit_content_create.

To use, you should have
the environment variable ``EDENAI_API_KEY`` set with your API token.
You can find your token here: https://app.edenai.run/admin/account/settings",3,['EdenaiTool'],6,"['_parse_json', '_parse_response', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\image_objectdetection.py,ObjectDetectionInput,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\image_objectdetection.py,EdenAiObjectDetectionTool,"Tool that queries the Eden AI Object detection API.

for api reference check edenai documentation:
https://docs.edenai.co/reference/image_object_detection_create.

To use, you should have
the environment variable ``EDENAI_API_KEY`` set with your API token.
You can find your token here: https://app.edenai.run/admin/account/settings",3,['EdenaiTool'],6,"['_parse_json', '_parse_response', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\ocr_identityparser.py,IDParsingInput,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\ocr_identityparser.py,EdenAiParsingIDTool,"Tool that queries the Eden AI  Identity parsing API.

for api reference check edenai documentation:
https://docs.edenai.co/reference/ocr_identity_parser_create.

To use, you should have
the environment variable ``EDENAI_API_KEY`` set with your API token.
You can find your token here: https://app.edenai.run/admin/account/settings",2,['EdenaiTool'],6,"['_parse_response', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\ocr_invoiceparser.py,InvoiceParsingInput,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\ocr_invoiceparser.py,EdenAiParsingInvoiceTool,"Tool that queries the Eden AI Invoice parsing API.

for api reference check edenai documentation:
https://docs.edenai.co/reference/ocr_invoice_parser_create.

To use, you should have
the environment variable ``EDENAI_API_KEY`` set with your API token.
You can find your token here: https://app.edenai.run/admin/account/settings",2,['EdenaiTool'],6,"['_parse_response', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\text_moderation.py,TextModerationInput,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\edenai\text_moderation.py,EdenAiTextModerationTool,"Tool that queries the Eden AI Explicit text detection.

for api reference check edenai documentation:
https://docs.edenai.co/reference/image_explicit_content_create.

To use, you should have
the environment variable ``EDENAI_API_KEY`` set with your API token.
You can find your token here: https://app.edenai.run/admin/account/settings",2,['EdenaiTool'],6,"['_parse_response', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\copy.py,FileCopyInput,Input for CopyFileTool.,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\copy.py,CopyFileTool,Tool that copies a file.,1,"['BaseFileToolMixin', 'BaseTool']",3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\delete.py,FileDeleteInput,Input for DeleteFileTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\delete.py,DeleteFileTool,Tool that deletes a file.,1,"['BaseFileToolMixin', 'BaseTool']",3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\file_search.py,FileSearchInput,Input for FileSearchTool.,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\file_search.py,FileSearchTool,Tool that searches for files in a subdirectory that match a regex pattern.,1,"['BaseFileToolMixin', 'BaseTool']",3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\list_dir.py,DirectoryListingInput,Input for ListDirectoryTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\list_dir.py,ListDirectoryTool,Tool that lists files and directories in a specified folder.,1,"['BaseFileToolMixin', 'BaseTool']",3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\move.py,FileMoveInput,Input for MoveFileTool.,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\move.py,MoveFileTool,Tool that moves a file.,1,"['BaseFileToolMixin', 'BaseTool']",3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\read.py,ReadFileInput,Input for ReadFileTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\read.py,ReadFileTool,Tool that reads a file.,1,"['BaseFileToolMixin', 'BaseTool']",3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\utils.py,FileValidationError,Error for paths outside the root directory.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\utils.py,BaseFileToolMixin,Mixin for file system tools.,1,['BaseModel'],1,['get_relative_path'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\write.py,WriteFileInput,Input for WriteFileTool.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\file_management\write.py,WriteFileTool,Tool that writes a file to disk.,1,"['BaseFileToolMixin', 'BaseTool']",3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\financial_datasets\balance_sheets.py,BalanceSheetsSchema,Input for BalanceSheets.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\financial_datasets\balance_sheets.py,BalanceSheets,Tool that gets balance sheets for a given ticker over a given period.,2,['BaseTool'],5,"['__init__', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\financial_datasets\cash_flow_statements.py,CashFlowStatementsSchema,Input for CashFlowStatements.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\financial_datasets\cash_flow_statements.py,CashFlowStatements,Tool that gets cash flow statements for a given ticker over a given period.,2,['BaseTool'],5,"['__init__', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\financial_datasets\income_statements.py,IncomeStatementsSchema,Input for IncomeStatements.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\financial_datasets\income_statements.py,IncomeStatements,Tool that gets income statements for a given ticker over a given period.,2,['BaseTool'],5,"['__init__', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\github\tool.py,GitHubAction,Tool for interacting with the GitHub API.,1,['BaseTool'],5,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gitlab\tool.py,GitLabAction,Tool for interacting with the GitLab API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gmail\create_draft.py,CreateDraftSchema,Input for CreateDraftTool.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gmail\create_draft.py,GmailCreateDraft,Tool that creates a draft email for Gmail.,2,['GmailBaseTool'],3,"['_prepare_draft_message', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gmail\get_message.py,SearchArgsSchema,Input for GetMessageTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gmail\get_message.py,GmailGetMessage,Tool that gets a message by ID from Gmail.,1,['GmailBaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gmail\get_thread.py,GetThreadSchema,Input for GetMessageTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gmail\get_thread.py,GmailGetThread,Tool that gets a thread by ID from Gmail.,1,['GmailBaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gmail\send_message.py,SendMessageSchema,Input for SendMessageTool.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\gmail\send_message.py,GmailSendMessage,Tool that sends a message to Gmail.,2,['GmailBaseTool'],3,"['_prepare_message', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\golden_query\tool.py,GoldenQueryRun,Tool that adds the capability to query using the Golden API and get back JSON.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_cloud\texttospeech.py,GoogleCloudTextToSpeechTool,"Tool that queries the Google Cloud Text to Speech API.

In order to set this up, follow instructions at:
https://cloud.google.com/text-to-speech/docs/before-you-begin",2,['BaseTool'],3,"['__init__', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_finance\tool.py,GoogleFinanceQueryRun,Tool that queries the Google Finance API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_jobs\tool.py,GoogleJobsQueryRun,Tool that queries the Google Jobs API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_lens\tool.py,GoogleLensQueryRun,Tool that queries the Google Lens API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_places\tool.py,GooglePlacesSchema,Input for GooglePlacesTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_places\tool.py,GooglePlacesTool,Tool that queries the Google places API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_scholar\tool.py,GoogleScholarQueryRun,Tool that queries the Google search API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_search\tool.py,GoogleSearchRun,Tool that queries the Google search API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_search\tool.py,GoogleSearchResults,Tool that queries the Google Search API and gets back json.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_serper\tool.py,GoogleSerperRun,Tool that queries the Serper.dev Google search API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_serper\tool.py,GoogleSerperResults,"Tool that queries the Serper.dev Google Search API
and get back json.",1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\google_trends\tool.py,GoogleTrendsQueryRun,Tool that queries the Google trends API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\human\tool.py,HumanInputRun,Tool that asks user for input.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\jina_search\tool.py,JinaInput,Input for the Jina search tool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\jina_search\tool.py,JinaSearch,"Tool that queries the JinaSearch.

..versionadded:: 0.2.16",1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\jira\tool.py,JiraAction,Tool that queries the Atlassian Jira API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\merriam_webster\tool.py,MerriamWebsterQueryRun,Tool that searches the Merriam-Webster API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\metaphor_search\tool.py,MetaphorSearchResults,Tool that queries the Metaphor Search API and gets back json.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\multion\close_session.py,CloseSessionSchema,Input for UpdateSessionTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\multion\close_session.py,MultionCloseSession,"Tool that closes an existing Multion Browser Window with provided fields.

Attributes:
    name: The name of the tool. Default: ""close_multion_session""
    description: The description of the tool.
    args_schema: The schema for the tool's arguments. Default: UpdateSessionSchema",1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\multion\create_session.py,CreateSessionSchema,Input for CreateSessionTool.,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\multion\create_session.py,MultionCreateSession,"Tool that creates a new Multion Browser Window with provided fields.

Attributes:
    name: The name of the tool. Default: ""create_multion_session""
    description: The description of the tool.
    args_schema: The schema for the tool's arguments.",1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\multion\update_session.py,UpdateSessionSchema,Input for UpdateSessionTool.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\multion\update_session.py,MultionUpdateSession,"Tool that updates an existing Multion Browser Window with provided fields.

Attributes:
    name: The name of the tool. Default: ""update_multion_session""
    description: The description of the tool.
    args_schema: The schema for the tool's arguments. Default: UpdateSessionSchema",1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\nasa\tool.py,NasaAction,Tool that queries the Atlassian Jira API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\nuclia\tool.py,NUASchema,"Input for Nuclia Understanding API.

Attributes:
    action: Action to perform. Either `push` or `pull`.
    id: ID of the file to push or pull.
    path: Path to the file to push (needed only for `push` action).
    text: Text content to process (needed only for `push` action).",0,['BaseModel'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\nuclia\tool.py,NucliaUnderstandingAPI,Tool to process files with the Nuclia Understanding API.,9,['BaseTool'],5,"['__init__', '_run', '_pushText', '_pushFile', '_pushField', '_pull', '_pull_queue', '_find_matching_id', '_check_params']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\office365\base.py,O365BaseTool,Base class for the Office 365 tools.,0,['BaseTool'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\office365\create_draft_message.py,CreateDraftMessageSchema,Input for SendMessageTool.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\office365\create_draft_message.py,O365CreateDraftMessage,Tool for creating a draft email in Office 365.,1,['O365BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\office365\send_event.py,SendEventSchema,Input for CreateEvent Tool.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\office365\send_event.py,O365SendEvent,Tool for sending calendar events in Office 365.,1,['O365BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\office365\send_message.py,SendMessageSchema,Input for SendMessageTool.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\office365\send_message.py,O365SendMessage,Send an email in Office 365.,1,['O365BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\openai_dalle_image_generation\tool.py,OpenAIDALLEImageGenerationTool,Tool that generates an image using OpenAI DALLE.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\openweathermap\tool.py,OpenWeatherMapQueryRun,Tool that queries the OpenWeatherMap API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\passio_nutrition_ai\tool.py,NutritionAIInputs,Inputs to the Passio Nutrition AI tool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\passio_nutrition_ai\tool.py,NutritionAI,Tool that queries the Passio Nutrition AI API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\playwright\click.py,ClickToolInput,Input for ClickTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\playwright\click.py,ClickTool,Tool for clicking on an element with the given CSS selector.,2,['BaseBrowserTool'],6,"['_selector_effective', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\playwright\current_page.py,CurrentWebPageToolInput,Explicit no-args input for CurrentWebPageTool.,0,['BaseModel'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\playwright\current_page.py,CurrentWebPageTool,Tool for getting the URL of the current webpage.,1,['BaseBrowserTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\playwright\get_elements.py,GetElementsToolInput,Input for GetElementsTool.,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\playwright\get_elements.py,GetElementsTool,Tool for getting elements in the current web page matching a CSS selector.,1,['BaseBrowserTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\playwright\navigate_back.py,NavigateBackToolInput,Explicit no-args input for NavigateBackTool.,0,['BaseModel'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\playwright\navigate_back.py,NavigateBackTool,Navigate back to the previous page in the browser history.,1,['BaseBrowserTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\polygon\aggregates.py,PolygonAggregatesSchema,Input for PolygonAggregates.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\polygon\aggregates.py,PolygonAggregates,"Tool that gets aggregate bars (stock prices) over a
given date range for a given ticker from Polygon.",1,['BaseTool'],5,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\polygon\financials.py,Inputs,Inputs for Polygon's Financials API,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\polygon\financials.py,PolygonFinancials,Tool that gets the financials of a ticker from Polygon,1,['BaseTool'],5,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\polygon\last_quote.py,Inputs,Inputs for Polygon's Last Quote API,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\polygon\last_quote.py,PolygonLastQuote,Tool that gets the last quote of a ticker from Polygon,1,['BaseTool'],5,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\polygon\ticker_news.py,Inputs,Inputs for Polygon's Ticker News API,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\polygon\ticker_news.py,PolygonTickerNews,Tool that gets the latest news for a given ticker from Polygon,1,['BaseTool'],5,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\pubmed\tool.py,PubmedQueryRun,Tool that searches the PubMed API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\reddit_search\tool.py,RedditSearchSchema,Input for Reddit search.,0,['BaseModel'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\reddit_search\tool.py,RedditSearchRun,Tool that queries for posts on a subreddit.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\requests\tool.py,BaseRequestsTool,Base class for requests tools.,1,['BaseModel'],2,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\requests\tool.py,RequestsGetTool,Tool for making a GET request to an API endpoint.,1,"['BaseRequestsTool', 'BaseTool']",2,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\requests\tool.py,RequestsPostTool,Tool for making a POST request to an API endpoint.,1,"['BaseRequestsTool', 'BaseTool']",2,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\requests\tool.py,RequestsPatchTool,Tool for making a PATCH request to an API endpoint.,1,"['BaseRequestsTool', 'BaseTool']",2,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\requests\tool.py,RequestsPutTool,Tool for making a PUT request to an API endpoint.,1,"['BaseRequestsTool', 'BaseTool']",2,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\requests\tool.py,RequestsDeleteTool,Tool for making a DELETE request to an API endpoint.,1,"['BaseRequestsTool', 'BaseTool']",2,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\riza\command.py,ExecPythonInput,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\riza\command.py,ExecPython,"Riza Code tool.

Setup:
    Install ``langchain-community`` and ``rizaio`` and set environment variable ``RIZA_API_KEY``.

    .. code-block:: bash

        pip install -U langchain-community rizaio
        export RIZA_API_KEY=""your-api-key""

Instantiation:
    .. code-block:: python

        from langchain_community.tools.riza.command import ExecPython

        tool = ExecPython()

Invocation with args:
    .. code-block:: python

        tool.invoke(""x = 5; print(x)"")

    .. code-block:: python

        '5\n'

Invocation with ToolCall:

    .. code-block:: python

        tool.invoke({""args"": {""code"":""x = 5; print(x)""}, ""id"": ""1"", ""name"": tool.name, ""type"": ""tool_call""})

    .. code-block:: python

        tool.invoke({""args"": {""code"":""x = 5; print(x)""}, ""id"": ""1"", ""name"": tool.name, ""type"": ""tool_call""})",2,['BaseTool'],5,"['__init__', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\riza\command.py,ExecJavaScriptInput,,0,['BaseModel'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\riza\command.py,ExecJavaScript,A tool implementation to execute JavaScript via Riza's Code Interpreter API.,2,['BaseTool'],5,"['__init__', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\scenexplain\tool.py,SceneXplainInput,Input for SceneXplain.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\scenexplain\tool.py,SceneXplainTool,Tool that explains images.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\searchapi\tool.py,SearchAPIRun,Tool that queries the SearchApi.io search API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\searchapi\tool.py,SearchAPIResults,Tool that queries the SearchApi.io search API and returns JSON.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\semanticscholar\tool.py,SemantscholarInput,Input for the SemanticScholar tool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\semanticscholar\tool.py,SemanticScholarQueryRun,Tool that searches the semanticscholar API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\slack\base.py,SlackBaseTool,Base class for Slack tools.,0,['BaseTool'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\slack\get_channel.py,SlackGetChannel,Tool that gets Slack channel information.,1,['SlackBaseTool'],2,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\slack\get_message.py,SlackGetMessageSchema,Input schema for SlackGetMessages.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\slack\get_message.py,SlackGetMessage,Tool that gets Slack messages.,1,['SlackBaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\slack\schedule_message.py,ScheduleMessageSchema,Input for ScheduleMessageTool.,0,['BaseModel'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\slack\schedule_message.py,SlackScheduleMessage,Tool for scheduling a message in Slack.,1,['SlackBaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\slack\send_message.py,SendMessageSchema,Input for SendMessageTool.,0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\slack\send_message.py,SlackSendMessage,Tool for sending a message in Slack.,1,['SlackBaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\sleep\tool.py,SleepInput,Input for CopyFileTool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\sleep\tool.py,SleepTool,Tool that adds the capability to sleep.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\stackexchange\tool.py,StackExchangeTool,Tool that uses StackExchange,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\steam\tool.py,SteamWebAPIQueryRun,Tool that searches the Steam Web API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\tavily_search\tool.py,TavilyInput,Input for the Tavily tool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\tavily_search\tool.py,TavilySearchResults,"Tool that queries the Tavily Search API and gets back json.

    Setup:
        Install ``langchain-openai`` and ``tavily-python``, and set environment variable ``TAVILY_API_KEY``.

        .. code-block:: bash

            pip install -U langchain-community tavily-python
            export TAVILY_API_KEY=""your-api-key""

    Instantiate:

        .. code-block:: python

            from langchain_community.tools import TavilySearchResults

            tool = TavilySearchResults(
                max_results=5,
                include_answer=True,
                include_raw_content=True,
                include_images=True,
                # search_depth=""advanced"",
                # include_domains = []
                # exclude_domains = []
            )

    Invoke directly with args:

        .. code-block:: python

            tool.invoke({'query': 'who won the last french open'})

        .. code-block:: python

            '{
  ""url"": ""https://www.nytimes.com..."", ""content"": ""Novak Djokovic won the last French Open by beating Casper Ruud ...'

    Invoke with tool call:

        .. code-block:: python

            tool.invoke({""args"": {'query': 'who won the last french open'}, ""type"": ""tool_call"", ""id"": ""foo"", ""name"": ""tavily""})

        .. code-block:: python

            ToolMessage(
                content='{
  ""url"": ""https://www.nytimes.com..."", ""content"": ""Novak Djokovic won the last French Open by beating Casper Ruud ...',
                artifact={
                    'query': 'who won the last french open',
                    'follow_up_questions': None,
                    'answer': 'Novak ...',
                    'images': [
                        'https://www.amny.com/wp-content/uploads/2023/06/AP23162622181176-1200x800.jpg',
                        ...
                        ],
                    'results': [
                        {
                            'title': 'Djokovic ...',
                            'url': 'https://www.nytimes.com...',
                            'content': ""Novak..."",
                            'score': 0.99505633,
                            'raw_content': 'Tennis
Novak ...'
                        },
                        ...
                    ],
                    'response_time': 2.92
                },
                tool_call_id='1',
                name='tavily_search_results_json',
            )

    ",1,['BaseTool'],12,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\tavily_search\tool.py,TavilyAnswer,Tool that queries the Tavily Search API and gets back an answer.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\wikidata\tool.py,WikidataQueryRun,Tool that searches the Wikidata API.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\wikipedia\tool.py,WikipediaQueryInput,Input for the WikipediaQuery tool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\wikipedia\tool.py,WikipediaQueryRun,Tool that searches the Wikipedia API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\wolfram_alpha\tool.py,WolframAlphaQueryRun,Tool that queries using the Wolfram Alpha SDK.,1,['BaseTool'],3,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\you\tool.py,YouInput,Input schema for the you.com tool.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\you\tool.py,YouSearchTool,Tool that searches the you.com API.,1,['BaseTool'],4,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_community\tools\youtube\search.py,YouTubeSearchTool,Tool that queries YouTube.,2,['BaseTool'],2,"['_search', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\callbacks\file.py,FileCallbackHandler,"Callback Handler that writes to a file.

Parameters:
    file: The file to write to.
    color: The color to use for the text.",8,['BaseCallbackHandler'],0,"['__init__', '__del__', 'on_chain_start', 'on_chain_end', 'on_agent_action', 'on_tool_end', 'on_text', 'on_agent_finish']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\callbacks\stdout.py,StdOutCallbackHandler,Callback Handler that prints to std out.,7,['BaseCallbackHandler'],0,"['__init__', 'on_chain_start', 'on_chain_end', 'on_agent_action', 'on_tool_end', 'on_text', 'on_agent_finish']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\callbacks\streaming_stdout.py,StreamingStdOutCallbackHandler,Callback handler for streaming. Only works with LLMs that support streaming.,14,['BaseCallbackHandler'],0,"['on_llm_start', 'on_chat_model_start', 'on_llm_new_token', 'on_llm_end', 'on_llm_error', 'on_chain_start', 'on_chain_end', 'on_chain_error', 'on_tool_start', 'on_agent_action', 'on_tool_end', 'on_tool_error', 'on_text', 'on_agent_finish']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\document_loaders\langsmith.py,LangSmithLoader,"Load LangSmith Dataset examples as Documents.

Loads the example inputs as the Document page content and places the entire example
into the Document metadata. This allows you to easily create few-shot example
retrievers from the loaded documents.

.. dropdown:: Lazy load

    .. code-block:: python

        from langchain_core.document_loaders import LangSmithLoader

        loader = LangSmithLoader(dataset_id=""..."", limit=100)
        docs = []
        for doc in loader.lazy_load():
            docs.append(doc)

    .. code-block:: pycon

        # -> [Document(""..."", metadata={""inputs"": {...}, ""outputs"": {...}, ...}), ...]

.. versionadded:: 0.2.34",2,['BaseLoader'],0,"['__init__', 'lazy_load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\embeddings\fake.py,FakeEmbeddings,"Fake embedding model for unit testing purposes.

This embedding model creates embeddings by sampling from a normal distribution.

Do not use this outside of testing, as it is not a real embedding model.

Instantiate:
    .. code-block:: python

        from langchain_core.embeddings import FakeEmbeddings
        embed = FakeEmbeddings(size=100)

Embed single text:
    .. code-block:: python

        input_text = ""The meaning of life is 42""
        vector = embed.embed_query(input_text)
        print(vector[:3])

    .. code-block:: python

        [-0.700234640213188, -0.581266257710429, -1.1328482266445354]

Embed multiple texts:
    .. code-block:: python

        input_texts = [""Document 1..."", ""Document 2...""]
        vectors = embed.embed_documents(input_texts)
        print(len(vectors))
        # The first 3 coordinates for the first vector
        print(vectors[0][:3])

    .. code-block:: python

        2
        [-0.5670477847544458, -0.31403828652395727, -0.5840547508955257]",3,"['Embeddings', 'BaseModel']",1,"['_get_embedding', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\embeddings\fake.py,DeterministicFakeEmbedding,"Deterministic fake embedding model for unit testing purposes.

This embedding model creates embeddings by sampling from a normal distribution
with a seed based on the hash of the text.

Do not use this outside of testing, as it is not a real embedding model.

Instantiate:
    .. code-block:: python

        from langchain_core.embeddings import DeterministicFakeEmbedding
        embed = DeterministicFakeEmbedding(size=100)

Embed single text:
    .. code-block:: python

        input_text = ""The meaning of life is 42""
        vector = embed.embed_query(input_text)
        print(vector[:3])

    .. code-block:: python

        [-0.700234640213188, -0.581266257710429, -1.1328482266445354]

Embed multiple texts:
    .. code-block:: python

        input_texts = [""Document 1..."", ""Document 2...""]
        vectors = embed.embed_documents(input_texts)
        print(len(vectors))
        # The first 3 coordinates for the first vector
        print(vectors[0][:3])

    .. code-block:: python

        2
        [-0.5670477847544458, -0.31403828652395727, -0.5840547508955257]",4,"['Embeddings', 'BaseModel']",1,"['_get_embedding', '_get_seed', 'embed_documents', 'embed_query']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\indexing\in_memory.py,InMemoryDocumentIndex,"In memory document index.

This is an in-memory document index that stores documents in a dictionary.

It provides a simple search API that returns documents by the number of
counts the given query appears in the document.

.. versionadded:: 0.2.29",4,['DocumentIndex'],2,"['upsert', 'delete', 'get', '_get_relevant_documents']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\load\load.py,Reviver,Reviver for JSON objects.,2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\outputs\chat_result.py,ChatResult,"Use to represent the result of a chat model call with a single prompt.

This container is used internally by some implementations of chat model,
it will eventually be mapped to a more general `LLMResult` object, and
then projected into an `AIMessage` object.

LangChain users working with chat models will usually access information via
`AIMessage` (returned from runnable interfaces) or `LLMResult` (available
via callbacks). Please refer the `AIMessage` and `LLMResult` schema documentation
for more information.",0,['BaseModel'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\outputs\llm_result.py,LLMResult,"A container for results of an LLM call.

Both chat models and LLMs generate an LLMResult object. This object contains
the generated outputs and any additional information that the model provider
wants to return.",2,['BaseModel'],4,"['flatten', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\outputs\run_info.py,RunInfo,"Class that contains metadata for a single execution of a Chain or model.

Defined for backwards compatibility with older versions of langchain_core.

This model will likely be deprecated in the future.

Users can acquire the run_id information from callbacks or via run_id
information present in the astream_event API (depending on the use case).",0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\output_parsers\openai_tools.py,JsonOutputToolsParser,Parse tools from OpenAI response.,2,['BaseCumulativeTransformOutputParser[Any]'],3,"['parse_result', 'parse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\output_parsers\openai_tools.py,JsonOutputKeyToolsParser,Parse tools from OpenAI response.,1,['JsonOutputToolsParser'],1,['parse_result'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\output_parsers\openai_tools.py,PydanticToolsParser,Parse tools from OpenAI response.,1,['JsonOutputToolsParser'],1,['parse_result'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\output_parsers\transform.py,BaseTransformOutputParser,Base class for an output parser that can handle streaming input.,2,['BaseOutputParser[T]'],0,"['_transform', 'transform']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\output_parsers\transform.py,BaseCumulativeTransformOutputParser,Base class for an output parser that can handle streaming input.,2,['BaseTransformOutputParser[T]'],1,"['_diff', '_transform']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\runnables\config.py,EmptyDict,Empty dict type.,0,['TypedDict'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\runnables\config.py,RunnableConfig,Configuration for a Runnable.,0,['TypedDict'],8,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\runnables\config.py,ContextThreadPoolExecutor,ThreadPoolExecutor that copies the context to the child thread.,2,['ThreadPoolExecutor'],0,"['submit', 'map']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\runnables\graph_png.py,PngDrawer,"Helper class to draw a state graph into a PNG file.

It requires `graphviz` and `pygraphviz` to be installed.
:param fontname: The font to use for the labels
:param labels: A dictionary of label overrides. The dictionary
    should have the following format:
    {
        ""nodes"": {
            ""node1"": ""CustomLabel1"",
            ""node2"": ""CustomLabel2"",
            ""__end__"": ""End Node""
        },
        ""edges"": {
            ""continue"": ""ContinueLabel"",
            ""end"": ""EndLabel""
        }
    }
    The keys are the original labels, and the values are the new labels.
Usage:
    drawer = PngDrawer()
    drawer.draw(state_graph, 'graph.png')",9,[],0,"['__init__', 'get_node_label', 'get_edge_label', 'add_node', 'add_edge', 'draw', 'add_nodes', 'add_edges', 'update_styles']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\runnables\schema.py,EventData,Data associated with a streaming event.,0,['TypedDict'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\runnables\schema.py,BaseStreamEvent,"Streaming event.

Schema of a streaming event which is produced from the astream_events method.

Example:

    .. code-block:: python

        from langchain_core.runnables import RunnableLambda

        async def reverse(s: str) -> str:
            return s[::-1]

        chain = RunnableLambda(func=reverse)

        events = [event async for event in chain.astream_events(""hello"")]

        # will produce the following events
        # (where some fields have been omitted for brevity):
        [
            {
                ""data"": {""input"": ""hello""},
                ""event"": ""on_chain_start"",
                ""metadata"": {},
                ""name"": ""reverse"",
                ""tags"": [],
            },
            {
                ""data"": {""chunk"": ""olleh""},
                ""event"": ""on_chain_stream"",
                ""metadata"": {},
                ""name"": ""reverse"",
                ""tags"": [],
            },
            {
                ""data"": {""output"": ""olleh""},
                ""event"": ""on_chain_end"",
                ""metadata"": {},
                ""name"": ""reverse"",
                ""tags"": [],
            },
        ]",0,['TypedDict'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\runnables\schema.py,StandardStreamEvent,A standard stream event that follows LangChain convention for event data.,0,['BaseStreamEvent'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\runnables\schema.py,CustomStreamEvent,"Custom stream event created by the user.

.. versionadded:: 0.2.15",0,['BaseStreamEvent'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tools\retriever.py,RetrieverInput,Input to the retriever.,0,['BaseModel'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\evaluation.py,EvaluatorCallbackHandler,"Tracer that runs a run evaluator whenever a run is persisted.

Parameters
----------
evaluators : Sequence[RunEvaluator]
    The run evaluators to apply to all top level runs.
client : LangSmith Client, optional
    The LangSmith client instance to use for evaluating the runs.
    If not specified, a new instance will be created.
example_id : Union[UUID, str], optional
    The example ID to be associated with the runs.
project_name : str, optional
    The LangSmith project name to be organize eval chain runs under.

Attributes
----------
example_id : Union[UUID, None]
    The example ID associated with the runs.
client : Client
    The LangSmith client instance used for evaluating the runs.
evaluators : Sequence[RunEvaluator]
    The sequence of run evaluators to be executed.
executor : ThreadPoolExecutor
    The thread pool executor used for running the evaluators.
futures : Set[Future]
    The set of futures representing the running evaluators.
skip_unfinished : bool
    Whether to skip runs that are not finished or raised
    an error.
project_name : Optional[str]
    The LangSmith project name to be organize eval chain runs under.",6,['BaseTracer'],1,"['__init__', '_evaluate_in_project', '_select_eval_results', '_log_evaluation_feedback', '_persist_run', 'wait_for_futures']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\event_stream.py,RunInfo,"Information about a run.

This is used to keep track of the metadata associated with a run.

Parameters:
    name: The name of the run.
    tags: The tags associated with the run.
    metadata: The metadata associated with the run.
    run_type: The type of the run.
    inputs: The inputs to the run.
    parent_run_id: The ID of the parent run.",0,['TypedDict'],6,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\event_stream.py,_AstreamEventsCallbackHandler,An implementation of an async callback handler for astream events.,8,"['AsyncCallbackHandler', '_StreamingCallbackHandler']",0,"['__init__', '_get_parent_ids', '_send', '__aiter__', 'tap_output_iter', '_write_run_start_info', '__deepcopy__', '__copy__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\log_stream.py,LogEntry,A single entry in the run log.,0,['TypedDict'],11,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\log_stream.py,RunState,State of the run.,0,['TypedDict'],6,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\log_stream.py,RunLogPatch,Patch to the run log.,4,[],1,"['__init__', '__add__', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\log_stream.py,RunLog,Run log.,4,['RunLogPatch'],1,"['__init__', '__add__', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\log_stream.py,LogStreamCallbackHandler,Tracer that streams run logs to a stream.,9,"['BaseTracer', '_StreamingCallbackHandler']",0,"['__init__', '__aiter__', 'send', 'tap_output_iter', 'include_run', '_persist_run', '_on_run_create', '_on_run_update', '_on_llm_new_token']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\memory_stream.py,_SendStream,,3,['Generic[T]'],0,"['__init__', 'send_nowait', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\memory_stream.py,_ReceiveStream,,1,['Generic[T]'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\memory_stream.py,_MemoryStream,"Stream data from a writer to a reader even if they are in different threads.

Uses asyncio queues to communicate between two co-routines. This implementation
should work even if the writer and reader co-routines belong to two different
event loops (e.g. one running from an event loop in the main thread
and the other running in an event loop in a background thread).

This implementation is meant to be used with a single writer and a single reader.

This is an internal implementation to LangChain. Please do not use it directly.",3,['Generic[T]'],0,"['__init__', 'get_send_stream', 'get_receive_stream']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\run_collector.py,RunCollectorCallbackHandler,"Tracer that collects all nested runs in a list.

This tracer is useful for inspection and evaluation purposes.

Parameters
----------
name : str, default=""run-collector_callback_handler""
example_id : Optional[Union[UUID, str]], default=None
    The ID of the example being traced. It can be either a UUID or a string.",2,['BaseTracer'],1,"['__init__', '_persist_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,TracerSessionV1Base,Base class for TracerSessionV1.,0,['BaseModelV1'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,TracerSessionV1Create,Create class for TracerSessionV1.,0,['TracerSessionV1Base'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,TracerSessionV1,TracerSessionV1 schema.,0,['TracerSessionV1Base'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,TracerSessionBase,Base class for TracerSession.,0,['TracerSessionV1Base'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,TracerSession,TracerSessionV1 schema for the V2 API.,0,['TracerSessionBase'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,BaseRun,Base class for Run.,0,['BaseModelV1'],10,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,LLMRun,Class for LLMRun.,0,['BaseRun'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,ChainRun,Class for ChainRun.,0,['BaseRun'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\schemas.py,ToolRun,Class for ToolRun.,0,['BaseRun'],6,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\stdout.py,FunctionCallbackHandler,Tracer that calls a function with a single str parameter.,13,['BaseTracer'],1,"['__init__', '_persist_run', 'get_parents', 'get_breadcrumbs', '_on_chain_start', '_on_chain_end', '_on_chain_error', '_on_llm_start', '_on_llm_end', '_on_llm_error', '_on_tool_start', '_on_tool_end', '_on_tool_error']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\tracers\stdout.py,ConsoleCallbackHandler,Tracer that prints to the console.,1,['FunctionCallbackHandler'],1,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\utils\formatting.py,StrictFormatter,Formatter that checks for extra keys.,2,['Formatter'],0,"['vformat', 'validate_input_variables']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\utils\function_calling.py,FunctionDescription,Representation of a callable function to send to an LLM.,0,['TypedDict'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\utils\function_calling.py,ToolDescription,Representation of a callable function to the OpenAI API.,0,['TypedDict'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\utils\mustache.py,ChevronError,Custom exception for Chevron errors.,0,['SyntaxError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\utils\pydantic.py,_IgnoreUnserializable,"A JSON schema generator that ignores unknown types.

https://docs.pydantic.dev/latest/concepts/json_schema/#customizing-the-json-schema-generation-process",1,['GenerateJsonSchema'],0,['handle_invalid_for_json_schema'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\utils\utils.py,_NoDefaultType,Type to indicate no default value is provided.,0,[],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\_api\beta_decorator.py,LangChainBetaWarning,A class for issuing beta warnings for LangChain users.,0,['DeprecationWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\_api\deprecation.py,LangChainDeprecationWarning,A class for issuing deprecation warnings for LangChain users.,0,['DeprecationWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langchain_core\_api\deprecation.py,LangChainPendingDeprecationWarning,A class for issuing deprecation warnings for LangChain users.,0,['PendingDeprecationWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\env\_git.py,GitInfo,,0,['TypedDict'],9,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\evaluation\string_evaluator.py,StringEvaluator,"Grades the run's string input, output, and optional answer.",1,"['RunEvaluator', 'BaseModel']",5,['evaluate_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\wrappers\_openai.py,TracingExtra,,0,['TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\_internal\_beta_decorator.py,LangSmithBetaWarning,This is a warning specific to the LangSmithBeta module.,0,['UserWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\evaluation\integrations\_langchain.py,SingleEvaluatorInput,The input to a `StringEvaluator`.,0,['TypedDict'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\langsmith\evaluation\integrations\_langchain.py,LangChainStringEvaluator,"A class for wrapping a LangChain StringEvaluator.

Requires the `langchain` package to be installed.

Attributes:
    evaluator (StringEvaluator): The underlying StringEvaluator OR the name
        of the evaluator to load.

Methods:
    as_run_evaluator() -> RunEvaluator:
        Convert the LangChainStringEvaluator to a RunEvaluator.

Examples:
    Creating a simple LangChainStringEvaluator:

    >>> evaluator = LangChainStringEvaluator(""exact_match"")

    Converting a LangChainStringEvaluator to a RunEvaluator:

    >>> from langsmith.evaluation import LangChainStringEvaluator
    >>> from langchain_openai import ChatOpenAI
    >>> evaluator = LangChainStringEvaluator(
    ...     ""criteria"",
    ...     config={
    ...         ""criteria"": {
    ...             ""usefulness"": ""The prediction is useful if""
    ...             "" it is correct and/or asks a useful followup question.""
    ...         },
    ...         ""llm"": ChatOpenAI(model=""gpt-4o""),
    ...     },
    ... )
    >>> run_evaluator = evaluator.as_run_evaluator()
    >>> run_evaluator  # doctest: +ELLIPSIS
    <DynamicRunEvaluator ...>

    Customizing the LLM model used by the evaluator:

    >>> from langsmith.evaluation import LangChainStringEvaluator
    >>> from langchain_anthropic import ChatAnthropic
    >>> evaluator = LangChainStringEvaluator(
    ...     ""criteria"",
    ...     config={
    ...         ""criteria"": {
    ...             ""usefulness"": ""The prediction is useful if""
    ...             "" it is correct and/or asks a useful followup question.""
    ...         },
    ...         ""llm"": ChatAnthropic(model=""claude-3-opus-20240229""),
    ...     },
    ... )
    >>> run_evaluator = evaluator.as_run_evaluator()
    >>> run_evaluator  # doctest: +ELLIPSIS
    <DynamicRunEvaluator ...>

    Using the `evaluate` API with different evaluators:
    >>> def prepare_data(run: Run, example: Example):
    ...     # Convert the evaluation data into the format expected by the evaluator
    ...     # Only required for datasets with multiple inputs/output keys
    ...     return {
    ...         ""prediction"": run.outputs[""prediction""],
    ...         ""reference"": example.outputs[""answer""],
    ...         ""input"": str(example.inputs),
    ...     }
    >>> import re
    >>> from langchain_anthropic import ChatAnthropic
    >>> import langsmith
    >>> from langsmith.evaluation import LangChainStringEvaluator, evaluate
    >>> criteria_evaluator = LangChainStringEvaluator(
    ...     ""criteria"",
    ...     config={
    ...         ""criteria"": {
    ...             ""usefulness"": ""The prediction is useful if it is correct""
    ...             "" and/or asks a useful followup question.""
    ...         },
    ...         ""llm"": ChatAnthropic(model=""claude-3-opus-20240229""),
    ...     },
    ...     prepare_data=prepare_data,
    ... )
    >>> embedding_evaluator = LangChainStringEvaluator(""embedding_distance"")
    >>> exact_match_evaluator = LangChainStringEvaluator(""exact_match"")
    >>> regex_match_evaluator = LangChainStringEvaluator(
    ...     ""regex_match"", config={""flags"": re.IGNORECASE}, prepare_data=prepare_data
    ... )
    >>> scoring_evaluator = LangChainStringEvaluator(
    ...     ""labeled_score_string"",
    ...     config={
    ...         ""criteria"": {
    ...             ""accuracy"": ""Score 1: Completely inaccurate\nScore 5: Somewhat accurate\nScore 10: Completely accurate""
    ...         },
    ...         ""normalize_by"": 10,
    ...         ""llm"": ChatAnthropic(model=""claude-3-opus-20240229""),
    ...     },
    ...     prepare_data=prepare_data,
    ... )
    >>> string_distance_evaluator = LangChainStringEvaluator(
    ...     ""string_distance"",
    ...     config={""distance_metric"": ""levenshtein""},
    ...     prepare_data=prepare_data,
    ... )
    >>> from langsmith import Client
    >>> client = Client()
    >>> results = evaluate(
    ...     lambda inputs: {""prediction"": ""foo""},
    ...     data=client.list_examples(dataset_name=""Evaluate Examples"", limit=1),
    ...     evaluators=[
    ...         embedding_evaluator,
    ...         criteria_evaluator,
    ...         exact_match_evaluator,
    ...         regex_match_evaluator,
    ...         scoring_evaluator,
    ...         string_distance_evaluator,
    ...     ],
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...",2,[],0,"['__init__', 'as_run_evaluator']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\gather.py,Usage,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\gen_matcher_classes.py,CleanseFullTypeNames,,4,['cst.CSTTransformer'],0,"['leave_Call', 'leave_Attribute', 'leave_Name', 'leave_SubscriptElement']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\gen_matcher_classes.py,RemoveTypesFromGeneric,,2,['cst.CSTTransformer'],0,"['__init__', 'leave_SubscriptElement']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\gen_matcher_classes.py,MatcherClassToLibCSTClass,,1,['cst.CSTTransformer'],0,['leave_SimpleString'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\gen_matcher_classes.py,AddLogicMatchersToUnions,,1,['cst.CSTTransformer'],0,['leave_Subscript'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\gen_matcher_classes.py,AddWildcardsToSequenceUnions,,3,['cst.CSTTransformer'],0,"['__init__', 'visit_Subscript', 'leave_Subscript']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\gen_matcher_classes.py,Alias,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\gen_matcher_classes.py,Field,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\_cli.py,ExecutionResult,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\_cli.py,ExecutionConfig,,0,[],8,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\_cli.py,Progress,,5,[],1,"['__init__', 'print', '_human_seconds', 'estimate_completion', 'clear']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\_cli.py,ParallelTransformResult,"The result of running
:func:`~libcst.codemod.parallel_exec_transform_with_prettyprint` against
a series of files. This is a simple summary, with counts for number of
successfully codemodded files, number of files that we failed to codemod,
number of warnings generated when running the codemod across the files, and
the number of files that we skipped when running the codemod.",0,[],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\_dummy_pool.py,DummyPool,Synchronous dummy `multiprocessing.Pool` analogue.,4,[],0,"['__init__', 'imap_unordered', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\helpers\module.py,ModuleNameAndPackage,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\helpers\_template.py,TemplateTransformer,,14,['cst.CSTTransformer'],0,"['__init__', 'leave_Name', 'leave_Annotation', 'leave_AssignTarget', 'leave_Param', 'leave_Parameters', 'leave_Arg', 'leave_SimpleStatementLine', 'leave_Expr', 'leave_SimpleStatementSuite', 'leave_IndentedBlock', 'leave_Index', 'leave_SubscriptElement', 'leave_Decorator']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\helpers\_template.py,TemplateChecker,,2,['cst.CSTVisitor'],0,"['__init__', 'visit_Name']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\_visitors.py,MatchDecoratorMismatch,,2,['Exception'],0,"['__init__', '__reduce__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\_visitors.py,MatcherDecoratableTransformer,"This class provides all of the features of a :class:`libcst.CSTTransformer`, and
additionally supports various decorators to control when methods get called when
traversing a tree. Use this instead of a :class:`libcst.CSTTransformer` if you
wish to do more powerful decorator-based visiting.",10,['CSTTransformer'],0,"['__init__', 'on_visit', 'on_leave', 'on_visit_attribute', 'on_leave_attribute', 'matches', 'findall', 'extract', 'extractall', 'replace']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\_visitors.py,MatcherDecoratableVisitor,"This class provides all of the features of a :class:`libcst.CSTVisitor`, and
additionally supports various decorators to control when methods get called
when traversing a tree. Use this instead of a :class:`libcst.CSTVisitor` if
you wish to do more powerful decorator-based visiting.",10,['CSTVisitor'],0,"['__init__', 'on_visit', 'on_leave', 'on_visit_attribute', 'on_leave_attribute', 'matches', 'findall', 'extract', 'extractall', 'replace']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\accessor_provider.py,AccessorProvider,,1,['VisitorMetadataProvider[str]'],0,['on_visit'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\base_provider.py,BaseMetadataProvider,"The low-level base class for all metadata providers. This class should be
extended for metadata providers that are not visitor-based.

This class is generic. A subclass of ``BaseMetadataProvider[T]`` will
provider metadata of type ``T``.",5,"['MetadataDependent', 'Generic[_ProvidedMetadataT]']",2,"['__init__', '_gen', '_gen_impl', 'set_metadata', 'get_metadata']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\base_provider.py,VisitorMetadataProvider,"The low-level base class for all non-batchable visitor-based metadata
providers. Inherits from :class:`~libcst.CSTVisitor`.

This class is generic. A subclass of ``VisitorMetadataProvider[T]`` will
provider metadata of type ``T``.",1,"['CSTVisitor', 'BaseMetadataProvider[_ProvidedMetadataT]']",0,['_gen_impl'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\base_provider.py,BatchableMetadataProvider,"The low-level base class for all batchable visitor-based metadata providers.
Batchable providers should be preferred when possible as they are more
efficient to run compared to non-batchable visitor-based providers.
Inherits from :class:`~libcst.BatchableCSTVisitor`.

This class is generic. A subclass of ``BatchableMetadataProvider[T]`` will
provider metadata of type ``T``.",1,"['BatchableCSTVisitor', 'BaseMetadataProvider[_ProvidedMetadataT]']",0,['_gen_impl'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\parent_node_provider.py,ParentNodeVisitor,,2,['cst.CSTVisitor'],0,"['__init__', 'on_leave']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\parent_node_provider.py,ParentNodeProvider,,1,['BatchableMetadataProvider[cst.CSTNode]'],0,['visit_Module'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\testing\utils.py,BaseTestMeta,,1,['type'],0,['__new__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\testing\utils.py,UnitTest,,0,['TestCase'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\test_add_slots.py,A,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\test_add_slots.py,AddSlotsTest,,2,['UnitTest'],0,"['test_pickle', 'test_prevents_slots_overlap']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\test_batched_visitor.py,BatchedVisitorTest,,2,['UnitTest'],0,"['test_simple', 'test_all_visits']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\test_deep_replace.py,DeepReplaceTest,,5,['UnitTest'],0,"['test_deep_replace_simple', 'test_deep_replace_complex', 'test_deep_replace_identity', 'test_deep_remove_complex', 'test_with_deep_changes_complex']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\test_e2e.py,PrintToPPrintCommand,,1,['VisitorBasedCodemodCommand'],0,['leave_Call'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\test_e2e.py,ToolE2ETest,,1,['TestCase'],0,['test_leaky_codemod'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\test_tool.py,PrettyPrintNodesTest,,8,['UnitTest'],0,"['test_full_tree', 'test_hidden_whitespace', 'test_hidden_defaults', 'test_hidden_whitespace_and_defaults', 'test_hidden_syntax', 'test_hidden_whitespace_and_syntax', 'test_hidden_defaults_and_syntax', 'test_hidden_whitespace_and_defaults_and_syntax']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\test_visitor.py,VisitorTest,,2,['UnitTest'],0,"['test_visitor', 'test_transformer']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\detect_config.py,ConfigDetectionResult,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\python_parser.py,PythonCSTParser,,3,"['BaseParser[Token, TokenType, Any]']",3,"['__init__', 'convert_nonterminal', 'convert_terminal']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codegen\tests\test_codegen_clean.py,TestCodegenClean,,4,['UnitTest'],0,"['assert_code_matches', 'test_codegen_clean_visitor_functions', 'test_codegen_clean_matcher_classes', 'test_codegen_clean_return_types']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\add_pyre_directive.py,AddPyreDirectiveCommand,,3,"['VisitorBasedCodemodCommand', 'ABC']",1,"['__init__', 'visit_Comment', 'leave_Module']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\add_pyre_directive.py,AddPyreStrictCommand,"Given a source file, we'll add the strict tag if the file doesn't already
contain it.",0,['AddPyreDirectiveCommand'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\add_pyre_directive.py,AddPyreUnsafeCommand,"Given a source file, we'll add the unsafe tag if the file doesn't already
contain it.",0,['AddPyreDirectiveCommand'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\convert_namedtuple_to_dataclass.py,ConvertNamedTupleToDataclassCommand,"Convert NamedTuple class declarations to Python 3.7 dataclasses.

This only performs a conversion at the class declaration level.
It does not perform type annotation conversions, nor does it convert
NamedTuple-specific attributes and methods.",1,['VisitorBasedCodemodCommand'],3,['leave_ClassDef'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\convert_percent_format_to_fstring.py,EscapeStringQuote,,2,['cst.CSTTransformer'],0,"['__init__', 'leave_SimpleString']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\convert_percent_format_to_fstring.py,ConvertPercentFormatStringCommand,,1,['VisitorBasedCodemodCommand'],1,['leave_BinaryOperation'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\fix_pyre_directives.py,FixPyreDirectivesCommand,"Given a source file, we'll move the any strict or unsafe tag to the top of the
file if it contains one. Also tries to fix typo'd directives.",5,['VisitorBasedCodemodCommand'],2,"['__init__', 'visit_Module_header', 'leave_Module_header', 'leave_EmptyLine', 'leave_Module']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\noop.py,NOOPCommand,,1,['CodemodCommand'],1,['transform_module_impl'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\remove_pyre_directive.py,RemovePyreDirectiveCommand,,2,"['VisitorBasedCodemodCommand', 'ABC']",1,"['__init__', 'leave_EmptyLine']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\remove_pyre_directive.py,RemovePyreStrictCommand,"Given a source file, we'll remove the any strict tag if the file already
contains it.",0,['RemovePyreDirectiveCommand'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\remove_pyre_directive.py,RemovePyreUnsafeCommand,"Given a source file, we'll remove the any unsafe tag if the file already
contains it.",0,['RemovePyreDirectiveCommand'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\remove_unused_imports.py,RemoveUnusedImportsCommand,"Remove all unused imports from a file based on scope analysis.

This command analyses individual files in isolation and does not attempt
to track cross-references between them. If a symbol is imported in a file
but otherwise unused in it, that import will be removed even if it is being
referenced from another file.",5,['VisitorBasedCodemodCommand'],2,"['__init__', 'visit_Module', 'visit_Import', 'visit_ImportFrom', '_handle_import']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\tests\test_runner.py,TestRunner,,4,['CodemodTest'],0,"['test_runner_default', 'test_runner_interrupted', 'test_runner_skip', 'test_runner_failure']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\_gather_exports.py,GatherExportsVisitor,"Gathers all explicit exports in a module and stores them as attributes on the
instance. Intended to be instantiated and passed to a :class:`~libcst.Module`
:meth:`~libcst.CSTNode.visit` method in order to gather up information about
exports specified in an ``__all__`` variable inside a module.

After visiting a module the following attributes will be populated:

 explicit_exported_objects
  A sequence of strings representing objects that the module exports
  directly. Note that when ``__all__`` is absent, this attribute does not
  store default exported objects by name.

For more information on ``__all__``, please see Python's `Modules Documentation
<https://docs.python.org/3/tutorial/modules.html>`_.",14,['ContextAwareVisitor'],0,"['__init__', 'visit_AnnAssign', 'visit_AugAssign', 'visit_Assign', '_handle_assign_target', 'visit_List', 'leave_List', 'visit_Tuple', 'leave_Tuple', 'visit_Set', 'leave_Set', 'visit_SimpleString', 'visit_ConcatenatedString', '_handle_string_export']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\_gather_global_names.py,GatherGlobalNamesVisitor,"Gathers all globally accessible names defined in a module and stores them as
attributes on the instance.
Intended to be instantiated and passed to a :class:`~libcst.Module`
:meth:`~libcst.CSTNode.visit` method in order to gather up information about
names defined on a module. Note that this is not a substitute for scope
analysis or qualified name support. Please see :ref:`libcst-scope-tutorial`
for a more robust way of determining the qualified name and definition for
an arbitrary node.
Names that are globally accessible through imports are currently not included
but can be retrieved with GatherImportsVisitor.

After visiting a module the following attributes will be populated:

 global_names
  A sequence of strings representing global variables defined in the module
  toplevel.
 class_names
  A sequence of strings representing classes defined in the module toplevel.
 function_names
  A sequence of strings representing functions defined in the module toplevel.",7,['ContextAwareVisitor'],0,"['__init__', 'visit_ClassDef', 'leave_ClassDef', 'visit_FunctionDef', 'leave_FunctionDef', 'visit_Assign', 'visit_AnnAssign']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\_gather_imports.py,_GatherImportsMixin,A Mixin class for tracking visited imports.,3,['ContextAwareVisitor'],0,"['__init__', '_handle_Import', '_handle_ImportFrom']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\_gather_imports.py,GatherImportsVisitor,"Gathers all imports in a module and stores them as attributes on the instance.
Intended to be instantiated and passed to a :class:`~libcst.Module`
:meth:`~libcst.CSTNode.visit` method in order to gather up information about
imports on a module. Note that this is not a substitute for scope analysis or
qualified name support. Please see :ref:`libcst-scope-tutorial` for a more
robust way of determining the qualified name and definition for an arbitrary
node.

After visiting a module the following attributes will be populated:

 module_imports
  A sequence of strings representing modules that were imported directly, such as
  in the case of ``import typing``. Each module directly imported but not aliased
  will be included here.
 object_mapping
  A mapping of strings to sequences of strings representing modules where we
  imported objects from, such as in the case of ``from typing import Optional``.
  Each from import that was not aliased will be included here, where the keys of
  the mapping are the module we are importing from, and the value is a
  sequence of objects we are importing from the module.
 module_aliases
  A mapping of strings representing modules that were imported and aliased,
  such as in the case of ``import typing as t``. Each module imported this
  way will be represented as a key in this mapping, and the value will be
  the local alias of the module.
 alias_mapping
  A mapping of strings to sequences of tuples representing modules where we
  imported objects from and aliased using ``as`` syntax, such as in the case
  of ``from typing import Optional as opt``. Each from import that was aliased
  will be included here, where the keys of the mapping are the module we are
  importing from, and the value is a tuple representing the original object
  name and the alias.
 all_imports
  A collection of all :class:`~libcst.Import` and :class:`~libcst.ImportFrom`
  statements that were encountered in the module.",3,['_GatherImportsMixin'],0,"['__init__', 'visit_Import', 'visit_ImportFrom']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\_gather_unused_imports.py,GatherUnusedImportsVisitor,"Collects all imports from a module not directly used in the same module.
Intended to be instantiated and passed to a :class:`libcst.Module`
:meth:`~libcst.CSTNode.visit` method to process the full module.

Note that imports that are only used indirectly (from other modules) are
still collected.

After visiting a module the attribute ``unused_imports`` will contain a
set of unused :class:`~libcst.ImportAlias` objects, paired with their
parent import node.",8,['ContextAwareVisitor'],1,"['__init__', 'visit_Module', 'visit_Import', 'visit_ImportFrom', 'handle_import', 'leave_Module', 'filter_unused_imports', 'is_in_use']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\commands\tests\test_unnecessary_format_string.py,TestUnnecessaryFormatString,,1,['CodemodTest'],1,['test_replace'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\tests\test_gather_comments.py,TestGatherCommentsVisitor,,3,['UnitTest'],0,"['gather_comments', 'test_no_comments', 'test_noqa_comments']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\tests\test_gather_exports.py,TestGatherExportsVisitor,,12,['UnitTest'],0,"['gather_exports', 'test_gather_noop', 'test_gather_exports_simple', 'test_gather_exports_simple2', 'test_gather_exports_simple_set', 'test_gather_exports_simple_tuple', 'test_gather_exports_simple_annotated', 'test_gather_exports_ignore_invalid_1', 'test_gather_exports_ignore_invalid_2', 'test_gather_exports_ignore_valid_1', 'test_gather_exports_ignore_valid_2', 'test_gather_exports_ignore_valid_3']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\tests\test_gather_global_names.py,TestGatherGlobalNamesVisitor,,4,['UnitTest'],0,"['gather_global_names', 'test_gather_nothing', 'test_globals', 'test_omit_nested']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\tests\test_gather_imports.py,TestGatherImportsVisitor,,10,['UnitTest'],0,"['gather_imports', 'test_gather_nothing', 'test_gather_module', 'test_gather_aliased_module', 'test_gather_object', 'test_gather_object_disjoint', 'test_gather_aliased_object', 'test_gather_aliased_object_disjoint', 'test_gather_aliased_object_mixed', 'test_gather_relative_object']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\tests\test_gather_string_annotation_names.py,TestGatherNamesFromStringAnnotationsVisitor,,7,['UnitTest'],0,"['gather_names', 'test_no_annotations', 'test_simple_string_annotations', 'test_concatenated_string_annotations', 'test_typevars', 'test_complex', 'test_dotted_names']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\codemod\visitors\tests\test_gather_unused_imports.py,TestGatherUnusedImportsVisitor,,10,['UnitTest'],0,"['gather_imports', 'test_no_imports', 'test_dotted_imports', 'test_alias', 'test_import_complex', 'test_import_from_complex', 'test_exports', 'test_string_annotation', 'test_typevars', 'test_future']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\helpers\tests\test_paths.py,PathsTest,,1,['UnitTest'],0,['test_chdir'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\helpers\tests\test_template.py,TemplateTest,,13,['UnitTest'],0,"['dedent', 'code', 'test_simple_module', 'test_simple_statement', 'test_simple_expression', 'test_annotation', 'test_assign_target', 'test_parameters', 'test_args', 'test_statement', 'test_suite', 'test_subscript', 'test_decorators']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\tests\test_extract.py,MatchersExtractTest,,17,['UnitTest'],0,"['_make_coderange', 'test_extract_sentinel', 'test_extract_tautology', 'test_extract_simple', 'test_extract_multiple', 'test_extract_predicates', 'test_extract_metadata', 'test_extract_precedence_parent', 'test_extract_precedence_sequence', 'test_extract_precedence_sequence_wildcard', 'test_extract_optional_wildcard', 'test_extract_optional_wildcard_head', 'test_extract_optional_wildcard_tail', 'test_extract_optional_wildcard_present', 'test_extract_sequence', 'test_extract_sequence_element', 'test_extract_sequence_multiple_wildcards']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\tests\test_findall.py,MatchersFindAllTest,,6,['UnitTest'],0,"['assertNodeSequenceEqual', 'test_findall_with_sentinels', 'test_simple_findall', 'test_findall_with_metadata_wrapper', 'test_findall_with_visitors', 'test_findall_with_transformers']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\tests\test_findall.py,MatchersExtractAllTest,,1,['UnitTest'],0,['test_extractall_simple'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\tests\test_matchers.py,MatchersMatcherTest,,34,['UnitTest'],0,"['test_simple_matcher_true', 'test_simple_matcher_false', 'test_complex_matcher_true', 'test_complex_matcher_false', 'test_type_of_matcher_true', 'test_type_of_matcher_false', 'test_or_matcher_true', 'test_or_matcher_false', 'test_or_operator_matcher_true', 'test_or_operator_matcher_false', 'test_zero_or_more_matcher_no_args_true', 'test_at_least_n_matcher_no_args_true', 'test_at_least_n_matcher_no_args_false', 'test_zero_or_more_matcher_args_true', 'test_zero_or_more_matcher_args_false', 'test_at_least_n_matcher_args_true', 'test_at_least_n_matcher_args_false', 'test_at_most_n_matcher_no_args_true', 'test_at_most_n_matcher_no_args_false', 'test_at_most_n_matcher_args_true', 'test_at_most_n_matcher_args_false', 'test_lambda_matcher_true', 'test_lambda_matcher_false', 'test_regex_matcher_true', 'test_regex_matcher_false', 'test_and_matcher_true', 'test_and_matcher_false', 'test_and_operator_matcher_true', 'test_and_operator_matcher_false', 'test_does_not_match_true', 'test_does_not_match_operator_true', 'test_does_not_match_false', 'test_does_not_match_operator_false', 'test_inverse_inverse_is_identity']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\tests\test_matchers_with_metadata.py,MatchersMetadataTest,,12,['UnitTest'],0,"['_make_fixture', '_make_coderange', 'test_simple_matcher_true', 'test_simple_matcher_false', 'test_predicate_logic', 'test_predicate_logic_operators', 'test_predicate_logic_on_attributes', 'test_predicate_logic_operators_on_attributes', 'test_lambda_metadata_matcher', 'test_lambda_metadata_matcher_with_unresolved_metadata', 'test_lambda_metadata_matcher_with_no_metadata', 'test_lambda_metadata_matcher_operators']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\tests\test_matchers_with_metadata.py,MatchersVisitorMetadataTest,,5,['UnitTest'],0,"['_make_fixture', 'test_matches_on_visitors', 'test_matches_on_transformers', 'test_matches_decorator_on_visitors', 'test_matches_decorator_on_transformers']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\tests\test_replace.py,MatchersReplaceTest,,11,['UnitTest'],0,"['test_replace_sentinel', 'test_replace_noop', 'test_replace_simple', 'test_replace_simple_sentinel', 'test_replace_actual', 'test_replace_add_one', 'test_replace_add_one_to_foo_args', 'test_replace_sequence_extract', 'test_replace_metadata', 'test_replace_metadata_on_transform', 'test_replace_updated_node_changes']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\matchers\tests\test_visitors.py,MatchersVisitLeaveDecoratorTypingTest,,38,['UnitTest'],0,"['test_valid_collector_simple', 'test_valid_transformer_simple', 'test_valid_transformer_base_class', 'test_valid_collector_visit_union', 'test_valid_transformer_visit_union', 'test_valid_collector_visit_superclass', 'test_valid_transformer_visit_superclass', 'test_valid_collector_leave_union', 'test_valid_transformer_leave_union', 'test_valid_collector_leave_superclass', 'test_valid_transformer_leave_superclass', 'test_valid_transformer_leave_return_maybe', 'test_valid_transformer_leave_return_remove', 'test_invalid_collector_visit_return', 'test_invalid_transformer_visit_return', 'test_invalid_transformer_visit_num_params', 'test_invalid_collector_visit_num_params', 'test_invalid_transformer_leave_num_params', 'test_invalid_collector_leave_num_params', 'test_invalid_collector_leave_return', 'test_invalid_transformer_leave_return_invalid_superclass', 'test_invalid_transformer_leave_return_wrong_type', 'test_invalid_transformer_leave_return_invalid_maybe', 'test_invalid_transformer_leave_return_invalid_remove', 'test_invalid_transformer_leave_return_invalid_union', 'test_invalid_collector_visit_union', 'test_invalid_transformer_visit_union', 'test_invalid_collector_visit_superclass', 'test_invalid_transformer_visit_superclass', 'test_invalid_collector_leave_union', 'test_invalid_transformer_leave_union', 'test_invalid_collector_leave_superclass', 'test_invalid_transformer_leave_superclass', 'test_bad_visit_collecter_decorator', 'test_bad_leave_collecter_decorator', 'test_bad_visit_transform_decorator', 'test_bad_leave_transform_decorator', 'test_pickleable_exception']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\tests\test_base_provider.py,BaseMetadataProviderTest,,4,['UnitTest'],0,"['test_visitor_provider', 'test_batchable_provider', 'test_lazy_visitor_provider', 'testlazy_batchable_provider']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\tests\test_file_path_provider.py,FilePathProviderTest,,3,['UnitTest'],0,"['setUp', 'test_provider_cache', 'test_visitor']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\tests\test_metadata_provider.py,MetadataProviderTest,,10,['UnitTest'],0,"['test_visitor_provider', 'test_batched_provider', 'test_mixed_providers', 'test_inherited_metadata', 'test_provider_inherited_metadata', 'test_batchable_provider_inherited_metadata', 'test_self_metadata', 'test_unset_metadata', 'test_undeclared_metadata', 'test_circular_dependency']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\tests\test_metadata_wrapper.py,MetadataWrapperTest,,7,['UnitTest'],0,"['test_copies_tree', 'test_unsafe_skip_copy', 'test_equality_by_identity', 'test_hash_by_identity', 'test_metadata_cache', 'test_resolve_provider_twice', 'test_resolve_dependent_provider_twice']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\tests\test_position_provider.py,PositionProviderTest,,3,['UnitTest'],0,"['test_visitor_provider', 'test_equal_range', 'test_batchable_provider']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\tests\test_position_provider.py,PositionProvidingCodegenStateTest,,8,['UnitTest'],0,"['test_codegen_initial_position', 'test_codegen_add_token', 'test_codegen_add_tokens', 'test_codegen_add_newline', 'test_codegen_add_indent_tokens', 'test_codegen_decrease_indent', 'test_whitespace_inclusive_position', 'test_position']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\tests\test_span_provider.py,SpanProvidingCodegenStateTest,,5,['UnitTest'],0,"['test_initial_position', 'test_add_token', 'test_add_non_ascii_token', 'test_add_indent_tokens', 'test_span']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\metadata\tests\test_span_provider.py,ByteSpanPositionProviderTest,,2,['UnitTest'],0,"['test_visitor_provider', 'test_batchable_provider']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\pyre\simple_class.py,Item,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\tests\pyre\simple_class.py,ItemCollector,,1,[],0,['get_items'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_nodes\tests\base.py,_CSTCodegenPatchTarget,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_nodes\tests\base.py,_NOOPVisitor,,0,['CSTTransformer'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_nodes\tests\base.py,CSTNodeTest,,10,['UnitTest'],0,"['setUp', 'validate_node', 'assert_invalid', 'assert_invalid_types', '__assert_codegen', '__assert_children_match_codegen', '__derive_children_from_codegen', '__assert_children_match_fields', '__assert_visit_returns_identity', 'assert_parses']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_nodes\tests\base.py,DummyIndentedBlock,"A stripped-down version of cst.IndentedBlock that only sets/clears the indentation
state for the purpose of testing cst.IndentWhitespace in isolation.",2,['cst.CSTNode'],2,"['_codegen_impl', '_visit_and_replace_children']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_nodes\tests\test_simple_string.py,TestSimpleString,,1,['unittest.TestCase'],0,['test_quote'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\utils.py,Version,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\utils.py,PythonVersionInfo,,7,[],2,"['__gt__', '__ge__', '__lt__', '__le__', '__eq__', '__ne__', '__hash__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\tests\test_config.py,ConfigTest,,1,['UnitTest'],0,['test_pick_compatible'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\config.py,AutoConfig,A sentinel value used in PartialParserConfig,1,['Enum'],1,['__repr__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\config.py,PartialParserConfig,"An optional object that can be supplied to the parser entrypoints (e.g.
:func:`parse_module`) to configure the parser.

Unspecified fields will be inferred from the input source code or from the execution
environment.

>>> import libcst as cst
>>> tree = cst.parse_module(""abc"")
>>> tree.bytes
b'abc'
>>> # override the default utf-8 encoding
... tree = cst.parse_module(""abc"", cst.PartialParserConfig(encoding=""utf-32""))
>>> tree.bytes
b'\xff\xfe\x00\x00a\x00\x00\x00b\x00\x00\x00c\x00\x00\x00'",2,[],6,"['__post_init__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,WithLeadingWhitespace,,0,['Generic[_T]'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,SimpleStatementPartial,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,SlicePartial,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,AttributePartial,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,ArglistPartial,,0,[],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,CallPartial,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,SubscriptPartial,,0,[],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,AnnAssignPartial,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,AugAssignPartial,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,AssignPartial,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,ParamStarPartial,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,FuncdefPartial,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,DecoratorPartial,,0,[],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,ImportPartial,,0,[],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,ImportRelativePartial,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,FormattedStringConversionPartial,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,FormattedStringFormatSpecPartial,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\partials.py,ExceptClausePartial,,0,[],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\production.py,Production,,1,[],4,['__str__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\py_config.py,BaseWhitespaceParserConfig,"Represents the subset of ParserConfig that the whitespace parser requires. This
makes calling the whitespace parser in tests with a mocked configuration easier.",0,['abc.ABC'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\py_config.py,MockWhitespaceParserConfig,An internal type used by unit tests.,0,['BaseWhitespaceParserConfig'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\py_config.py,ParserConfig,"An internal configuration object that the python parser passes around. These
values are global to the parsed code and should not change during the lifetime
of the parser object.",0,['BaseWhitespaceParserConfig'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\py_token.py,Token,,0,[],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\types\py_whitespace_state.py,WhitespaceState,"A frequently mutated store of the whitespace parser's current state. This object
must be cloned prior to speculative parsing.

This is in contrast to the `config` object each whitespace parser function takes,
which is frozen and never mutated.

Whitespace parsing works by mutating this state object. By encapsulating saving, and
re-using state objects inside the top-level python parser, the whitespace parser is
able to be reentrant. One 'convert' function can consume part of the whitespace, and
another 'convert' function can consume the rest, depending on who owns what
whitespace.

This is similar to the approach you might take to parse nested languages (e.g.
JavaScript inside of HTML). We're treating whitespace as a separate language and
grammar from the rest of Python's grammar.",0,[],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\pgen2\generator.py,DFAPlan,"Plans are used for the parser to create stack nodes and do the proper
DFA state transitions.",2,[],0,"['__init__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\pgen2\generator.py,DFAState,"The DFAState object is the core class for pretty much anything. DFAState
are the vertices of an ordered graph while arcs and transitions are the
edges.

Arcs are the initial edges, where most DFAStates are not connected and
transitions are then calculated to connect the DFA state machines that have
different nonterminals.",5,['Generic[_TokenTypeT]'],0,"['__init__', 'add_arc', 'unifystate', '__eq__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\pgen2\generator.py,ReservedString,"Most grammars will have certain keywords and operators that are mentioned
in the grammar as strings (e.g. ""if"") and not token types (e.g. NUMBER).
This class basically is the former.",2,[],0,"['__init__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\pgen2\generator.py,Grammar,"Once initialized, this class supplies the grammar tables for the
parsing engine implemented by parse.py.  The parsing engine
accesses the instance variables directly.

The only important part in this parsers are dfas and transitions between
dfas.",1,['Generic[_TokenTypeT]'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\pgen2\grammar_parser.py,NFAArc,,2,[],0,"['__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\pgen2\grammar_parser.py,NFAState,,3,[],0,"['__init__', 'add_arc', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\pgen2\grammar_parser.py,GrammarParser,The parser for Python grammar files.,9,[],0,"['__init__', 'parse', '_parse_rhs', '_parse_items', '_parse_item', '_parse_atom', '_expect', '_gettoken', '_raise_error']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\python\py_token.py,TokenType,,1,[],2,['__repr__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\libcst\_parser\parso\python\py_token.py,PythonTokenTypes,"Basically an enum, but Python 2 doesn't have enums in the standard library.",0,[],15,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\markdown_it\rules_block\state_block.py,StateBlock,,13,['StateBase'],0,"['__init__', '__repr__', 'push', 'isEmpty', 'skipEmptyLines', 'skipSpaces', 'skipSpacesBack', 'skipChars', 'skipCharsStr', 'skipCharsBack', 'skipCharsStrBack', 'getLines', 'is_code_block']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\markdown_it\rules_core\linkify.py,_LinkType,,0,['Protocol'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\markdown_it\rules_core\state_core.py,StateCore,,1,['StateBase'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\markdown_it\rules_inline\state_inline.py,Delimiter,,0,[],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\markdown_it\rules_inline\state_inline.py,StateInline,,5,['StateBase'],0,"['__init__', '__repr__', 'pushPending', 'push', 'scanDelims']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\calculus.py,CalculusMethods,,0,['object'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\extrapolation.py,levin_class,"This interface implements Levin's (nonlinear) sequence transformation for
convergence acceleration and summation of divergent series. It performs
better than the Shanks/Wynn-epsilon algorithm for logarithmic convergent
or alternating divergent series.

Let *A* be the series we want to sum:

.. math ::

    A = \sum_{k=0}^{\infty} a_k

Attention: all `a_k` must be non-zero!

Let `s_n` be the partial sums of this series:

.. math ::

    s_n = \sum_{k=0}^n a_k.

**Methods**

Calling ``levin`` returns an object with the following methods.

``update(...)`` works with the list of individual terms `a_k` of *A*, and
``update_step(...)`` works with the list of partial sums `s_k` of *A*:

.. code ::

    v, e = ...update([a_0, a_1,..., a_k])
    v, e = ...update_psum([s_0, s_1,..., s_k])

``step(...)`` works with the individual terms `a_k` and ``step_psum(...)``
works with the partial sums `s_k`:

.. code ::

    v, e = ...step(a_k)
    v, e = ...step_psum(s_k)

*v* is the current estimate for *A*, and *e* is an error estimate which is
simply the difference between the current estimate and the last estimate.
One should not mix ``update``, ``update_psum``, ``step`` and ``step_psum``.

**A word of caution**

One can only hope for good results (i.e. convergence acceleration or
resummation) if the `s_n` have some well defind asymptotic behavior for
large `n` and are not erratic or random. Furthermore one usually needs very
high working precision because of the numerical cancellation. If the working
precision is insufficient, levin may produce silently numerical garbage.
Furthermore even if the Levin-transformation converges, in the general case
there is no proof that the result is mathematically sound. Only for very
special classes of problems one can prove that the Levin-transformation
converges to the expected result (for example Stieltjes-type integrals).
Furthermore the Levin-transform is quite expensive (i.e. slow) in comparison
to Shanks/Wynn-epsilon, Richardson & co.
In summary one can say that the Levin-transformation is powerful but
unreliable and that it may need a copious amount of working precision.

The Levin transform has several variants differing in the choice of weights.
Some variants are better suited for the possible flavours of convergence
behaviour of *A* than other variants:

.. code ::

   convergence behaviour   levin-u   levin-t   levin-v   shanks/wynn-epsilon

   logarithmic               +         -         +           -
   linear                    +         +         +           +
   alternating divergent     +         +         +           +

     ""+"" means the variant is suitable,""-"" means the variant is not suitable;
     for comparison the Shanks/Wynn-epsilon transform is listed, too.

The variant is controlled though the variant keyword (i.e. ``variant=""u""``,
``variant=""t""`` or ``variant=""v""``). Overall ""u"" is probably the best choice.

Finally it is possible to use the Sidi-S transform instead of the Levin transform
by using the keyword ``method='sidi'``. The Sidi-S transform works better than the
Levin transformation for some divergent series (see the examples).

Parameters:

.. code ::

   method      ""levin"" or ""sidi"" chooses either the Levin or the Sidi-S transformation
   variant     ""u"",""t"" or ""v"" chooses the weight variant.

The Levin transform is also accessible through the nsum interface.
``method=""l""`` or ``method=""levin""`` select the normal Levin transform while
``method=""sidi""``
selects the Sidi-S transform. The variant is in both cases selected through the
levin_variant keyword. The stepsize in :func:`~mpmath.nsum` must not be chosen too large, otherwise
it will miss the point where the Levin transform converges resulting in numerical
overflow/garbage. For highly divergent series a copious amount of working precision
must be chosen.

**Examples**

First we sum the zeta function::

    >>> from mpmath import mp
    >>> mp.prec = 53
    >>> eps = mp.mpf(mp.eps)
    >>> with mp.extraprec(2 * mp.prec): # levin needs a high working precision
    ...     L = mp.levin(method = ""levin"", variant = ""u"")
    ...     S, s, n = [], 0, 1
    ...     while 1:
    ...         s += mp.one / (n * n)
    ...         n += 1
    ...         S.append(s)
    ...         v, e = L.update_psum(S)
    ...         if e < eps:
    ...             break
    ...         if n > 1000: raise RuntimeError(""iteration limit exceeded"")
    >>> print(mp.chop(v - mp.pi ** 2 / 6))
    0.0
    >>> w = mp.nsum(lambda n: 1 / (n*n), [1, mp.inf], method = ""levin"", levin_variant = ""u"")
    >>> print(mp.chop(v - w))
    0.0

Now we sum the zeta function outside its range of convergence
(attention: This does not work at the negative integers!)::

    >>> eps = mp.mpf(mp.eps)
    >>> with mp.extraprec(2 * mp.prec): # levin needs a high working precision
    ...     L = mp.levin(method = ""levin"", variant = ""v"")
    ...     A, n = [], 1
    ...     while 1:
    ...         s = mp.mpf(n) ** (2 + 3j)
    ...         n += 1
    ...         A.append(s)
    ...         v, e = L.update(A)
    ...         if e < eps:
    ...             break
    ...         if n > 1000: raise RuntimeError(""iteration limit exceeded"")
    >>> print(mp.chop(v - mp.zeta(-2-3j)))
    0.0
    >>> w = mp.nsum(lambda n: n ** (2 + 3j), [1, mp.inf], method = ""levin"", levin_variant = ""v"")
    >>> print(mp.chop(v - w))
    0.0

Now we sum the divergent asymptotic expansion of an integral related to the
exponential integral (see also [2] p.373). The Sidi-S transform works best here::

    >>> z = mp.mpf(10)
    >>> exact = mp.quad(lambda x: mp.exp(-x)/(1+x/z),[0,mp.inf])
    >>> # exact = z * mp.exp(z) * mp.expint(1,z) # this is the symbolic expression for the integral
    >>> eps = mp.mpf(mp.eps)
    >>> with mp.extraprec(2 * mp.prec): # high working precisions are mandatory for divergent resummation
    ...     L = mp.levin(method = ""sidi"", variant = ""t"")
    ...     n = 0
    ...     while 1:
    ...         s = (-1)**n * mp.fac(n) * z ** (-n)
    ...         v, e = L.step(s)
    ...         n += 1
    ...         if e < eps:
    ...             break
    ...         if n > 1000: raise RuntimeError(""iteration limit exceeded"")
    >>> print(mp.chop(v - exact))
    0.0
    >>> w = mp.nsum(lambda n: (-1) ** n * mp.fac(n) * z ** (-n), [0, mp.inf], method = ""sidi"", levin_variant = ""t"")
    >>> print(mp.chop(v - w))
    0.0

Another highly divergent integral is also summable::

    >>> z = mp.mpf(2)
    >>> eps = mp.mpf(mp.eps)
    >>> exact = mp.quad(lambda x: mp.exp( -x * x / 2 - z * x ** 4), [0,mp.inf]) * 2 / mp.sqrt(2 * mp.pi)
    >>> # exact = mp.exp(mp.one / (32 * z)) * mp.besselk(mp.one / 4, mp.one / (32 * z)) / (4 * mp.sqrt(z * mp.pi)) # this is the symbolic expression for the integral
    >>> with mp.extraprec(7 * mp.prec):  # we need copious amount of precision to sum this highly divergent series
    ...     L = mp.levin(method = ""levin"", variant = ""t"")
    ...     n, s = 0, 0
    ...     while 1:
    ...         s += (-z)**n * mp.fac(4 * n) / (mp.fac(n) * mp.fac(2 * n) * (4 ** n))
    ...         n += 1
    ...         v, e = L.step_psum(s)
    ...         if e < eps:
    ...             break
    ...         if n > 1000: raise RuntimeError(""iteration limit exceeded"")
    >>> print(mp.chop(v - exact))
    0.0
    >>> w = mp.nsum(lambda n: (-z)**n * mp.fac(4 * n) / (mp.fac(n) * mp.fac(2 * n) * (4 ** n)),
    ...   [0, mp.inf], method = ""levin"", levin_variant = ""t"", workprec = 8*mp.prec, steps = [2] + [1 for x in xrange(1000)])
    >>> print(mp.chop(v - w))
    0.0

These examples run with 15-20 decimal digits precision. For higher precision the
working precision must be raised.

**Examples for nsum**

Here we calculate Euler's constant as the constant term in the Laurent
expansion of `\zeta(s)` at `s=1`. This sum converges extremly slowly because of
the logarithmic convergence behaviour of the Dirichlet series for zeta::

    >>> mp.dps = 30
    >>> z = mp.mpf(10) ** (-10)
    >>> a = mp.nsum(lambda n: n**(-(1+z)), [1, mp.inf], method = ""l"") - 1 / z
    >>> print(mp.chop(a - mp.euler, tol = 1e-10))
    0.0

The Sidi-S transform performs excellently for the alternating series of `\log(2)`::

    >>> a = mp.nsum(lambda n: (-1)**(n-1) / n, [1, mp.inf], method = ""sidi"")
    >>> print(mp.chop(a - mp.log(2)))
    0.0

Hypergeometric series can also be summed outside their range of convergence.
The stepsize in :func:`~mpmath.nsum` must not be chosen too large, otherwise it will miss the
point where the Levin transform converges resulting in numerical overflow/garbage::

    >>> z = 2 + 1j
    >>> exact = mp.hyp2f1(2 / mp.mpf(3), 4 / mp.mpf(3), 1 / mp.mpf(3), z)
    >>> f = lambda n: mp.rf(2 / mp.mpf(3), n) * mp.rf(4 / mp.mpf(3), n) * z**n / (mp.rf(1 / mp.mpf(3), n) * mp.fac(n))
    >>> v = mp.nsum(f, [0, mp.inf], method = ""levin"", steps = [10 for x in xrange(1000)])
    >>> print(mp.chop(exact-v))
    0.0

References:

  [1] E.J. Weniger - ""Nonlinear Sequence Transformations for the Acceleration of
      Convergence and the Summation of Divergent Series"" arXiv:math/0306302

  [2] A. Sidi - ""Pratical Extrapolation Methods""

  [3] H.H.H. Homeier - ""Scalar Levin-Type Sequence Transformations"" arXiv:math/0005209",8,[],0,"['__init__', 'factor_levin', 'factor_sidi', 'run', 'update_psum', 'update', 'step_psum', 'step']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\extrapolation.py,cohen_alt_class,"This interface implements the convergence acceleration of alternating series
as described in H. Cohen, F.R. Villegas, D. Zagier - ""Convergence Acceleration
of Alternating Series"". This series transformation works only well if the
individual terms of the series have an alternating sign. It belongs to the
class of linear series transformations (in contrast to the Shanks/Wynn-epsilon
or Levin transform). This series transformation is also able to sum some types
of divergent series. See the paper under which conditions this resummation is
mathematical sound.

Let *A* be the series we want to sum:

.. math ::

    A = \sum_{k=0}^{\infty} a_k

Let `s_n` be the partial sums of this series:

.. math ::

    s_n = \sum_{k=0}^n a_k.


**Interface**

Calling ``cohen_alt`` returns an object with the following methods.

Then ``update(...)`` works with the list of individual terms `a_k` and
``update_psum(...)`` works with the list of partial sums `s_k`:

.. code ::

    v, e = ...update([a_0, a_1,..., a_k])
    v, e = ...update_psum([s_0, s_1,..., s_k])

*v* is the current estimate for *A*, and *e* is an error estimate which is
simply the difference between the current estimate and the last estimate.

**Examples**

Here we compute the alternating zeta function using ``update_psum``::

    >>> from mpmath import mp
    >>> AC = mp.cohen_alt()
    >>> S, s, n = [], 0, 1
    >>> while 1:
    ...     s += -((-1) ** n) * mp.one / (n * n)
    ...     n += 1
    ...     S.append(s)
    ...     v, e = AC.update_psum(S)
    ...     if e < mp.eps:
    ...         break
    ...     if n > 1000: raise RuntimeError(""iteration limit exceeded"")
    >>> print(mp.chop(v - mp.pi ** 2 / 12))
    0.0

Here we compute the product `\prod_{n=1}^{\infty} \Gamma(1+1/(2n-1)) / \Gamma(1+1/(2n))`::

    >>> A = []
    >>> AC = mp.cohen_alt()
    >>> n = 1
    >>> while 1:
    ...     A.append( mp.loggamma(1 + mp.one / (2 * n - 1)))
    ...     A.append(-mp.loggamma(1 + mp.one / (2 * n)))
    ...     n += 1
    ...     v, e = AC.update(A)
    ...     if e < mp.eps:
    ...         break
    ...     if n > 1000: raise RuntimeError(""iteration limit exceeded"")
    >>> v = mp.exp(v)
    >>> print(mp.chop(v - 1.06215090557106, tol = 1e-12))
    0.0

``cohen_alt`` is also accessible through the :func:`~mpmath.nsum` interface::

    >>> v = mp.nsum(lambda n: (-1)**(n-1) / n, [1, mp.inf], method = ""a"")
    >>> print(mp.chop(v - mp.log(2)))
    0.0
    >>> v = mp.nsum(lambda n: (-1)**n / (2 * n + 1), [0, mp.inf], method = ""a"")
    >>> print(mp.chop(v - mp.pi / 4))
    0.0
    >>> v = mp.nsum(lambda n: (-1)**n * mp.log(n) * n, [1, mp.inf], method = ""a"")
    >>> print(mp.chop(v - mp.diff(lambda s: mp.altzeta(s), -1)))
    0.0",3,[],0,"['__init__', 'update', 'update_psum']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\inverselaplace.py,InverseLaplaceTransform,"Inverse Laplace transform methods are implemented using this
class, in order to simplify the code and provide a common
infrastructure.

Implement a custom inverse Laplace transform algorithm by
subclassing :class:`InverseLaplaceTransform` and implementing the
appropriate methods. The subclass can then be used by
:func:`~mpmath.invertlaplace` by passing it as the *method*
argument.",3,['object'],0,"['__init__', 'calc_laplace_parameter', 'calc_time_domain_solution']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\inverselaplace.py,FixedTalbot,,2,['InverseLaplaceTransform'],0,"['calc_laplace_parameter', 'calc_time_domain_solution']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\inverselaplace.py,Stehfest,,3,['InverseLaplaceTransform'],0,"['calc_laplace_parameter', '_coeff', 'calc_time_domain_solution']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\inverselaplace.py,deHoog,,2,['InverseLaplaceTransform'],0,"['calc_laplace_parameter', 'calc_time_domain_solution']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\inverselaplace.py,Cohen,,2,['InverseLaplaceTransform'],0,"['calc_laplace_parameter', 'calc_time_domain_solution']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\inverselaplace.py,LaplaceTransformInversionMethods,,6,['object'],0,"['__init__', 'invertlaplace', 'invlaptalbot', 'invlapstehfest', 'invlapdehoog', 'invlapcohen']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\odes.py,ODEMethods,,0,['object'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\quadrature.py,QuadratureRule,"Quadrature rules are implemented using this class, in order to
simplify the code and provide a common infrastructure
for tasks such as error estimation and node caching.

You can implement a custom quadrature rule by subclassing
:class:`QuadratureRule` and implementing the appropriate
methods. The subclass can then be used by :func:`~mpmath.quad` by
passing it as the *method* argument.

:class:`QuadratureRule` instances are supposed to be singletons.
:class:`QuadratureRule` therefore implements instance caching
in :func:`~mpmath.__new__`.",9,['object'],0,"['__init__', 'clear', 'calc_nodes', 'get_nodes', 'transform_nodes', 'guess_degree', 'estimate_error', 'summation', 'sum_next']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\quadrature.py,TanhSinh,"This class implements ""tanh-sinh"" or ""doubly exponential""
quadrature. This quadrature rule is based on the Euler-Maclaurin
integral formula. By performing a change of variables involving
nested exponentials / hyperbolic functions (hence the name), the
derivatives at the endpoints vanish rapidly. Since the error term
in the Euler-Maclaurin formula depends on the derivatives at the
endpoints, a simple step sum becomes extremely accurate. In
practice, this means that doubling the number of evaluation
points roughly doubles the number of accurate digits.

Comparison to Gauss-Legendre:
  * Initial computation of nodes is usually faster
  * Handles endpoint singularities better
  * Handles infinite integration intervals better
  * Is slower for smooth integrands once nodes have been computed

The implementation of the tanh-sinh algorithm is based on the
description given in Borwein, Bailey & Girgensohn, ""Experimentation
in Mathematics - Computational Paths to Discovery"", A K Peters,
2003, pages 312-313. In the present implementation, a few
improvements have been made:

  * A more efficient scheme is used to compute nodes (exploiting
    recurrence for the exponential function)
  * The nodes are computed successively instead of all at once

**References**

* [Bailey]_
* http://users.cs.dal.ca/~jborwein/tanh-sinh.pdf",2,['QuadratureRule'],0,"['sum_next', 'calc_nodes']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\quadrature.py,GaussLegendre,"This class implements Gauss-Legendre quadrature, which is
exceptionally efficient for polynomials and polynomial-like (i.e.
very smooth) integrands.

The abscissas and weights are given by roots and values of
Legendre polynomials, which are the orthogonal polynomials
on `[-1, 1]` with respect to the unit weight
(see :func:`~mpmath.legendre`).

In this implementation, we take the ""degree"" `m` of the quadrature
to denote a Gauss-Legendre rule of degree `3 \cdot 2^m` (following
Borwein, Bailey & Girgensohn). This way we get quadratic, rather
than linear, convergence as the degree is incremented.

Comparison to tanh-sinh quadrature:
  * Is faster for smooth integrands once nodes have been computed
  * Initial computation of nodes is usually slower
  * Handles endpoint singularities worse
  * Handles infinite integration intervals worse",1,['QuadratureRule'],0,['calc_nodes'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\calculus\quadrature.py,QuadratureMethods,,6,['object'],0,"['__init__', 'quad', 'quadts', 'quadgl', 'quadosc', 'quadsubdiv']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\functions\rszeta.py,RSCache,,1,['object'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\libmp\libhyper.py,NoConvergence,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\libmp\libmpf.py,ComplexResult,,0,['ValueError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\libmp\libmpf.py,h_mask_big,,1,[],0,['__getitem__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\matrices\calculus.py,MatrixCalculusMethods,,8,['object'],0,"['_exp_pade', 'expm', 'cosm', 'sinm', '_sqrtm_rot', 'sqrtm', 'logm', 'powm']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\matrices\eigen.py,Eigen,,0,['object'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mpmath\matrices\linalg.py,LinearAlgebraMethods,,17,['object'],0,"['LU_decomp', 'L_solve', 'U_solve', 'lu_solve', 'improve_solution', 'lu', 'unitvector', 'inverse', 'householder', 'residual', 'qr_solve', 'cholesky', 'cholesky_solve', 'det', 'cond', 'lu_solve_mat', 'qr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\dmypy\client.py,AugmentedHelpFormatter,,1,['argparse.RawDescriptionHelpFormatter'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\dmypy\client.py,BadStatus,"Exception raised when there is something wrong with the status file.

For example:
- No status file found
- Status file malformed
- Process whose pid is in the status file does not exist",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\plugins\common.py,MethodSpec,"Represents a method signature to be added, except for `name`.",0,['NamedTuple'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\plugins\default.py,DefaultPlugin,Type checker plugin that is enabled by default.,7,['Plugin'],0,"['get_function_hook', 'get_function_signature_hook', 'get_method_signature_hook', 'get_method_hook', 'get_attribute_hook', 'get_class_decorator_hook', 'get_class_decorator_hook_2']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\plugins\functools.py,_MethodInfo,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\plugins\proper_plugin.py,ProperTypePlugin,"A plugin to ensure that every type is expanded before doing any special-casing.

This solves the problem that we have hundreds of call sites like:

    if isinstance(typ, UnionType):
        ...  # special-case union

But after introducing a new type TypeAliasType (and removing immediate expansion)
all these became dangerous because typ may be e.g. an alias to union.",1,['Plugin'],0,['get_function_hook'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\plugins\singledispatch.py,SingledispatchTypeVars,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\plugins\singledispatch.py,RegisterCallableInfo,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\astdiff.py,SnapshotTypeVisitor,"Creates a read-only, self-contained snapshot of a type object.

Properties of a snapshot:

- Contains (nested) tuples and other immutable primitive objects only.
- References to AST nodes are replaced with full names of targets.
- Has no references to mutable or non-primitive objects.
- Two snapshots represent the same object if and only if they are
  equal.
- Results must be sortable. It's important that tuples have
  consistent types and can't arbitrarily mix str and None values,
  for example, since they can't be compared.",22,['TypeVisitor[SnapshotItem]'],0,"['visit_unbound_type', 'visit_any', 'visit_none_type', 'visit_uninhabited_type', 'visit_erased_type', 'visit_deleted_type', 'visit_instance', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_parameters', 'visit_callable_type', 'normalize_callable_variables', 'visit_tuple_type', 'visit_typeddict_type', 'visit_literal_type', 'visit_union_type', 'visit_overloaded', 'visit_partial_type', 'visit_type_type', 'visit_type_alias_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\astmerge.py,NodeReplaceVisitor,"Transform some nodes to new identities in an AST.

Only nodes that live in the symbol table may be
replaced, which simplifies the implementation some. Also
replace all references to the old identities.",31,['TraverserVisitor'],0,"['__init__', 'visit_mypy_file', 'visit_block', 'visit_func_def', 'visit_overloaded_func_def', 'visit_class_def', 'process_base_func', 'process_type_var_def', 'process_param_spec_def', 'process_type_var_tuple_def', 'visit_assignment_stmt', 'visit_name_expr', 'visit_member_expr', 'visit_ref_expr', 'visit_namedtuple_expr', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_super_expr', 'visit_call_expr', 'visit_newtype_expr', 'visit_lambda_expr', 'visit_typeddict_expr', 'visit_enum_call_expr', 'visit_var', 'visit_type_alias', 'fixup', 'fixup_and_reset_typeinfo', 'fixup_type', 'process_type_info', 'process_synthetic_type_info', 'replace_statements']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\astmerge.py,TypeReplaceVisitor,"Similar to NodeReplaceVisitor, but for type objects.

Note: this visitor may sometimes visit unanalyzed types
such as 'UnboundType' and 'RawExpressionType' For example, see
NodeReplaceVisitor.process_base_func.",28,['SyntheticTypeVisitor[None]'],0,"['__init__', 'visit_instance', 'visit_type_alias_type', 'visit_any', 'visit_none_type', 'visit_callable_type', 'visit_overloaded', 'visit_erased_type', 'visit_deleted_type', 'visit_partial_type', 'visit_tuple_type', 'visit_type_type', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_parameters', 'visit_typeddict_type', 'visit_raw_expression_type', 'visit_literal_type', 'visit_unbound_type', 'visit_type_list', 'visit_callable_argument', 'visit_ellipsis_type', 'visit_uninhabited_type', 'visit_union_type', 'visit_placeholder_type', 'fixup']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\deps.py,DependencyVisitor,,50,['TraverserVisitor'],0,"['__init__', 'visit_mypy_file', 'visit_func_def', 'visit_decorator', 'visit_class_def', 'visit_newtype_expr', 'process_type_info', 'visit_import', 'visit_import_from', 'visit_import_all', 'visit_block', 'visit_assignment_stmt', 'process_lvalue', 'is_self_member_ref', 'get_non_partial_lvalue_type', 'visit_operator_assignment_stmt', 'visit_for_stmt', 'visit_with_stmt', 'visit_del_stmt', 'process_global_ref_expr', 'visit_name_expr', 'visit_member_expr', 'get_unimported_fullname', 'visit_super_expr', 'visit_call_expr', 'process_isinstance_call', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_type_application', 'visit_index_expr', 'visit_unary_expr', 'visit_op_expr', 'visit_comparison_expr', 'process_binary_op', 'add_operator_method_dependency', 'add_operator_method_dependency_for_type', 'visit_generator_expr', 'visit_dictionary_comprehension', 'visit_star_expr', 'visit_yield_from_expr', 'visit_await_expr', 'add_type_alias_deps', 'add_dependency', 'add_type_dependencies', 'add_attribute_dependency', 'attribute_triggers', 'add_attribute_dependency_for_expr', 'add_iter_dependency', 'use_logical_deps', 'get_type_triggers']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\deps.py,TypeTriggersVisitor,,23,['TypeVisitor[List[str]]'],0,"['__init__', 'get_type_triggers', 'visit_instance', 'visit_type_alias_type', 'visit_any', 'visit_none_type', 'visit_callable_type', 'visit_overloaded', 'visit_erased_type', 'visit_deleted_type', 'visit_partial_type', 'visit_tuple_type', 'visit_type_type', 'visit_type_var', 'visit_param_spec', 'visit_type_var_tuple', 'visit_unpack_type', 'visit_parameters', 'visit_typeddict_type', 'visit_literal_type', 'visit_unbound_type', 'visit_uninhabited_type', 'visit_union_type']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\subexpr.py,SubexpressionFinder,,43,['TraverserVisitor'],0,"['__init__', 'visit_int_expr', 'visit_name_expr', 'visit_float_expr', 'visit_str_expr', 'visit_bytes_expr', 'visit_unicode_expr', 'visit_complex_expr', 'visit_ellipsis', 'visit_super_expr', 'visit_type_var_expr', 'visit_type_alias_expr', 'visit_namedtuple_expr', 'visit_typeddict_expr', 'visit__promote_expr', 'visit_newtype_expr', 'visit_member_expr', 'visit_yield_from_expr', 'visit_yield_expr', 'visit_call_expr', 'visit_op_expr', 'visit_comparison_expr', 'visit_slice_expr', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_reveal_expr', 'visit_assignment_expr', 'visit_unary_expr', 'visit_list_expr', 'visit_tuple_expr', 'visit_dict_expr', 'visit_set_expr', 'visit_index_expr', 'visit_generator_expr', 'visit_dictionary_comprehension', 'visit_list_comprehension', 'visit_set_comprehension', 'visit_conditional_expr', 'visit_type_application', 'visit_lambda_expr', 'visit_star_expr', 'visit_await_expr', 'add']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\update.py,FineGrainedBuildManager,,6,[],0,"['__init__', 'update', 'trigger', 'flush_cache', 'update_one', 'update_module']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\update.py,NormalUpdate,,0,['NamedTuple'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\server\update.py,BlockedUpdate,,0,['NamedTuple'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testapi.py,APISuite,,6,['Suite'],0,"['setUp', 'tearDown', 'test_capture_bad_opt', 'test_capture_empty', 'test_capture_help', 'test_capture_version']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testargs.py,ArgSuite,,2,['Suite'],0,"['test_coherence', 'test_executable_inference']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testconstraints.py,ConstraintsSuite,,11,['Suite'],0,"['setUp', 'test_no_type_variables', 'test_basic_type_variable', 'test_basic_type_var_tuple_subtype', 'test_basic_type_var_tuple', 'test_type_var_tuple_with_prefix_and_suffix', 'test_unpack_homogenous_tuple', 'test_unpack_homogenous_tuple_with_prefix_and_suffix', 'test_unpack_with_prefix_and_suffix', 'test_unpack_tuple_length_non_match', 'test_var_length_tuple_with_fixed_length_tuple']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testformatter.py,FancyErrorFormattingTestCases,,2,['TestCase'],0,"['test_trim_source', 'test_split_words']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testfscache.py,TestFileSystemCache,,8,['unittest.TestCase'],0,"['setUp', 'tearDown', 'test_isfile_case_1', 'test_isfile_case_2', 'test_isfile_case_3', 'test_isfile_case_other_directory', 'make_file', 'isfile_case']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testgraph.py,GraphSuite,,5,['Suite'],0,"['test_topsort', 'test_scc', '_make_manager', 'test_sorted_components', 'test_order_ascc']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testinfer.py,MapActualsToFormalsSuite,Test cases for argmap.map_actuals_to_formals.,18,['Suite'],0,"['test_basic', 'test_positional_only', 'test_optional', 'test_callee_star', 'test_caller_star', 'test_too_many_caller_args', 'test_tuple_star', 'make_tuple', 'test_named_args', 'test_some_named_args', 'test_missing_named_arg', 'test_duplicate_named_arg', 'test_varargs_and_bare_asterisk', 'test_keyword_varargs', 'test_both_kinds_of_varargs', 'test_special_cases', 'assert_map', 'assert_vararg_map']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testinfer.py,OperandDisjointDictSuite,"Test cases for checker.DisjointDict, which is used for type inference with operands.",5,['Suite'],0,"['new', 'test_independent_maps', 'test_partial_merging', 'test_full_merging', 'test_merge_with_multiple_overlaps']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testinfer.py,OperandComparisonGroupingSuite,Test cases for checker.group_comparison_operands.,7,['Suite'],0,"['literal_keymap', 'test_basic_cases', 'test_multiple_groups', 'test_multiple_groups_coalescing', 'test_multiple_groups_different_operators', 'test_single_pair', 'test_empty_pair_list']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testmodulefinder.py,ModuleFinderSuite,,15,['Suite'],0,"['setUp', 'test__no_namespace_packages__nsx', 'test__no_namespace_packages__nsx_a', 'test__no_namespace_packages__find_a_in_pkg1', 'test__no_namespace_packages__find_b_in_pkg2', 'test__find_nsx_as_namespace_pkg_in_pkg1', 'test__find_nsx_a_init_in_pkg1', 'test__find_nsx_b_init_in_pkg2', 'test__find_nsx_c_c_in_pkg3', 'test__find_nsy_a__init_pyi', 'test__find_nsy_b__init_py', 'test__find_nsy_c_pyi', 'test__find_a_in_pkg1', 'test__find_b_init_in_pkg2', 'test__find_d_nowhere']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testmodulefinder.py,ModuleFinderSitePackagesSuite,,4,['Suite'],0,"['setUp', 'path', 'test__packages_with_ns', 'test__packages_without_ns']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testmypyc.py,MypycTest,,1,['TestCase'],0,['test_using_mypyc'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testsolve.py,SolveSuite,,27,['Suite'],0,"['setUp', 'test_empty_input', 'test_simple_supertype_constraints', 'test_simple_subtype_constraints', 'test_both_kinds_of_constraints', 'test_unsatisfiable_constraints', 'test_exactly_specified_result', 'test_multiple_variables', 'test_no_constraints_for_var', 'test_simple_constraints_with_dynamic_type', 'test_both_normal_and_any_types_in_results', 'test_poly_no_constraints', 'test_poly_trivial_free', 'test_poly_free_pair', 'test_poly_free_pair_with_bounds', 'test_poly_free_pair_with_bounds_uninhabited', 'test_poly_bounded_chain', 'test_poly_reverse_overlapping_chain', 'test_poly_reverse_split_chain', 'test_poly_unsolvable_chain', 'test_simple_chain_closure', 'test_reverse_chain_closure', 'test_secondary_constraint_closure', 'assert_solve', 'assert_transitive_closure', 'supc', 'subc']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\teststubinfo.py,TestStubInfo,,4,['unittest.TestCase'],0,"['test_is_legacy_bundled_packages', 'test_approved_stub_package_exists', 'test_stub_distribution_name', 'test_period_in_top_level']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testsubtypes.py,SubtypingSuite,,31,['Suite'],0,"['setUp', 'test_trivial_cases', 'test_instance_subtyping', 'test_simple_generic_instance_subtyping_invariant', 'test_simple_generic_instance_subtyping_covariant', 'test_simple_generic_instance_subtyping_contravariant', 'test_generic_subtyping_with_inheritance_invariant', 'test_generic_subtyping_with_inheritance_covariant', 'test_generic_subtyping_with_inheritance_contravariant', 'test_interface_subtyping', 'test_generic_interface_subtyping', 'test_basic_callable_subtyping', 'test_default_arg_callable_subtyping', 'test_var_arg_callable_subtyping_1', 'test_var_arg_callable_subtyping_2', 'test_var_arg_callable_subtyping_3', 'test_var_arg_callable_subtyping_4', 'test_var_arg_callable_subtyping_5', 'test_var_arg_callable_subtyping_6', 'test_var_arg_callable_subtyping_7', 'test_var_arg_callable_subtyping_8', 'test_var_arg_callable_subtyping_9', 'test_type_callable_subtyping', 'test_type_var_tuple', 'test_type_var_tuple_with_prefix_suffix', 'test_type_var_tuple_unpacked_variable_length_tuple', 'assert_subtype', 'assert_not_subtype', 'assert_strict_subtype', 'assert_equivalent', 'assert_unrelated']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testutil.py,TestGetTerminalSize,,2,['TestCase'],0,"['test_get_terminal_size_in_pty_defaults_to_80', 'test_parse_location_windows']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\testutil.py,TestWriteJunitXml,,4,['TestCase'],0,"['test_junit_pass', 'test_junit_fail_escape_xml_chars', 'test_junit_fail_two_files', 'test_serious_error']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\test_find_sources.py,FakeFSCache,,5,['FileSystemCache'],0,"['__init__', 'isfile', 'isdir', 'listdir', 'init_under_package_root']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\test_find_sources.py,SourceFinderSuite,,11,['unittest.TestCase'],0,"['setUp', 'tearDown', 'test_crawl_no_namespace', 'test_crawl_namespace', 'test_crawl_namespace_explicit_base', 'test_crawl_namespace_multi_dir', 'test_find_sources_in_dir_no_namespace', 'test_find_sources_in_dir_namespace', 'test_find_sources_in_dir_namespace_explicit_base', 'test_find_sources_in_dir_namespace_multi_dir', 'test_find_sources_exclude']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\typefixture.py,TypeFixture,"Helper class that is used as a fixture in type-related unit tests.

The members are initialized to contain various type-related values.",10,[],0,"['__init__', '_add_bool_dunder', 'callable', 'callable_type', 'callable_default', 'callable_var_arg', 'make_type_info', 'def_alias_1', 'def_alias_2', 'non_rec_alias']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\typefixture.py,InterfaceTypeFixture,"Extension of TypeFixture that contains additional generic
interface types.",1,['TypeFixture'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\visitors.py,SkippedNodeSearcher,,4,['TraverserVisitor'],0,"['__init__', 'visit_assignment_stmt', 'visit_name_expr', 'visit_int_expr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\visitors.py,TypeAssertTransformVisitor,,1,['TransformVisitor'],0,['type'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\meta\test_diff_helper.py,DiffHelperSuite,,2,['Suite'],0,"['test_render_diff_range', 'test_diff_ranges']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\meta\test_parse_data.py,ParseTestDataSuite,,4,['Suite'],0,"['test_parse_invalid_case', 'test_parse_invalid_section', 'test_bad_ge_version_check', 'test_bad_eq_version_check']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\meta\test_update_data.py,UpdateDataSuite,,1,['Suite'],0,['test_update_data'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypy\test\meta\_pytest.py,PytestResult,,0,[],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\analysis\attrdefined.py,AttributeMaybeDefinedVisitor,"Find attributes that may have been defined via some code path.

Consider initializations in class body and assignments to 'self.x'
and calls to base class '__init__'.",8,['BaseAnalysisVisitor[str]'],0,"['__init__', 'visit_branch', 'visit_return', 'visit_unreachable', 'visit_register_op', 'visit_assign', 'visit_assign_multi', 'visit_set_mem']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\analysis\attrdefined.py,AttributeMaybeUndefinedVisitor,"Find attributes that may be undefined via some code path.

Consider initializations in class body, assignments to 'self.x'
and calls to base class '__init__'.",8,['BaseAnalysisVisitor[str]'],0,"['__init__', 'visit_branch', 'visit_return', 'visit_unreachable', 'visit_register_op', 'visit_assign', 'visit_assign_multi', 'visit_set_mem']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\analysis\ircheck.py,FnError,,3,[],0,"['__init__', '__eq__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\analysis\ircheck.py,IrCheckException,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\analysis\ircheck.py,OpChecker,,47,['OpVisitor[None]'],0,"['__init__', 'fail', 'check_control_op_targets', 'check_type_coercion', 'check_compatibility', 'expect_float', 'expect_non_float', 'visit_goto', 'visit_branch', 'visit_return', 'visit_unreachable', 'visit_assign', 'visit_assign_multi', 'visit_load_error_value', 'check_tuple_items_valid_literals', 'check_frozenset_items_valid_literals', 'visit_load_literal', 'visit_get_attr', 'visit_set_attr', 'visit_load_static', 'visit_init_static', 'visit_tuple_get', 'visit_tuple_set', 'visit_inc_ref', 'visit_dec_ref', 'visit_call', 'visit_method_call', 'visit_cast', 'visit_box', 'visit_unbox', 'visit_raise_standard_error', 'visit_call_c', 'visit_primitive_op', 'visit_truncate', 'visit_extend', 'visit_load_global', 'visit_int_op', 'visit_comparison_op', 'visit_float_op', 'visit_float_neg', 'visit_float_comparison_op', 'visit_load_mem', 'visit_set_mem', 'visit_get_element_ptr', 'visit_load_address', 'visit_keep_alive', 'visit_unborrow']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\analysis\selfleaks.py,SelfLeakedVisitor,"Analyze whether 'self' may be seen by arbitrary code in '__init__'.

More formally, the set is not empty if along some path from IR entry point
arbitrary code could have been executed that has access to 'self'.

(We don't consider access via 'gc.get_objects()'.)",38,['OpVisitor[GenAndKill]'],0,"['__init__', 'visit_goto', 'visit_branch', 'visit_return', 'visit_unreachable', 'visit_assign', 'visit_assign_multi', 'visit_set_mem', 'visit_call', 'visit_method_call', 'visit_load_error_value', 'visit_load_literal', 'visit_get_attr', 'visit_set_attr', 'visit_load_static', 'visit_init_static', 'visit_tuple_get', 'visit_tuple_set', 'visit_box', 'visit_unbox', 'visit_cast', 'visit_raise_standard_error', 'visit_call_c', 'visit_primitive_op', 'visit_truncate', 'visit_extend', 'visit_load_global', 'visit_int_op', 'visit_comparison_op', 'visit_float_op', 'visit_float_neg', 'visit_float_comparison_op', 'visit_load_mem', 'visit_get_element_ptr', 'visit_load_address', 'visit_keep_alive', 'visit_unborrow', 'check_register_op']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emit.py,HeaderDeclaration,"A representation of a declaration in C.

This is used to generate declarations in header files and
(optionally) definitions in source files.

Attributes:
  decl: C source code for the declaration.
  defn: Optionally, C source code for a definition.
  dependencies: The names of any objects that must be declared prior.
  is_type: Whether the declaration is of a C type. (C types will be declared in
           external header files and not marked 'extern'.)
  needs_export: Whether the declared object needs to be exported to
                other modules in the linking table.",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emit.py,EmitterContext,Shared emitter state for a compilation group.,1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emit.py,ErrorHandler,Describes handling errors in unbox/cast operations.,0,[],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emit.py,AssignHandler,Assign an error value on error.,0,['ErrorHandler'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emit.py,GotoHandler,Goto label on error.,1,['ErrorHandler'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emit.py,TracebackAndGotoHandler,Add traceback item and goto label on error.,1,['ErrorHandler'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emit.py,ReturnHandler,Return a constant value on error.,1,['ErrorHandler'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emit.py,Emitter,Helper for C code generation.,53,[],0,"['__init__', 'indent', 'dedent', 'label', 'reg', 'attr', 'object_annotation', 'emit_line', 'emit_lines', 'emit_label', 'emit_from_emitter', 'emit_printf', 'temp_name', 'new_label', 'get_module_group_prefix', 'get_group_prefix', 'static_name', 'type_struct_name', 'ctype', 'ctype_spaced', 'c_undefined_value', 'c_error_value', 'native_function_name', 'tuple_c_declaration', 'bitmap_field', 'attr_bitmap_expr', 'emit_attr_bitmap_set', 'emit_attr_bitmap_clear', '_emit_attr_bitmap_update', 'use_vectorcall', 'emit_undefined_attr_check', 'error_value_check', 'tuple_undefined_check_cond', 'tuple_undefined_value', 'c_initializer_undefined_value', 'declare_tuple_struct', 'emit_inc_ref', 'emit_dec_ref', 'pretty_name', 'emit_cast', 'emit_cast_error_handler', 'emit_union_cast', 'emit_tuple_cast', 'emit_arg_check', 'emit_unbox', 'emit_box', 'emit_error_check', 'emit_gc_visit', 'emit_gc_clear', 'emit_traceback', 'emit_type_error_traceback', '_emit_traceback', 'emit_unbox_failure_with_overlapping_error_value']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emitfunc.py,FunctionEmitterVisitor,,57,['OpVisitor[None]'],1,"['__init__', 'temp_name', 'visit_goto', 'visit_branch', 'visit_return', 'visit_tuple_set', 'visit_assign', 'visit_assign_multi', 'visit_load_error_value', 'visit_load_literal', 'get_attr_expr', 'visit_get_attr', 'next_branch', 'visit_set_attr', 'visit_load_static', 'visit_init_static', 'visit_tuple_get', 'get_dest_assign', 'visit_call', 'visit_method_call', 'visit_inc_ref', 'visit_dec_ref', 'visit_box', 'visit_cast', 'visit_unbox', 'visit_unreachable', 'visit_raise_standard_error', 'visit_call_c', 'visit_primitive_op', 'visit_truncate', 'visit_extend', 'visit_load_global', 'visit_int_op', 'visit_comparison_op', 'visit_float_op', 'visit_float_neg', 'visit_float_comparison_op', 'visit_load_mem', 'visit_set_mem', 'visit_get_element_ptr', 'visit_load_address', 'visit_keep_alive', 'visit_unborrow', 'label', 'reg', 'ctype', 'c_error_value', 'c_undefined_value', 'emit_line', 'emit_lines', 'emit_inc_ref', 'emit_dec_ref', 'emit_declaration', 'emit_traceback', 'emit_attribute_error', 'emit_signed_int_cast', 'emit_unsigned_int_cast']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\emitwrapper.py,WrapperGenerator,Helper that simplifies the generation of wrapper functions.,10,[],0,"['__init__', 'set_target', 'wrapper_name', 'use_goto', 'emit_header', 'emit_arg_processing', 'emit_call', 'error', 'emit_error_handling', 'finish']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\codegen\literals.py,Literals,Collection of literal values used in a compilation group and related helpers.,12,[],0,"['__init__', 'record_literal', 'literal_index', 'num_literals', 'encoded_str_values', 'encoded_int_values', 'encoded_bytes_values', 'encoded_float_values', 'encoded_complex_values', 'encoded_tuple_values', 'encoded_frozenset_values', '_encode_collection_values']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\ir\pprint.py,IRPrettyPrintVisitor,Internal visitor that pretty-prints ops.,41,['OpVisitor[str]'],1,"['__init__', 'visit_goto', 'visit_branch', 'visit_return', 'visit_unreachable', 'visit_assign', 'visit_assign_multi', 'visit_load_error_value', 'visit_load_literal', 'visit_get_attr', 'borrow_prefix', 'visit_set_attr', 'visit_load_static', 'visit_init_static', 'visit_tuple_get', 'visit_tuple_set', 'visit_inc_ref', 'visit_dec_ref', 'visit_call', 'visit_method_call', 'visit_cast', 'visit_box', 'visit_unbox', 'visit_raise_standard_error', 'visit_call_c', 'visit_primitive_op', 'visit_truncate', 'visit_extend', 'visit_load_global', 'visit_int_op', 'visit_comparison_op', 'visit_float_op', 'visit_float_neg', 'visit_float_comparison_op', 'visit_load_mem', 'visit_set_mem', 'visit_get_element_ptr', 'visit_load_address', 'visit_keep_alive', 'visit_unborrow', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\function.py,ArgInfo,,0,['NamedTuple'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\ll_builder.py,LowLevelIRBuilder,,91,[],0,"['__init__', 'set_module', 'add', 'goto', 'activate_block', 'goto_and_activate', 'keep_alive', 'push_error_handler', 'pop_error_handler', 'self', 'flush_keep_alives', 'box', 'unbox_or_cast', 'coerce', 'coerce_int_to_fixed_width', 'coerce_short_int_to_fixed_width', 'coerce_fixed_width_to_int', 'coerce_nullable', 'get_attr', 'union_get_attr', 'py_get_attr', 'isinstance_helper', 'get_type_of_obj', 'type_is_op', 'isinstance_native', '_construct_varargs', 'py_call', '_py_vector_call', '_vectorcall_keywords', 'py_method_call', '_py_vector_method_call', 'call', 'native_args_to_positional', 'gen_method_call', 'union_method_call', 'none', 'true', 'false', 'none_object', 'load_int', 'load_float', 'load_str', 'load_bytes', 'load_complex', 'load_static_checked', 'load_module', 'get_native_type', 'load_native_type_object', 'binary_op', 'check_tagged_short_int', 'compare_strings', 'compare_bytes', 'compare_tuples', 'translate_instance_contains', 'bool_bitwise_op', 'bool_comparison_op', 'unary_not', 'unary_op', 'make_dict', 'new_list_op_with_length', 'new_list_op', 'new_set_op', 'setup_rarray', 'shortcircuit_helper', 'bool_value', 'add_bool_branch', 'call_c', 'matching_call_c', 'primitive_op', 'matching_primitive_op', 'int_op', 'float_op', 'float_mod', 'compare_floats', 'fixed_width_int_op', 'check_for_zero_division', 'inline_fixed_width_divide', 'inline_fixed_width_mod', 'is_same_native_int_signs', 'is_same_float_signs', 'comparison_op', 'builtin_len', 'new_tuple', 'new_tuple_with_length', 'int_to_float', 'decompose_union_helper', 'translate_special_method_call', 'translate_eq_cmp', 'translate_is_op', '_create_dict', 'error']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\mapper.py,Mapper,"Keep track of mappings from mypy concepts to IR concepts.

For example, we keep track of how the mypy TypeInfos of compiled
classes map to class IR objects.

This state is shared across all modules being compiled in all
compilation groups.",7,[],0,"['__init__', 'type_to_rtype', 'get_arg_rtype', 'fdef_to_sig', 'is_native_module', 'is_native_ref_expr', 'is_native_module_ref_expr']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\prebuildvisitor.py,PreBuildVisitor,"Mypy file AST visitor run before building the IR.

This collects various things, including:

* Determine relationships between nested functions and functions that
  contain nested functions
* Find non-local variables (free variables)
* Find property setters
* Find decorators of functions
* Find module import groups

The main IR build pass uses this information.",13,['ExtendedTraverserVisitor'],0,"['__init__', 'visit', 'visit_block', 'visit_decorator', 'visit_func_def', 'visit_lambda_expr', 'visit_func', 'visit_import', 'visit_name_expr', 'visit_var', 'visit_symbol_node', 'is_parent', 'add_free_variable']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\prepare.py,SingledispatchInfo,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\prepare.py,SingledispatchVisitor,,2,['TraverserVisitor'],1,"['__init__', 'visit_decorator']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\prepare.py,RegisteredImpl,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\targets.py,AssignmentTarget,Abstract base class for assignment targets during IR building.,0,[],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\targets.py,AssignmentTargetRegister,"Register as an assignment target.

This is used for local variables and some temporaries.",1,['AssignmentTarget'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\targets.py,AssignmentTargetIndex,base[index] as assignment target,1,['AssignmentTarget'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\targets.py,AssignmentTargetAttr,obj.attr as assignment target,1,['AssignmentTarget'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\targets.py,AssignmentTargetTuple,"x, ..., y as assignment target",1,['AssignmentTarget'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\irbuild\visitor.py,IRBuilderVisitor,"Mypy node visitor that dispatches to node transform implementations.

This class should have no non-trivial logic.

This visitor is separated from the rest of code to improve modularity and
to avoid import cycles.

This is based on the visitor pattern
(https://en.wikipedia.org/wiki/Visitor_pattern).",74,['IRVisitor'],1,"['visit_mypy_file', 'visit_class_def', 'visit_import', 'visit_import_from', 'visit_import_all', 'visit_func_def', 'visit_overloaded_func_def', 'visit_decorator', 'visit_block', 'visit_expression_stmt', 'visit_return_stmt', 'visit_assignment_stmt', 'visit_operator_assignment_stmt', 'visit_if_stmt', 'visit_while_stmt', 'visit_for_stmt', 'visit_break_stmt', 'visit_continue_stmt', 'visit_raise_stmt', 'visit_try_stmt', 'visit_with_stmt', 'visit_pass_stmt', 'visit_assert_stmt', 'visit_del_stmt', 'visit_global_decl', 'visit_nonlocal_decl', 'visit_match_stmt', 'visit_type_alias_stmt', 'visit_name_expr', 'visit_member_expr', 'visit_super_expr', 'visit_call_expr', 'visit_unary_expr', 'visit_op_expr', 'visit_index_expr', 'visit_conditional_expr', 'visit_comparison_expr', 'visit_int_expr', 'visit_float_expr', 'visit_complex_expr', 'visit_str_expr', 'visit_bytes_expr', 'visit_ellipsis', 'visit_list_expr', 'visit_tuple_expr', 'visit_dict_expr', 'visit_set_expr', 'visit_list_comprehension', 'visit_set_comprehension', 'visit_dictionary_comprehension', 'visit_slice_expr', 'visit_generator_expr', 'visit_lambda_expr', 'visit_yield_expr', 'visit_yield_from_expr', 'visit_await_expr', 'visit_assignment_expr', 'visit_enum_call_expr', 'visit__promote_expr', 'visit_namedtuple_expr', 'visit_newtype_expr', 'visit_temp_node', 'visit_type_alias_expr', 'visit_type_application', 'visit_type_var_expr', 'visit_paramspec_expr', 'visit_type_var_tuple_expr', 'visit_typeddict_expr', 'visit_reveal_expr', 'visit_var', 'visit_cast_expr', 'visit_assert_type_expr', 'visit_star_expr', 'bail']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\lib-rt\setup.py,build_ext_custom,,2,['build_ext'],0,"['get_library_names', 'run']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\lower\int_ops.py,IntComparisonOpDescription,,0,['NamedTuple'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\primitives\registry.py,CFunctionDescription,,0,['NamedTuple'],13,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\primitives\registry.py,LoadAddressDescription,,0,['NamedTuple'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_cheader.py,TestHeaderInclusion,,1,['unittest.TestCase'],0,['test_primitives_included_in_header'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_emit.py,TestEmitter,,7,['unittest.TestCase'],0,"['setUp', 'test_label', 'test_reg', 'test_object_annotation', 'test_emit_line', 'test_emit_undefined_value_for_simple_type', 'test_emit_undefined_value_for_tuple']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_emitclass.py,TestEmitClass,,3,['unittest.TestCase'],0,"['test_slot_key', 'test_setter_name', 'test_getter_name']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_emitfunc.py,TestFunctionEmitterVisitor,Test generation of fragments of C from individual IR ops.,64,['unittest.TestCase'],0,"['setUp', 'test_goto', 'test_goto_next_block', 'test_return', 'test_integer', 'test_tuple_get', 'test_load_None', 'test_assign_int', 'test_int_add', 'test_int_sub', 'test_int_neg', 'test_branch', 'test_branch_no_else', 'test_branch_no_else_negated', 'test_branch_is_error', 'test_branch_is_error_next_block', 'test_branch_rare', 'test_call', 'test_call_two_args', 'test_inc_ref', 'test_dec_ref', 'test_inc_ref_int', 'test_dec_ref_int', 'test_dec_ref_tuple', 'test_dec_ref_tuple_nested', 'test_list_get_item', 'test_list_set_item', 'test_box_int', 'test_unbox_int', 'test_box_i64', 'test_unbox_i64', 'test_list_append', 'test_get_attr', 'test_get_attr_non_refcounted', 'test_get_attr_merged', 'test_get_attr_with_bitmap', 'test_set_attr', 'test_set_attr_non_refcounted', 'test_set_attr_no_error', 'test_set_attr_non_refcounted_no_error', 'test_set_attr_with_bitmap', 'test_set_attr_init_with_bitmap', 'test_dict_get_item', 'test_dict_set_item', 'test_dict_update', 'test_new_dict', 'test_dict_contains', 'test_int_op', 'test_comparison_op', 'test_load_mem', 'test_set_mem', 'test_get_element_ptr', 'test_load_address', 'test_assign_multi', 'test_long_unsigned', 'test_long_signed', 'test_cast_and_branch_merge', 'test_cast_and_branch_no_merge_1', 'test_cast_and_branch_no_merge_2', 'test_cast_and_branch_no_merge_3', 'test_cast_and_branch_no_merge_4', 'test_extend', 'assert_emit', 'assert_emit_binary_op']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_emitfunc.py,TestGenerateFunction,,3,['unittest.TestCase'],0,"['setUp', 'test_simple', 'test_register']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_emitwrapper.py,TestArgCheck,,4,['unittest.TestCase'],0,"['setUp', 'test_check_list', 'test_check_int', 'assert_lines']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_ircheck.py,TestIrcheck,,15,['unittest.TestCase'],0,"['setUp', 'basic_block', 'func_decl', 'test_valid_fn', 'test_block_not_terminated_empty_block', 'test_valid_goto', 'test_invalid_goto', 'test_invalid_register_source', 'test_invalid_op_source', 'test_invalid_return_type', 'test_invalid_assign', 'test_can_coerce_to', 'test_duplicate_op', 'test_pprint', 'test_load_address_declares_register']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_literals.py,TestLiterals,,6,['unittest.TestCase'],0,"['test_format_str_literal', 'test_encode_str_values', 'test_encode_bytes_values', 'test_encode_int_values', 'test_simple_literal_index', 'test_tuple_literal']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_namegen.py,TestNameGen,,4,['unittest.TestCase'],0,"['test_candidate_suffixes', 'test_exported_name', 'test_make_module_translation_map', 'test_name_generator']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_pprint.py,TestGenerateNames,,4,['unittest.TestCase'],0,"['test_empty', 'test_arg', 'test_int_op', 'test_assign']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_rarray.py,TestRArray,,6,['unittest.TestCase'],0,"['test_basics', 'test_str_conversion', 'test_eq', 'test_hash', 'test_alignment', 'test_size']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_struct.py,TestStruct,,4,['unittest.TestCase'],0,"['test_struct_offsets', 'test_struct_str', 'test_runtime_subtype', 'test_eq_and_hash']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_tuplename.py,TestTupleNames,,2,['unittest.TestCase'],0,"['setUp', 'test_names']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_typeops.py,TestSubtype,,5,['unittest.TestCase'],0,"['test_bit', 'test_bool', 'test_int64', 'test_int32', 'test_int16']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_typeops.py,TestRuntimeSubtype,,3,['unittest.TestCase'],0,"['test_bit', 'test_bool', 'test_union']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test\test_typeops.py,TestUnionSimplification,,4,['unittest.TestCase'],0,"['test_simple_type_result', 'test_remove_duplicate', 'test_cannot_simplify', 'test_nested']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\transform\copy_propagation.py,CopyPropagationTransform,,2,['IRTransform'],0,"['__init__', 'visit_assign']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\transform\flag_elimination.py,FlagEliminationTransform,,4,['IRTransform'],0,"['__init__', 'visit_assign', 'visit_goto', 'visit_branch']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\transform\ir_transform.py,IRTransform,"Identity transform.

Subclass and override to perform changes to IR.

Subclass IRTransform and override any OpVisitor visit_* methods
that perform any IR changes. The default implementations implement
an identity transform.

A visit method can return None to remove ops. In this case the
transform must ensure that no op uses the original removed op
as a source after the transform.

You can retain old BasicBlock and op references in ops. The transform
will automatically patch these for you as needed.",41,['OpVisitor[Optional[Value]]'],0,"['__init__', 'transform_blocks', 'add', 'visit_goto', 'visit_branch', 'visit_return', 'visit_unreachable', 'visit_assign', 'visit_assign_multi', 'visit_load_error_value', 'visit_load_literal', 'visit_get_attr', 'visit_set_attr', 'visit_load_static', 'visit_init_static', 'visit_tuple_get', 'visit_tuple_set', 'visit_inc_ref', 'visit_dec_ref', 'visit_call', 'visit_method_call', 'visit_cast', 'visit_box', 'visit_unbox', 'visit_raise_standard_error', 'visit_call_c', 'visit_primitive_op', 'visit_truncate', 'visit_extend', 'visit_load_global', 'visit_int_op', 'visit_comparison_op', 'visit_float_op', 'visit_float_neg', 'visit_float_comparison_op', 'visit_load_mem', 'visit_set_mem', 'visit_get_element_ptr', 'visit_load_address', 'visit_keep_alive', 'visit_unborrow']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\transform\ir_transform.py,PatchVisitor,,41,['OpVisitor[None]'],0,"['__init__', 'fix_op', 'fix_block', 'visit_goto', 'visit_branch', 'visit_return', 'visit_unreachable', 'visit_assign', 'visit_assign_multi', 'visit_load_error_value', 'visit_load_literal', 'visit_get_attr', 'visit_set_attr', 'visit_load_static', 'visit_init_static', 'visit_tuple_get', 'visit_tuple_set', 'visit_inc_ref', 'visit_dec_ref', 'visit_call', 'visit_method_call', 'visit_cast', 'visit_box', 'visit_unbox', 'visit_raise_standard_error', 'visit_call_c', 'visit_primitive_op', 'visit_truncate', 'visit_extend', 'visit_load_global', 'visit_int_op', 'visit_comparison_op', 'visit_float_op', 'visit_float_neg', 'visit_float_comparison_op', 'visit_load_mem', 'visit_set_mem', 'visit_get_element_ptr', 'visit_load_address', 'visit_keep_alive', 'visit_unborrow']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\transform\lower.py,LoweringVisitor,,1,['IRTransform'],0,['visit_primitive_op'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\mypyc\test-data\fixtures\testutil.py,async_val,,2,"['Awaitable[V]', 'Generic[T, V]']",0,"['__init__', '__await__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\chordal.py,NetworkXTreewidthBoundExceeded,"Exception raised when a treewidth bound has been provided and it has
been exceeded",0,['nx.NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\clique.py,MaxWeightClique,"A class for the maximum weight clique algorithm.

This class is a helper for the `max_weight_clique` function.  The class
should not normally be used directly.

Parameters
----------
G : NetworkX graph
    The undirected graph for which a maximum weight clique is sought
weight : string or None, optional (default='weight')
    The node attribute that holds the integer value used as a weight.
    If None, then each node has weight 1.

Attributes
----------
G : NetworkX graph
    The undirected graph for which a maximum weight clique is sought
node_weights: dict
    The weight of each node
incumbent_nodes : list
    The nodes of the incumbent clique (the best clique found so far)
incumbent_weight: int
    The weight of the incumbent clique",6,[],0,"['__init__', 'update_incumbent_if_improved', 'greedily_find_independent_set', 'find_branching_nodes', 'expand', 'find_max_weight_clique']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\cycles.py,_NeighborhoodCache,"Very lightweight graph wrapper which caches neighborhoods as list.

This dict subclass uses the __missing__ functionality to query graphs for
their neighborhoods, and store the result as a list.  This is used to avoid
the performance penalty incurred by subgraph views.",2,['dict'],0,"['__init__', '__missing__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\simple_paths.py,PathBuffer,,4,[],0,"['__init__', '__len__', 'push', 'pop']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\filters.py,show_nodes,"Filter class to show specific nodes.

Attach the set of nodes as an attribute to speed up this commonly used filter

Note that another allowed attribute for filters is to store the number of nodes
on the filter as attribute `length` (used in `__len__`). It is a user
responsibility to ensure this attribute is accurate if present.",2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\drawing\nx_pylab.py,FancyArrowFactory,Draw arrows with `matplotlib.patches.FancyarrowPatch`,3,[],0,"['__init__', '__call__', 'to_marker_edge']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\degree_seq.py,DegreeSequenceRandomGraph,,9,[],0,"['__init__', 'generate', 'update_remaining', 'p', 'q', 'suitable_edge', 'phase1', 'phase2', 'phase3']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\internet_as_graphs.py,AS_graph_generator,Generates random internet AS graphs.,12,[],0,"['__init__', 't_graph', 'add_edge', 'choose_peer_pref_attach', 'choose_node_pref_attach', 'add_customer', 'add_node', 'add_m_peering_link', 'add_cp_peering_link', 'graph_regions', 'add_peering_links', 'generate']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\linalg\algebraicconnectivity.py,_PCGSolver,"Preconditioned conjugate gradient method.

To solve Ax = b:
    M = A.diagonal() # or some other preconditioner
    solver = _PCGSolver(lambda x: A * x, lambda x: M * x)
    x = solver.solve(b)

The inputs A and M are functions which compute
matrix multiplication on the argument.
A - multiply by the matrix A in Ax=b
M - multiply by M, the preconditioner surrogate for A

Warning: There is no limit on number of iterations.",3,[],0,"['__init__', 'solve', '_solve']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\linalg\algebraicconnectivity.py,_LUSolver,"LU factorization.

To solve Ax = b:
    solver = _LUSolver(A)
    x = solver.solve(b)

optional argument `tol` on solve method is ignored but included
to match _PCGsolver API.",2,[],0,"['__init__', 'solve']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\tests\test_convert.py,TestConvert,,11,[],0,"['edgelists_equal', 'test_simple_graphs', 'test_exceptions', 'test_digraphs', 'test_graph', 'test_with_multiedges_self_loops', 'test_edgelists', 'test_directed_to_undirected', 'test_attribute_dict_integrity', 'test_to_edgelist', 'test_custom_node_attr_dict_safekeeping']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\tests\test_convert_scipy.py,TestConvertScipy,,20,[],0,"['setup_method', 'test_exceptions', 'create_weighted', 'identity_conversion', 'test_shape', 'test_identity_graph_matrix', 'test_identity_digraph_matrix', 'test_identity_weighted_graph_matrix', 'test_identity_weighted_digraph_matrix', 'test_nodelist', 'test_weight_keyword', 'test_format_keyword', 'test_format_keyword_raise', 'test_null_raise', 'test_empty', 'test_ordering', 'test_selfloop_graph', 'test_selfloop_digraph', 'test_from_scipy_sparse_array_parallel_edges', 'test_symmetric']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\heaps.py,MinHeap,"Base class for min-heaps.

A MinHeap stores a collection of key-value pairs ordered by their values.
It supports querying the minimum pair, inserting a new pair, decreasing the
value in an existing pair and deleting the minimum pair.",9,[],0,"['__init__', 'min', 'pop', 'get', 'insert', '__nonzero__', '__bool__', '__len__', '__contains__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\heaps.py,PairingHeap,A pairing heap.,8,['MinHeap'],0,"['__init__', 'min', 'pop', 'get', 'insert', '_link', '_merge_children', '_cut']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\heaps.py,BinaryHeap,A binary heap.,5,['MinHeap'],0,"['__init__', 'min', 'pop', 'get', 'insert']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\misc.py,PythonRandomViaNumpyBits,"Provide the random.random algorithms using a numpy.random bit generator

The intent is to allow people to contribute code that uses Python's random
library, but still allow users to provide a single easily controlled random
bit-stream for all work with NetworkX. This implementation is based on helpful
comments and code from Robert Kern on NumPy's GitHub Issue #24458.

This implementation supersedes that of `PythonRandomInterface` which rewrote
methods to account for subtle differences in API between `random` and
`numpy.random`. Instead this subclasses `random.Random` and overwrites
the methods `random`, `getrandbits`, `getstate`, `setstate` and `seed`.
It makes them use the rng values from an input numpy `RandomState` or `Generator`.
Those few methods allow the rest of the `random.Random` methods to provide
the API interface of `random.random` while using randomness generated by
a numpy generator.",6,['random.Random'],0,"['__init__', 'random', 'getrandbits', 'getstate', 'setstate', 'seed']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\misc.py,PythonRandomInterface,"PythonRandomInterface is included for backward compatibility
New code should use PythonRandomViaNumpyBits instead.",11,[],0,"['__init__', 'random', 'uniform', 'randrange', 'choice', 'gauss', 'shuffle', 'sample', 'randint', 'expovariate', 'paretovariate']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\union_find.py,UnionFind,"Union-find data structure.

Each unionFind instance X maintains a family of disjoint sets of
hashable objects, supporting the following two methods:

- X[item] returns a name for the set containing the given item.
  Each set is named by an arbitrarily-chosen one of its members; as
  long as the set remains unchanged it will keep the same name. If
  the item is not yet part of a set in X, a new singleton set is
  created for it.

- X.union(item1, item2, ...) merges the sets containing each item
  into a single larger set.  If any item is not yet part of a set
  in X, it is added to X as one of the members of the merged set.

  Union-find data structure. Based on Josiah Carlson's code,
  https://code.activestate.com/recipes/215912/
  with significant additional changes by D. Eppstein.
  http://www.ics.uci.edu/~eppstein/PADS/UnionFind.py",5,[],0,"['__init__', '__getitem__', '__iter__', 'to_sets', 'union']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\approximation\treewidth.py,MinDegreeHeuristic,"Implements the Minimum Degree heuristic.

The heuristic chooses the nodes according to their degree
(number of neighbors), i.e., first the node with the lowest degree is
chosen, then the graph is updated and the corresponding node is
removed. Next, a new node with the lowest degree is chosen, and so on.",2,[],0,"['__init__', 'best_node']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\flow_matrix.py,InverseLaplacian,,7,[],0,"['__init__', 'init_solver', 'solve', 'solve_inverse', 'get_rows', 'get_row', 'width']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\flow_matrix.py,FullInverseLaplacian,,3,['InverseLaplacian'],0,"['init_solver', 'solve', 'solve_inverse']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\flow_matrix.py,SuperLUInverseLaplacian,,3,['InverseLaplacian'],0,"['init_solver', 'solve_inverse', 'solve']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\flow_matrix.py,CGInverseLaplacian,,3,['InverseLaplacian'],0,"['init_solver', 'solve', 'solve_inverse']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\community\quality.py,NotAPartition,Raised if a given collection is not a partition.,1,['NetworkXError'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\flow\networksimplex.py,_DataEssentialsAndFunctions,,15,[],0,"['__init__', 'initialize_spanning_tree', 'find_apex', 'trace_path', 'find_cycle', 'augment_flow', 'trace_subtree', 'remove_edge', 'make_root', 'add_edge', 'update_potentials', 'reduced_cost', 'find_entering_edges', 'residual_capacity', 'find_leaving_edge']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\isomorphvf2.py,GraphMatcher,"Implementation of VF2 algorithm for matching undirected graphs.

Suitable for Graph and MultiGraph instances.",13,[],0,"['__init__', 'reset_recursion_limit', 'candidate_pairs_iter', 'initialize', 'is_isomorphic', 'isomorphisms_iter', 'match', 'semantic_feasibility', 'subgraph_is_isomorphic', 'subgraph_is_monomorphic', 'subgraph_isomorphisms_iter', 'subgraph_monomorphisms_iter', 'syntactic_feasibility']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\isomorphvf2.py,DiGraphMatcher,"Implementation of VF2 algorithm for matching directed graphs.

Suitable for DiGraph and MultiDiGraph instances.",8,['GraphMatcher'],0,"['__init__', 'candidate_pairs_iter', 'initialize', 'syntactic_feasibility', 'subgraph_is_isomorphic', 'subgraph_is_monomorphic', 'subgraph_isomorphisms_iter', 'subgraph_monomorphisms_iter']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\isomorphvf2.py,GMState,"Internal representation of state for the GraphMatcher class.

This class is used internally by the GraphMatcher class.  It is used
only to store state specific data. There will be at most G2.order() of
these objects in memory at a time, due to the depth-first search
strategy employed by the VF2 algorithm.",2,[],0,"['__init__', 'restore']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\isomorphvf2.py,DiGMState,"Internal representation of state for the DiGraphMatcher class.

This class is used internally by the DiGraphMatcher class.  It is used
only to store state specific data. There will be at most G2.order() of
these objects in memory at a time, due to the depth-first search
strategy employed by the VF2 algorithm.",2,[],0,"['__init__', 'restore']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\temporalisomorphvf2.py,TimeRespectingGraphMatcher,,4,['GraphMatcher'],0,"['__init__', 'one_hop', 'two_hop', 'semantic_feasibility']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\temporalisomorphvf2.py,TimeRespectingDiGraphMatcher,,11,['DiGraphMatcher'],0,"['__init__', 'get_pred_dates', 'get_succ_dates', 'one_hop', 'two_hop_pred', 'two_hop_succ', 'preds', 'succs', 'test_one', 'test_two', 'semantic_feasibility']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_boundary.py,TestNodeBoundary,Unit tests for the :func:`~networkx.node_boundary` function.,7,[],0,"['test_null_graph', 'test_path_graph', 'test_complete_graph', 'test_petersen', 'test_directed', 'test_multigraph', 'test_multidigraph']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_boundary.py,TestEdgeBoundary,Unit tests for the :func:`~networkx.edge_boundary` function.,6,[],0,"['test_null_graph', 'test_path_graph', 'test_complete_graph', 'test_directed', 'test_multigraph', 'test_multidigraph']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_chains.py,TestChainDecomposition,Unit tests for the chain decomposition function.,6,[],0,"['assertContainsChain', 'test_decomposition', 'test_barbell_graph', 'test_disconnected_graph', 'test_disconnected_graph_root_node', 'test_chain_decomposition_root_not_in_G']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_clique.py,TestCliques,,12,[],0,"['setup_method', 'test_find_cliques1', 'test_selfloops', 'test_find_cliques2', 'test_find_cliques3', 'test_number_of_cliques', 'test_node_clique_number', 'test_make_clique_bipartite', 'test_make_max_clique_graph', 'test_directed', 'test_find_cliques_trivial', 'test_make_max_clique_graph_create_using']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_clique.py,TestEnumerateAllCliques,,1,[],0,['test_paper_figure_4'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_communicability.py,TestCommunicability,,2,[],0,"['test_communicability', 'test_communicability2']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_covering.py,TestMinEdgeCover,Tests for :func:`networkx.algorithms.min_edge_cover`,8,[],0,"['test_empty_graph', 'test_graph_with_loop', 'test_graph_with_isolated_v', 'test_graph_single_edge', 'test_graph_two_edge_path', 'test_bipartite_explicit', 'test_complete_graph_even', 'test_complete_graph_odd']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_covering.py,TestIsEdgeCover,Tests for :func:`networkx.algorithms.is_edge_cover`,3,[],0,"['test_empty_graph', 'test_graph_with_loop', 'test_graph_single_edge']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_cuts.py,TestCutSize,Unit tests for the :func:`~networkx.cut_size` function.,5,[],0,"['test_symmetric', 'test_single_edge', 'test_directed', 'test_directed_symmetric', 'test_multigraph']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_cuts.py,TestVolume,Unit tests for the :func:`~networkx.volume` function.,5,[],0,"['test_graph', 'test_digraph', 'test_multigraph', 'test_multidigraph', 'test_barbell']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_cuts.py,TestNormalizedCutSize,Unit tests for the :func:`~networkx.normalized_cut_size` function.,2,[],0,"['test_graph', 'test_directed']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_cuts.py,TestConductance,Unit tests for the :func:`~networkx.conductance` function.,1,[],0,['test_graph'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_cuts.py,TestEdgeExpansion,Unit tests for the :func:`~networkx.edge_expansion` function.,1,[],0,['test_graph'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_cuts.py,TestNodeExpansion,Unit tests for the :func:`~networkx.node_expansion` function.,1,[],0,['test_graph'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_cuts.py,TestBoundaryExpansion,Unit tests for the :func:`~networkx.boundary_expansion` function.,1,[],0,['test_graph'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_cuts.py,TestMixingExpansion,Unit tests for the :func:`~networkx.mixing_expansion` function.,1,[],0,['test_graph'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_distance_regular.py,TestDistanceRegular,,4,[],0,"['test_is_distance_regular', 'test_not_connected', 'test_global_parameters', 'test_intersection_array']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_distance_regular.py,TestStronglyRegular,"Unit tests for the :func:`~networkx.is_strongly_regular`
function.",3,[],0,"['test_cycle_graph', 'test_petersen_graph', 'test_path_graph']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_dominance.py,TestImmediateDominators,,9,[],0,"['test_exceptions', 'test_singleton', 'test_path', 'test_cycle', 'test_unreachable', 'test_irreducible1', 'test_irreducible2', 'test_domrel_png', 'test_boost_example']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_dominance.py,TestDominanceFrontiers,,13,[],0,"['test_exceptions', 'test_singleton', 'test_path', 'test_cycle', 'test_unreachable', 'test_irreducible1', 'test_irreducible2', 'test_domrel_png', 'test_boost_example', 'test_discard_issue', 'test_loop', 'test_missing_immediate_doms', 'test_loops_larger']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_efficiency.py,TestEfficiency,,8,[],0,"['setup_method', 'test_efficiency_disconnected_nodes', 'test_local_efficiency_disconnected_graph', 'test_efficiency', 'test_global_efficiency', 'test_global_efficiency_complete_graph', 'test_local_efficiency_complete_graph', 'test_using_ego_graph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_matching.py,TestMaxWeightMatching,"Unit tests for the
:func:`~networkx.algorithms.matching.max_weight_matching` function.",22,[],0,"['test_trivial1', 'test_selfloop', 'test_single_edge', 'test_two_path', 'test_path', 'test_square', 'test_edge_attribute_name', 'test_floating_point_weights', 'test_negative_weights', 'test_s_blossom', 'test_s_t_blossom', 'test_nested_s_blossom', 'test_nested_s_blossom_relabel', 'test_nested_s_blossom_expand', 'test_s_blossom_relabel_expand', 'test_nested_s_blossom_relabel_expand', 'test_nasty_blossom1', 'test_nasty_blossom2', 'test_nasty_blossom_least_slack', 'test_nasty_blossom_augmenting', 'test_nasty_blossom_expand_recursively', 'test_wrong_graph_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_matching.py,TestIsMatching,"Unit tests for the
:func:`~networkx.algorithms.matching.is_matching` function.",9,[],0,"['test_dict', 'test_empty_matching', 'test_single_edge', 'test_edge_order', 'test_valid_matching', 'test_invalid_input', 'test_selfloops', 'test_invalid_matching', 'test_invalid_edge']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_matching.py,TestIsMaximalMatching,"Unit tests for the
:func:`~networkx.algorithms.matching.is_maximal_matching` function.",5,[],0,"['test_dict', 'test_invalid_input', 'test_valid', 'test_not_matching', 'test_not_maximal']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_matching.py,TestIsPerfectMatching,"Unit tests for the
:func:`~networkx.algorithms.matching.is_perfect_matching` function.",7,[],0,"['test_dict', 'test_valid', 'test_valid_not_path', 'test_invalid_input', 'test_selfloops', 'test_not_matching', 'test_maximal_but_not_perfect']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_matching.py,TestMaximalMatching,"Unit tests for the
:func:`~networkx.algorithms.matching.maximal_matching`.",5,[],0,"['test_valid_matching', 'test_single_edge_matching', 'test_self_loops', 'test_ordering', 'test_wrong_graph_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_max_weight_clique.py,TestMaximumWeightClique,,5,[],0,"['test_basic_cases', 'test_key_error', 'test_error_on_non_integer_weight', 'test_unaffected_by_self_loops', 'test_30_node_prob']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_node_classification.py,TestHarmonicFunction,,8,[],0,"['test_path_graph', 'test_no_labels', 'test_no_nodes', 'test_no_edges', 'test_digraph', 'test_one_labeled_node', 'test_nodes_all_labeled', 'test_labeled_nodes_are_not_changed']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_node_classification.py,TestLocalAndGlobalConsistency,,7,[],0,"['test_path_graph', 'test_no_labels', 'test_no_nodes', 'test_no_edges', 'test_digraph', 'test_one_labeled_node', 'test_nodes_all_labeled']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_reciprocity.py,TestReciprocity,,5,[],0,"['test_reciprocity_digraph', 'test_overall_reciprocity_empty_graph', 'test_reciprocity_graph_nodes', 'test_reciprocity_graph_node', 'test_reciprocity_graph_isolated_nodes']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_regular.py,TestKFactor,,6,[],0,"['test_k_factor_trivial', 'test_k_factor1', 'test_k_factor2', 'test_k_factor3', 'test_k_factor4', 'test_k_factor5']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_regular.py,TestIsRegular,,4,[],0,"['test_is_regular1', 'test_is_regular2', 'test_is_regular3', 'test_is_regular4']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_regular.py,TestIsKRegular,,3,[],0,"['test_is_k_regular1', 'test_is_k_regular2', 'test_is_k_regular3']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_simple_paths.py,TestIsSimplePath,"Unit tests for the
:func:`networkx.algorithms.simple_paths.is_simple_path` function.",13,[],0,"['test_empty_list', 'test_trivial_path', 'test_trivial_nonpath', 'test_simple_path', 'test_non_simple_path', 'test_cycle', 'test_missing_node', 'test_missing_starting_node', 'test_directed_path', 'test_directed_non_path', 'test_directed_cycle', 'test_multigraph', 'test_multidigraph']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_structuralholes.py,TestStructuralHoles,"Unit tests for computing measures of structural holes.

The expected values for these functions were originally computed using the
proprietary software `UCINET`_ and the free software `IGraph`_ , and then
computed by hand to make sure that the results are correct.

.. _UCINET: https://sites.google.com/site/ucinetsoftware/home
.. _IGraph: http://igraph.org/",13,[],0,"['setup_method', 'test_constraint_directed', 'test_effective_size_directed', 'test_constraint_weighted_directed', 'test_effective_size_weighted_directed', 'test_constraint_undirected', 'test_effective_size_undirected_borgatti', 'test_effective_size_undirected', 'test_constraint_weighted_undirected', 'test_effective_size_weighted_undirected', 'test_constraint_isolated', 'test_effective_size_isolated', 'test_effective_size_borgatti_isolated']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_threshold.py,TestGeneratorThreshold,,18,[],0,"['test_threshold_sequence_graph_test', 'test_creation_sequences', 'test_make_compact', 'test_uncompact', 'test_creation_sequence_to_weights', 'test_weights_to_creation_sequence', 'test_find_alternating_4_cycle', 'test_shortest_path', 'test_shortest_path_length', 'test_random_threshold_sequence', 'test_right_d_threshold_sequence', 'test_left_d_threshold_sequence', 'test_weights_thresholds', 'test_finding_routines', 'test_fast_versions_properties_threshold_graphs', 'test_tg_creation_routines', 'test_eigenvectors', 'test_create_using']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_time_dependent.py,TestCdIndex,Unit testing for the cd index function.,12,[],0,"['test_common_graph', 'test_common_graph_with_given_attributes', 'test_common_graph_with_int_attributes', 'test_common_graph_with_float_attributes', 'test_common_graph_with_weights', 'test_node_with_no_predecessors', 'test_node_with_no_successors', 'test_n_equals_zero', 'test_time_timedelta_compatibility', 'test_node_with_no_time', 'test_maximally_consolidating', 'test_maximally_destabilizing']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_vitality.py,TestClosenessVitality,,6,[],0,"['test_unweighted', 'test_weighted', 'test_unweighted_digraph', 'test_weighted_digraph', 'test_weighted_multidigraph', 'test_disconnecting_graph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tests\test_voronoi.py,TestVoronoiCells,Unit tests for the Voronoi cells function.,10,[],0,"['test_isolates', 'test_undirected_unweighted', 'test_directed_unweighted', 'test_directed_inward', 'test_undirected_weighted', 'test_directed_weighted', 'test_multigraph_unweighted', 'test_multidigraph_unweighted', 'test_multigraph_weighted', 'test_multidigraph_weighted']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tree\branchings.py,ArborescenceIterator,"Iterate over all spanning arborescences of a graph in either increasing or
decreasing cost.

Notes
-----
This iterator uses the partition scheme from [1]_ (included edges,
excluded edges and open edges). It generates minimum spanning
arborescences using a modified Edmonds' Algorithm which respects the
partition of edges. For arborescences with the same weight, ties are
broken arbitrarily.

References
----------
.. [1] G.K. Janssens, K. Sörensen, An algorithm to generate all spanning
       trees in order of increasing cost, Pesquisa Operacional, 2005-08,
       Vol. 25 (2), p. 219-229,
       https://www.scielo.br/j/pope/a/XHswBwRwJyrfL88dmMwYNWp/?lang=en",6,[],0,"['__init__', '__iter__', '__next__', '_partition', '_write_partition', '_clear_partition']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tree\coding.py,NotATree,"Raised when a function expects a tree (that is, a connected
undirected graph with no cycles) but gets a non-tree graph as input
instead.",0,['nx.NetworkXException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\approximation\tests\test_clique.py,TestCliqueRemoval,"Unit tests for the
:func:`~networkx.algorithms.approximation.clique_removal` function.",3,[],0,"['test_trivial_graph', 'test_complete_graph', 'test_barbell_graph']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\approximation\tests\test_clique.py,TestMaxClique,"Unit tests for the :func:`networkx.algorithms.approximation.max_clique`
function.",3,[],0,"['test_null_graph', 'test_complete_graph', 'test_maximal_by_cardinality']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\approximation\tests\test_distance_measures.py,TestDiameter,"Unit tests for the approximate diameter function
:func:`~networkx.algorithms.approximation.distance_measures.diameter`.",8,[],0,"['test_null_graph', 'test_undirected_non_connected', 'test_directed_non_strongly_connected', 'test_complete_undirected_graph', 'test_complete_directed_graph', 'test_undirected_path_graph', 'test_directed_path_graph', 'test_single_node']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\approximation\tests\test_dominating_set.py,TestMinWeightDominatingSet,,4,[],0,"['test_min_weighted_dominating_set', 'test_star_graph', 'test_null_graph', 'test_min_edge_dominating_set']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\approximation\tests\test_vertex_cover.py,TestMWVC,"Unit tests for the approximate minimum weighted vertex cover
function,
:func:`~networkx.algorithms.approximation.vertex_cover.min_weighted_vertex_cover`.",4,[],0,"['test_unweighted_directed', 'test_unweighted_undirected', 'test_weighted', 'test_unweighted_self_loop']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_connectivity.py,TestNeighborConnectivity,,10,[],0,"['test_degree_p4', 'test_degree_p4_weighted', 'test_weight_keyword', 'test_degree_barrat', 'test_zero_deg', 'test_in_out_weight', 'test_invalid_source', 'test_invalid_target', 'test_invalid_undirected_graph', 'test_single_node']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_correlation.py,TestDegreeMixingCorrelation,,11,['BaseTestDegreeMixing'],0,"['test_degree_assortativity_undirected', 'test_degree_assortativity_node_kwargs', 'test_degree_assortativity_directed', 'test_degree_assortativity_directed2', 'test_degree_assortativity_multigraph', 'test_degree_pearson_assortativity_undirected', 'test_degree_pearson_assortativity_directed', 'test_degree_pearson_assortativity_directed2', 'test_degree_pearson_assortativity_multigraph', 'test_degree_assortativity_weighted', 'test_degree_assortativity_double_star']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_correlation.py,TestAttributeMixingCorrelation,,10,['BaseTestAttributeMixing'],0,"['test_attribute_assortativity_undirected', 'test_attribute_assortativity_directed', 'test_attribute_assortativity_multigraph', 'test_attribute_assortativity_coefficient', 'test_attribute_assortativity_coefficient2', 'test_attribute_assortativity', 'test_attribute_assortativity_negative', 'test_assortativity_node_kwargs', 'test_attribute_assortativity_float', 'test_attribute_assortativity_mixed']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_mixing.py,TestDegreeMixingDict,,5,['BaseTestDegreeMixing'],0,"['test_degree_mixing_dict_undirected', 'test_degree_mixing_dict_undirected_normalized', 'test_degree_mixing_dict_directed', 'test_degree_mixing_dict_multigraph', 'test_degree_mixing_dict_weighted']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_mixing.py,TestDegreeMixingMatrix,,6,['BaseTestDegreeMixing'],0,"['test_degree_mixing_matrix_undirected', 'test_degree_mixing_matrix_directed', 'test_degree_mixing_matrix_multigraph', 'test_degree_mixing_matrix_selfloop', 'test_degree_mixing_matrix_weighted', 'test_degree_mixing_matrix_mapping']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_mixing.py,TestAttributeMixingDict,,3,['BaseTestAttributeMixing'],0,"['test_attribute_mixing_dict_undirected', 'test_attribute_mixing_dict_directed', 'test_attribute_mixing_dict_multigraph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_mixing.py,TestAttributeMixingMatrix,,5,['BaseTestAttributeMixing'],0,"['test_attribute_mixing_matrix_undirected', 'test_attribute_mixing_matrix_directed', 'test_attribute_mixing_matrix_multigraph', 'test_attribute_mixing_matrix_negative', 'test_attribute_mixing_matrix_float']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_neighbor_degree.py,TestAverageNeighbor,,6,[],0,"['test_degree_p4', 'test_degree_p4_weighted', 'test_degree_k4', 'test_degree_k4_nodes', 'test_degree_barrat', 'test_error_invalid_source_target']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_pairs.py,TestAttributeMixingXY,,5,['BaseTestAttributeMixing'],0,"['test_node_attribute_xy_undirected', 'test_node_attribute_xy_undirected_nodes', 'test_node_attribute_xy_directed', 'test_node_attribute_xy_multigraph', 'test_node_attribute_xy_selfloop']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\assortativity\tests\test_pairs.py,TestDegreeMixingXY,,6,['BaseTestDegreeMixing'],0,"['test_node_degree_xy_undirected', 'test_node_degree_xy_undirected_nodes', 'test_node_degree_xy_directed', 'test_node_degree_xy_multigraph', 'test_node_degree_xy_selfloop', 'test_node_degree_xy_weighted']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\bipartite\tests\test_basic.py,TestBipartiteBasic,,15,[],0,"['test_is_bipartite', 'test_bipartite_color', 'test_not_bipartite_color', 'test_bipartite_directed', 'test_bipartite_sets', 'test_bipartite_sets_directed', 'test_bipartite_sets_given_top_nodes', 'test_bipartite_sets_disconnected', 'test_is_bipartite_node_set', 'test_bipartite_density', 'test_bipartite_degrees', 'test_bipartite_weighted_degrees', 'test_biadjacency_matrix_weight', 'test_biadjacency_matrix', 'test_biadjacency_matrix_order']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\bipartite\tests\test_covering.py,TestMinEdgeCover,Tests for :func:`networkx.algorithms.bipartite.min_edge_cover`,4,[],0,"['test_empty_graph', 'test_graph_single_edge', 'test_bipartite_default', 'test_bipartite_explicit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\bipartite\tests\test_matrix.py,TestBiadjacencyMatrix,,12,[],0,"['test_biadjacency_matrix_weight', 'test_biadjacency_matrix', 'test_biadjacency_matrix_order', 'test_biadjacency_matrix_empty_graph', 'test_null_graph', 'test_empty_graph', 'test_duplicate_row', 'test_duplicate_col', 'test_format_keyword', 'test_from_biadjacency_roundtrip', 'test_from_biadjacency_weight', 'test_from_biadjacency_multigraph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\bipartite\tests\test_spectral_bipartivity.py,TestSpectralBipartivity,,3,[],0,"['test_star_like', 'test_k23_like', 'test_single_nodes']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_betweenness_centrality.py,TestBetweennessCentrality,,15,[],0,"['test_K5', 'test_K5_endpoints', 'test_P3_normalized', 'test_P3', 'test_sample_from_P3', 'test_P3_endpoints', 'test_krackhardt_kite_graph', 'test_krackhardt_kite_graph_normalized', 'test_florentine_families_graph', 'test_les_miserables_graph', 'test_ladder_graph', 'test_disconnected_path', 'test_disconnected_path_endpoints', 'test_directed_path', 'test_directed_path_normalized']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_betweenness_centrality.py,TestWeightedBetweennessCentrality,,12,[],0,"['test_K5', 'test_P3_normalized', 'test_P3', 'test_krackhardt_kite_graph', 'test_krackhardt_kite_graph_normalized', 'test_florentine_families_graph', 'test_les_miserables_graph', 'test_ladder_graph', 'test_G', 'test_G2', 'test_G3', 'test_G4']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_betweenness_centrality.py,TestEdgeBetweennessCentrality,,6,[],0,"['test_K5', 'test_normalized_K5', 'test_C4', 'test_P4', 'test_normalized_P4', 'test_balanced_tree']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_betweenness_centrality.py,TestWeightedEdgeBetweennessCentrality,,8,[],0,"['test_K5', 'test_C4', 'test_P4', 'test_balanced_tree', 'test_weighted_graph', 'test_normalized_weighted_graph', 'test_weighted_multigraph', 'test_normalized_weighted_multigraph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_betweenness_centrality_subset.py,TestSubsetBetweennessCentrality,,11,[],0,"['test_K5', 'test_P5_directed', 'test_P5', 'test_P5_multiple_target', 'test_box', 'test_box_and_path', 'test_box_and_path2', 'test_diamond_multi_path', 'test_normalized_p2', 'test_normalized_P5_directed', 'test_weighted_graph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_betweenness_centrality_subset.py,TestEdgeSubsetBetweennessCentrality,,11,[],0,"['test_K5', 'test_P5_directed', 'test_P5', 'test_P5_multiple_target', 'test_box', 'test_box_and_path', 'test_box_and_path2', 'test_diamond_multi_path', 'test_normalized_p1', 'test_normalized_P5_directed', 'test_weighted_graph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_current_flow_betweenness_centrality.py,TestFlowBetweennessCentrality,,6,[],0,"['test_K4_normalized', 'test_K4', 'test_P4_normalized', 'test_P4', 'test_star', 'test_solvers2']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_current_flow_betweenness_centrality.py,TestApproximateFlowBetweennessCentrality,,7,[],0,"['test_K4_normalized', 'test_K4', 'test_star', 'test_grid', 'test_seed', 'test_solvers', 'test_lower_kmax']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_current_flow_betweenness_centrality.py,TestWeightedFlowBetweennessCentrality,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_current_flow_betweenness_centrality.py,TestEdgeFlowBetweennessCentrality,,4,[],0,"['test_K4', 'test_K4_normalized', 'test_C4', 'test_P4']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_current_flow_betweenness_centrality_subset.py,TestFlowBetweennessCentrality,,5,[],0,"['test_K4_normalized', 'test_K4', 'test_P4_normalized', 'test_P4', 'test_star']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_current_flow_betweenness_centrality_subset.py,TestEdgeFlowBetweennessCentrality,,4,[],0,"['test_K4_normalized', 'test_K4', 'test_C4', 'test_P4']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_current_flow_closeness.py,TestFlowClosenessCentrality,,4,[],0,"['test_K4', 'test_P4', 'test_star', 'test_current_flow_closeness_centrality_not_connected']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_current_flow_closeness.py,TestWeightedFlowClosenessCentrality,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_degree_centrality.py,TestDegreeCentrality,,8,[],0,"['setup_method', 'test_degree_centrality_1', 'test_degree_centrality_2', 'test_degree_centrality_3', 'test_degree_centrality_4', 'test_indegree_centrality', 'test_outdegree_centrality', 'test_small_graph_centrality']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_dispersion.py,TestDispersion,,4,[],0,"['test_article', 'test_results_length', 'test_dispersion_v_only', 'test_impossible_things']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_reaching.py,TestGlobalReachingCentrality,Unit tests for the global reaching centrality function.,12,[],0,"['test_non_positive_weights', 'test_negatively_weighted', 'test_directed_star', 'test_undirected_unweighted_star', 'test_undirected_weighted_star', 'test_cycle_directed_unweighted', 'test_cycle_undirected_unweighted', 'test_cycle_directed_weighted', 'test_cycle_undirected_weighted', 'test_directed_weighted', 'test_single_node_with_cycle', 'test_single_node_with_weighted_cycle']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_reaching.py,TestLocalReachingCentrality,Unit tests for the local reaching centrality function.,7,[],0,"['test_non_positive_weights', 'test_negatively_weighted', 'test_undirected_unweighted_star', 'test_undirected_weighted_star', 'test_undirected_weighted_normalized', 'test_single_node_with_cycle', 'test_single_node_with_weighted_cycle']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_subgraph.py,TestSubgraph,,5,[],0,"['test_subgraph_centrality', 'test_subgraph_centrality_big_graph', 'test_communicability_betweenness_centrality_small', 'test_communicability_betweenness_centrality', 'test_estrada_index']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\centrality\tests\test_voterank.py,TestVoteRankCentrality,,6,[],0,"['test_voterank_centrality_1', 'test_voterank_emptygraph', 'test_voterank_centrality_2', 'test_voterank_centrality_3', 'test_voterank_centrality_4', 'test_voterank_centrality_5']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\coloring\tests\test_coloring.py,TestColoring,,17,[],0,"['test_basic_cases', 'test_special_cases', 'test_interchange_invalid', 'test_bad_inputs', 'test_strategy_as_function', 'test_seed_argument', 'test_is_coloring', 'test_is_equitable', 'test_num_colors', 'test_equitable_color', 'test_equitable_color_empty', 'test_equitable_color_large', 'test_case_V_plus_not_in_A_cal', 'test_cast_no_solo', 'test_hard_prob', 'test_hardest_prob', 'test_strategy_saturation_largest_first']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\community\tests\test_centrality.py,TestGirvanNewman,"Unit tests for the
:func:`networkx.algorithms.community.centrality.girvan_newman`
function.",5,[],0,"['test_no_edges', 'test_undirected', 'test_directed', 'test_selfloops', 'test_most_valuable_edge']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\community\tests\test_kclique.py,TestZacharyKarateClub,,7,[],0,"['setup_method', '_check_communities', 'test_k2', 'test_k3', 'test_k4', 'test_k5', 'test_k6']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\community\tests\test_quality.py,TestPerformance,Unit tests for the :func:`performance` function.,2,[],0,"['test_bad_partition', 'test_good_partition']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\community\tests\test_quality.py,TestCoverage,Unit tests for the :func:`coverage` function.,2,[],0,"['test_bad_partition', 'test_good_partition']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\components\tests\test_semiconnected.py,TestIsSemiconnected,,8,[],0,"['test_undirected', 'test_empty', 'test_single_node_graph', 'test_path', 'test_cycle', 'test_tree', 'test_dumbbell', 'test_alternating_path']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\flow\tests\test_maxflow.py,TestMaxflowMinCutCommon,,16,[],0,"['test_graph1', 'test_graph2', 'test_digraph1', 'test_digraph2', 'test_digraph3', 'test_digraph4', 'test_wikipedia_dinitz_example', 'test_optional_capacity', 'test_digraph_infcap_edges', 'test_digraph_infcap_path', 'test_graph_infcap_edges', 'test_digraph5', 'test_disconnected', 'test_source_target_not_in_graph', 'test_source_target_coincide', 'test_multigraphs_raise']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\flow\tests\test_maxflow.py,TestMaxFlowMinCutInterface,,7,[],0,"['setup_method', 'test_flow_func_not_callable', 'test_flow_func_parameters', 'test_minimum_cut_no_cutoff', 'test_kwargs', 'test_kwargs_default_flow_func', 'test_reusing_residual']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\flow\tests\test_maxflow.py,TestCutoff,,2,[],0,"['test_cutoff', 'test_complete_graph_cutoff']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\flow\tests\test_mincost.py,TestMinCostFlow,,19,[],0,"['test_simple_digraph', 'test_negcycle_infcap', 'test_sum_demands_not_zero', 'test_no_flow_satisfying_demands', 'test_transshipment', 'test_max_flow_min_cost', 'test_digraph1', 'test_digraph2', 'test_digraph3', 'test_zero_capacity_edges', 'test_digon', 'test_deadend', 'test_infinite_capacity_neg_digon', 'test_finite_capacity_neg_digon', 'test_multidigraph', 'test_negative_selfloops', 'test_bone_shaped', 'test_exceptions', 'test_large']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\tests\test_match_helpers.py,TestGenericMultiEdgeMatch,,2,[],0,"['setup_method', 'test_generic_multiedge_match']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\tests\test_temporalisomorphvf2.py,TestTimeRespectingGraphMatcher,A test class for the undirected temporal graph matcher.,9,[],0,"['provide_g1_topology', 'provide_g2_path_3edges', 'test_timdelta_zero_timeRespecting_returnsTrue', 'test_timdelta_zero_datetime_timeRespecting_returnsTrue', 'test_attNameStrange_timdelta_zero_timeRespecting_returnsTrue', 'test_notTimeRespecting_returnsFalse', 'test_timdelta_one_config0_returns_no_embeddings', 'test_timdelta_one_config1_returns_four_embedding', 'test_timdelta_one_config2_returns_ten_embeddings']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\tests\test_temporalisomorphvf2.py,TestDiTimeRespectingGraphMatcher,A test class for the directed time-respecting graph matcher.,7,[],0,"['provide_g1_topology', 'provide_g2_path_3edges', 'test_timdelta_zero_same_dates_returns_true', 'test_attNameStrange_timdelta_zero_same_dates_returns_true', 'test_timdelta_one_config0_returns_no_embeddings', 'test_timdelta_one_config1_returns_one_embedding', 'test_timdelta_one_config2_returns_two_embeddings']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\tests\test_vf2userfunc.py,TestNodeMatch_Graph,,10,[],0,"['setup_method', 'build', 'test_noweight_nocolor', 'test_color1', 'test_color2', 'test_weight1', 'test_weight2', 'test_colorsandweights1', 'test_colorsandweights2', 'test_colorsandweights3']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\tests\test_vf2userfunc.py,TestEdgeMatch_MultiGraph,,7,[],0,"['setup_method', 'build', 'test_weights_only', 'test_colors_only', 'test_colorsandweights', 'test_generic1', 'test_generic2']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\tests\test_vf2userfunc.py,TestEdgeMatch_DiGraph,,1,['TestNodeMatch_Graph'],0,['setup_method'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\isomorphism\tests\test_vf2userfunc.py,TestEdgeMatch_MultiDiGraph,,1,['TestEdgeMatch_MultiGraph'],0,['setup_method'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tree\tests\test_coding.py,TestPruferSequence,"Unit tests for the Prüfer sequence encoding and decoding
functions.",8,[],0,"['test_nontree', 'test_null_graph', 'test_trivial_graph', 'test_bad_integer_labels', 'test_encoding', 'test_decoding', 'test_decoding2', 'test_inverse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\algorithms\tree\tests\test_coding.py,TestNestedTuple,Unit tests for the nested tuple encoding and decoding functions.,6,[],0,"['test_nontree', 'test_unknown_root', 'test_encoding', 'test_canonical_form', 'test_decoding', 'test_sensible_relabeling']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_coreviews.py,TestAtlasView,,9,[],0,"['setup_method', 'test_pickle', 'test_len', 'test_iter', 'test_getitem', 'test_copy', 'test_items', 'test_str', 'test_repr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_coreviews.py,TestAdjacencyView,,9,[],0,"['setup_method', 'test_pickle', 'test_len', 'test_iter', 'test_getitem', 'test_copy', 'test_items', 'test_str', 'test_repr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_coreviews.py,TestMultiAdjacencyView,,3,['TestAdjacencyView'],0,"['setup_method', 'test_getitem', 'test_copy']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_coreviews.py,TestUnionAtlas,,9,[],0,"['setup_method', 'test_pickle', 'test_len', 'test_iter', 'test_getitem', 'test_copy', 'test_items', 'test_str', 'test_repr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_coreviews.py,TestUnionAdjacency,,8,[],0,"['setup_method', 'test_pickle', 'test_len', 'test_iter', 'test_getitem', 'test_copy', 'test_str', 'test_repr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_coreviews.py,TestUnionMultiInner,,4,['TestUnionAdjacency'],0,"['setup_method', 'test_len', 'test_getitem', 'test_copy']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_coreviews.py,TestUnionMultiAdjacency,,3,['TestUnionAdjacency'],0,"['setup_method', 'test_getitem', 'test_copy']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_coreviews.py,TestFilteredGraphs,,4,[],0,"['setup_method', 'test_hide_show_nodes', 'test_str_repr', 'test_copy']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_digraph.py,BaseDiGraphTester,,20,['BaseGraphTester'],0,"['test_has_successor', 'test_successors', 'test_has_predecessor', 'test_predecessors', 'test_edges', 'test_out_edges', 'test_out_edges_dir', 'test_out_edges_data', 'test_in_edges_dir', 'test_in_edges_data', 'test_degree', 'test_in_degree', 'test_out_degree', 'test_size', 'test_to_undirected_reciprocal', 'test_reverse_copy', 'test_reverse_nocopy', 'test_reverse_hashable', 'test_di_cache_reset', 'test_di_attributes_cached']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_digraph.py,BaseAttrDiGraphTester,,3,"['BaseDiGraphTester', 'BaseAttrGraphTester']",0,"['test_edges_data', 'test_in_degree_weighted', 'test_out_degree_weighted']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_digraph.py,TestDiGraph,Tests specific to dict-of-dict-of-dict digraph data structure,8,"['BaseAttrDiGraphTester', '_TestGraph']",0,"['setup_method', 'test_data_input', 'test_add_edge', 'test_add_edges_from', 'test_remove_edge', 'test_remove_edges_from', 'test_clear', 'test_clear_edges']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_digraph.py,TestEdgeSubgraph,Unit tests for the :meth:`DiGraph.edge_subgraph` method.,2,['_TestGraphEdgeSubgraph'],0,"['setup_method', 'test_pred_succ']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_filters.py,TestFilterFactory,,11,[],0,"['test_no_filter', 'test_hide_nodes', 'test_show_nodes', 'test_hide_edges', 'test_show_edges', 'test_hide_diedges', 'test_show_diedges', 'test_hide_multiedges', 'test_show_multiedges', 'test_hide_multidiedges', 'test_show_multidiedges']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_special.py,TestSpecialGraph,,1,['_TestGraph'],0,['setup_method'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_special.py,TestThinGraph,,1,['BaseGraphTester'],0,['setup_method'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_special.py,TestSpecialDiGraph,,1,['_TestDiGraph'],0,['setup_method'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_special.py,TestThinDiGraph,,1,['BaseDiGraphTester'],0,['setup_method'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_special.py,TestSpecialMultiGraph,,1,['_TestMultiGraph'],0,['setup_method'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\classes\tests\test_special.py,TestSpecialMultiDiGraph,,1,['_TestMultiDiGraph'],0,['setup_method'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_degree_seq.py,TestConfigurationModel,"Unit tests for the :func:`~networkx.configuration_model`
function.",6,[],0,"['test_empty_degree_sequence', 'test_degree_zero', 'test_degree_sequence', 'test_random_seed', 'test_directed_disallowed', 'test_odd_degree_sum']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_directed.py,TestGeneratorsDirected,,3,[],0,"['test_smoke_test_random_graphs', 'test_create_using_keyword_arguments', 'test_parameters']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_directed.py,TestRandomKOutGraph,"Unit tests for the
:func:`~networkx.generators.directed.random_k_out_graph` function.",3,[],0,"['test_regularity', 'test_no_self_loops', 'test_negative_alpha']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_directed.py,TestUniformRandomKOutGraph,"Unit tests for the
:func:`~networkx.generators.directed.random_uniform_k_out_graph`
function.",4,[],0,"['test_regularity', 'test_no_self_loops', 'test_with_replacement', 'test_without_replacement']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_duplication.py,TestDuplicationDivergenceGraph,"Unit tests for the
:func:`networkx.generators.duplication.duplication_divergence_graph`
function.",6,[],0,"['test_final_size', 'test_probability_too_large', 'test_probability_too_small', 'test_non_extreme_probability_value', 'test_minimum_desired_nodes', 'test_create_using']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_duplication.py,TestPartialDuplicationGraph,"Unit tests for the
:func:`networkx.generators.duplication.partial_duplication_graph`
function.",5,[],0,"['test_final_size', 'test_initial_clique_size', 'test_invalid_initial_size', 'test_invalid_probabilities', 'test_create_using']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_ego.py,TestGeneratorEgo,,2,[],0,"['test_ego', 'test_ego_distance']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_harary_graph.py,TestHararyGraph,"Suppose n nodes, m >= n-1 edges, d = 2m // n, r = 2m % n",2,[],0,"['test_hnm_harary_graph', 'test_hkn_harary_graph']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_intersection.py,TestIntersectionGraph,,4,[],0,"['test_random_intersection_graph', 'test_k_random_intersection_graph', 'test_k_random_intersection_graph_seeded', 'test_general_random_intersection_graph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_interval_graph.py,TestIntervalGraph,Unit tests for :func:`networkx.generators.interval_graph.interval_graph`,8,[],0,"['test_empty', 'test_interval_graph_check_invalid', 'test_interval_graph_0', 'test_interval_graph_1', 'test_interval_graph_2', 'test_interval_graph_3', 'test_interval_graph_4', 'test_interval_graph_5']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_lattice.py,TestGrid2DGraph,Unit tests for :func:`networkx.generators.lattice.grid_2d_graph`,10,[],0,"['test_number_of_vertices', 'test_degree_distribution', 'test_directed', 'test_multigraph', 'test_periodic', 'test_periodic_iterable', 'test_periodic_directed', 'test_periodic_multigraph', 'test_exceptions', 'test_node_input']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_lattice.py,TestGridGraph,Unit tests for :func:`networkx.generators.lattice.grid_graph`,3,[],0,"['test_grid_graph', 'test_node_input', 'test_periodic_iterable']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_lattice.py,TestHypercubeGraph,Unit tests for :func:`networkx.generators.lattice.hypercube_graph`,2,[],0,"['test_special_cases', 'test_degree_distribution']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_lattice.py,TestTriangularLatticeGraph,Tests for :func:`networkx.generators.lattice.triangular_lattice_graph`,4,[],0,"['test_lattice_points', 'test_directed', 'test_multigraph', 'test_periodic']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_lattice.py,TestHexagonalLatticeGraph,Tests for :func:`networkx.generators.lattice.hexagonal_lattice_graph`,4,[],0,"['test_lattice_points', 'test_directed', 'test_multigraph', 'test_periodic']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_line.py,TestGeneratorLine,,11,[],0,"['test_star', 'test_path', 'test_cycle', 'test_digraph1', 'test_multigraph1', 'test_multigraph2', 'test_multidigraph1', 'test_multidigraph2', 'test_digraph2', 'test_create1', 'test_create2']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_line.py,TestGeneratorInverseLine,,20,[],0,"['test_example', 'test_example_2', 'test_pair', 'test_line', 'test_triangle_graph', 'test_cycle', 'test_empty', 'test_K1', 'test_edgeless_graph', 'test_selfloops_error', 'test_non_line_graphs', 'test_wrong_graph_type', 'test_line_inverse_line_complete', 'test_line_inverse_line_path', 'test_line_inverse_line_hypercube', 'test_line_inverse_line_cycle', 'test_line_inverse_line_star', 'test_line_inverse_line_multipartite', 'test_line_inverse_line_dgm', 'test_line_different_node_types']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_line.py,TestGeneratorPrivateFunctions,,4,[],0,"['test_triangles_error', 'test_odd_triangles_error', 'test_select_starting_cell_error', 'test_diamond_graph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_mycielski.py,TestMycielski,,3,[],0,"['test_construction', 'test_size', 'test_mycielski_graph_generator']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_nonisomorphic_trees.py,TestGeneratorNonIsomorphicTrees,,5,[],0,"['test_tree_structure', 'test_nonisomorphism', 'test_number_of_nonisomorphic_trees', 'test_nonisomorphic_trees', 'test_nonisomorphic_trees_matrix']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_random_clustered.py,TestRandomClusteredGraph,,5,[],0,"['test_custom_joint_degree_sequence', 'test_tuple_joint_degree_sequence', 'test_invalid_joint_degree_sequence_type', 'test_invalid_joint_degree_sequence_value', 'test_directed_graph_raises_error']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_random_graphs.py,TestGeneratorsRandom,,7,[],0,"['test_random_graph', 'test_dual_barabasi_albert', 'test_extended_barabasi_albert', 'test_random_zero_regular_graph', 'test_gnm', 'test_watts_strogatz_big_k', 'test_random_kernel_graph']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_small.py,TestGeneratorsSmall,,2,[],0,"['test__LCF_graph', 'test_properties_named_small_graphs']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\generators\tests\test_stochastic.py,TestStochasticGraph,Unit tests for the :func:`~networkx.stochastic_graph` function.,7,[],0,"['test_default_weights', 'test_in_place', 'test_arbitrary_weights', 'test_multidigraph', 'test_zero_weights', 'test_graph_disallowed', 'test_multigraph_disallowed']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\readwrite\tests\test_leda.py,TestLEDA,,2,[],0,"['test_parse_leda', 'test_read_LEDA']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\readwrite\tests\test_sparse6.py,TestSparseGraph6,,4,[],0,"['test_from_sparse6_bytes', 'test_from_bytes_multigraph_graph', 'test_read_sparse6', 'test_read_many_graph6']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\readwrite\tests\test_sparse6.py,TestWriteSparse6,"Unit tests for writing graphs in the sparse6 format.

Most of the test cases were checked against the sparse6 encoder in Sage.",12,[],0,"['test_null_graph', 'test_trivial_graph', 'test_empty_graph', 'test_large_empty_graph', 'test_very_large_empty_graph', 'test_complete_graph', 'test_no_header', 'test_padding', 'test_complete_bipartite', 'test_read_write_inverse', 'test_no_directed_graphs', 'test_write_path']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\readwrite\json_graph\tests\test_adjacency.py,TestAdjacency,,8,[],0,"['test_graph', 'test_graph_attributes', 'test_digraph', 'test_multidigraph', 'test_multigraph', 'test_input_data_is_not_modified_when_building_graph', 'test_adjacency_form_json_serialisable', 'test_exception']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\readwrite\json_graph\tests\test_node_link.py,TestNodeLink,,11,[],0,"['test_custom_attrs_dep', 'test_exception_dep', 'test_graph', 'test_graph_attributes', 'test_digraph', 'test_multigraph', 'test_graph_with_tuple_nodes', 'test_unicode_keys', 'test_exception', 'test_string_ids', 'test_custom_attrs']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\tests\test_config.py,ExampleConfig,Example configuration.,1,['Config'],2,['_on_setattr'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\tests\test_config.py,EmptyConfig,,0,['Config'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\tests\test_heaps.py,X,,7,[],0,"['__eq__', '__ne__', '__lt__', '__le__', '__ge__', '__gt__', '__hash__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\tests\test_mapped_queue.py,TestMappedQueue,,22,[],0,"['setup_method', '_check_map', '_make_mapped_queue', 'test_heapify', 'test_init', 'test_incomparable', 'test_len', 'test_siftup_leaf', 'test_siftup_one_child', 'test_siftup_left_child', 'test_siftup_right_child', 'test_siftup_multiple', 'test_siftdown_leaf', 'test_siftdown_single', 'test_siftdown_multiple', 'test_push', 'test_push_duplicate', 'test_pop', 'test_remove_leaf', 'test_remove_root', 'test_update_leaf', 'test_update_root']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\networkx\utils\tests\test_mapped_queue.py,TestMappedDict,,10,['TestMappedQueue'],0,"['_make_mapped_queue', 'test_init', 'test_ties', 'test_pop', 'test_empty_pop', 'test_incomparable_ties', 'test_push', 'test_push_duplicate', 'test_update_leaf', 'test_update_root']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\linalg.py,EighResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\linalg.py,QRResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\linalg.py,SlogdetResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\linalg.py,SVDResult,,0,['NamedTuple'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\_data_type_functions.py,finfo_object,,0,[],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\_data_type_functions.py,iinfo_object,,0,[],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\_set_functions.py,UniqueAllResult,,0,['NamedTuple'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\_set_functions.py,UniqueCountsResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\_set_functions.py,UniqueInverseResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\_typing.py,NestedSequence,,2,['Protocol[_T_co]'],0,"['__getitem__', '__len__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\array_api\_typing.py,SupportsDLPack,,1,['Protocol'],0,['__dlpack__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\compat\py3k.py,contextlib_nullcontext,"Context manager that does no additional processing.

Used as a stand-in for a normal context manager, when a particular
block of code is only sometimes used with a normal context manager:

cm = optional_cm if condition else nullcontext()
with cm:
    # Perform operation, using optional_cm if condition is True

.. note::
    Prefer using `contextlib.nullcontext` instead of this context manager.",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\numerictypes.py,_typedict,"Base object for a dictionary for look-up with any alias for an array dtype.

Instances of `_typedict` can not be used as dictionaries directly,
first they have to be populated.",1,['dict'],0,['__getitem__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\_machar.py,MachAr,"Diagnosing machine parameters.

Attributes
----------
ibeta : int
    Radix in which numbers are represented.
it : int
    Number of base-`ibeta` digits in the floating point mantissa M.
machep : int
    Exponent of the smallest (most negative) power of `ibeta` that,
    added to 1.0, gives something different from 1.0
eps : float
    Floating-point number ``beta**machep`` (floating point precision)
negep : int
    Exponent of the smallest power of `ibeta` that, subtracted
    from 1.0, gives something different from 1.0.
epsneg : float
    Floating-point number ``beta**negep``.
iexp : int
    Number of bits in the exponent (including its sign and bias).
minexp : int
    Smallest (most negative) power of `ibeta` consistent with there
    being no leading zeros in the mantissa.
xmin : float
    Floating-point number ``beta**minexp`` (the smallest [in
    magnitude] positive floating point number with full precision).
maxexp : int
    Smallest (positive) power of `ibeta` that causes overflow.
xmax : float
    ``(1-epsneg) * beta**maxexp`` (the largest [in magnitude]
    usable floating value).
irnd : int
    In ``range(6)``, information on what kind of rounding is done
    in addition, and on how underflow is handled.
ngrd : int
    Number of 'guard digits' used when truncating the product
    of two mantissas to fit the representation.
epsilon : float
    Same as `eps`.
tiny : float
    An alias for `smallest_normal`, kept for backwards compatibility.
huge : float
    Same as `xmax`.
precision : float
    ``- int(-log10(eps))``
resolution : float
    ``- 10**(-precision)``
smallest_normal : float
    The smallest positive floating point number with 1 as leading bit in
    the mantissa following IEEE-754. Same as `xmin`.
smallest_subnormal : float
    The smallest positive floating point number with 0 as leading bit in
    the mantissa following IEEE-754.

Parameters
----------
float_conv : function, optional
    Function that converts an integer or integer array to a float
    or float array. Default is `float`.
int_conv : function, optional
    Function that converts a float or float array to an integer or
    integer array. Default is `int`.
float_to_float : function, optional
    Function that converts a float array to float. Default is `float`.
    Note that this does not seem to do anything useful in the current
    implementation.
float_to_str : function, optional
    Function that converts a single float to a string. Default is
    ``lambda v:'%24.16e' %v``.
title : str, optional
    Title that is printed in the string representation of `MachAr`.

See Also
--------
finfo : Machine limits for floating point types.
iinfo : Machine limits for integer types.

References
----------
.. [1] Press, Teukolsky, Vetterling and Flannery,
       ""Numerical Recipes in C++,"" 2nd ed,
       Cambridge University Press, 2002, p. 31.",3,[],0,"['__init__', '_do_init', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\_ufunc_config.py,_unspecified,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\_ufunc_config.py,errstate,"errstate(**kwargs)

Context manager for floating-point error handling.

Using an instance of `errstate` as a context manager allows statements in
that context to execute with a known error handling behavior. Upon entering
the context the error handling is set with `seterr` and `seterrcall`, and
upon exiting it is reset to what it was before.

..  versionchanged:: 1.17.0
    `errstate` is also usable as a function decorator, saving
    a level of indentation if an entire function is wrapped.
    See :py:class:`contextlib.ContextDecorator` for more information.

Parameters
----------
kwargs : {divide, over, under, invalid}
    Keyword arguments. The valid keywords are the possible floating-point
    exceptions. Each keyword should have a string value that defines the
    treatment for the particular error. Possible values are
    {'ignore', 'warn', 'raise', 'call', 'print', 'log'}.

See Also
--------
seterr, geterr, seterrcall, geterrcall

Notes
-----
For complete documentation of the types of floating-point exceptions and
treatment options, see `seterr`.

Examples
--------
>>> olderr = np.seterr(all='ignore')  # Set error handling to known state.

>>> np.arange(3) / 0.
array([nan, inf, inf])
>>> with np.errstate(divide='warn'):
...     np.arange(3) / 0.
array([nan, inf, inf])

>>> np.sqrt(-1)
nan
>>> with np.errstate(invalid='raise'):
...     np.sqrt(-1)
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
FloatingPointError: invalid value encountered in sqrt

Outside the context the error handling behavior has not changed:

>>> np.geterr()
{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}",3,['contextlib.ContextDecorator'],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\f2py\auxfuncs.py,F2PYError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\f2py\auxfuncs.py,throw_error,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\f2py\f2py2e.py,CombineIncludePaths,,1,['argparse.Action'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\function_base.py,vectorize,"vectorize(pyfunc=np._NoValue, otypes=None, doc=None, excluded=None,
cache=False, signature=None)

Returns an object that acts like pyfunc, but takes arrays as input.

Define a vectorized function which takes a nested sequence of objects or
numpy arrays as inputs and returns a single numpy array or a tuple of numpy
arrays. The vectorized function evaluates `pyfunc` over successive tuples
of the input arrays like the python map function, except it uses the
broadcasting rules of numpy.

The data type of the output of `vectorized` is determined by calling
the function with the first element of the input.  This can be avoided
by specifying the `otypes` argument.

Parameters
----------
pyfunc : callable, optional
    A python function or method.
    Can be omitted to produce a decorator with keyword arguments.
otypes : str or list of dtypes, optional
    The output data type. It must be specified as either a string of
    typecode characters or a list of data type specifiers. There should
    be one data type specifier for each output.
doc : str, optional
    The docstring for the function. If None, the docstring will be the
    ``pyfunc.__doc__``.
excluded : set, optional
    Set of strings or integers representing the positional or keyword
    arguments for which the function will not be vectorized.  These will be
    passed directly to `pyfunc` unmodified.

    .. versionadded:: 1.7.0

cache : bool, optional
    If `True`, then cache the first function call that determines the number
    of outputs if `otypes` is not provided.

    .. versionadded:: 1.7.0

signature : string, optional
    Generalized universal function signature, e.g., ``(m,n),(n)->(m)`` for
    vectorized matrix-vector multiplication. If provided, ``pyfunc`` will
    be called with (and expected to return) arrays with shapes given by the
    size of corresponding core dimensions. By default, ``pyfunc`` is
    assumed to take scalars as input and output.

    .. versionadded:: 1.12.0

Returns
-------
out : callable
    A vectorized function if ``pyfunc`` was provided,
    a decorator otherwise.

See Also
--------
frompyfunc : Takes an arbitrary Python function and returns a ufunc

Notes
-----
The `vectorize` function is provided primarily for convenience, not for
performance. The implementation is essentially a for loop.

If `otypes` is not specified, then a call to the function with the
first argument will be used to determine the number of outputs.  The
results of this call will be cached if `cache` is `True` to prevent
calling the function twice.  However, to implement the cache, the
original function must be wrapped which will slow down subsequent
calls, so only do this if your function is expensive.

The new keyword argument interface and `excluded` argument support
further degrades performance.

References
----------
.. [1] :doc:`/reference/c-api/generalized-ufuncs`

Examples
--------
>>> def myfunc(a, b):
...     ""Return a-b if a>b, otherwise return a+b""
...     if a > b:
...         return a - b
...     else:
...         return a + b

>>> vfunc = np.vectorize(myfunc)
>>> vfunc([1, 2, 3, 4], 2)
array([3, 4, 1, 2])

The docstring is taken from the input function to `vectorize` unless it
is specified:

>>> vfunc.__doc__
'Return a-b if a>b, otherwise return a+b'
>>> vfunc = np.vectorize(myfunc, doc='Vectorized `myfunc`')
>>> vfunc.__doc__
'Vectorized `myfunc`'

The output type is determined by evaluating the first element of the input,
unless it is specified:

>>> out = vfunc([1, 2, 3, 4], 2)
>>> type(out[0])
<class 'numpy.int64'>
>>> vfunc = np.vectorize(myfunc, otypes=[float])
>>> out = vfunc([1, 2, 3, 4], 2)
>>> type(out[0])
<class 'numpy.float64'>

The `excluded` argument can be used to prevent vectorizing over certain
arguments.  This can be useful for array-like arguments of a fixed length
such as the coefficients for a polynomial as in `polyval`:

>>> def mypolyval(p, x):
...     _p = list(p)
...     res = _p.pop(0)
...     while _p:
...         res = res*x + _p.pop(0)
...     return res
>>> vpolyval = np.vectorize(mypolyval, excluded=['p'])
>>> vpolyval(p=[1, 2, 3], x=[0, 1])
array([3, 6])

Positional arguments may also be excluded by specifying their position:

>>> vpolyval.excluded.add(0)
>>> vpolyval([1, 2, 3], x=[0, 1])
array([3, 6])

The `signature` argument allows for vectorizing functions that act on
non-scalar arrays of fixed length. For example, you can use it for a
vectorized calculation of Pearson correlation coefficient and its p-value:

>>> import scipy.stats
>>> pearsonr = np.vectorize(scipy.stats.pearsonr,
...                 signature='(n),(n)->(),()')
>>> pearsonr([[0, 1, 2, 3]], [[1, 2, 3, 4], [4, 3, 2, 1]])
(array([ 1., -1.]), array([ 0.,  0.]))

Or for a vectorized convolution:

>>> convolve = np.vectorize(np.convolve, signature='(n),(m)->(k)')
>>> convolve(np.eye(4), [1, 2, 1])
array([[1., 2., 1., 0., 0., 0.],
       [0., 1., 2., 1., 0., 0.],
       [0., 0., 1., 2., 1., 0.],
       [0., 0., 0., 1., 2., 1.]])

Decorator syntax is supported.  The decorator can be called as
a function to provide keyword arguments.
>>>@np.vectorize
...def identity(x):
...    return x
...
>>>identity([0, 1, 2])
array([0, 1, 2])
>>>@np.vectorize(otypes=[float])
...def as_float(x):
...    return x
...
>>>as_float([0, 1, 2])
array([0., 1., 2.])",7,[],0,"['__init__', '_init_stage_2', '_call_as_normal', '__call__', '_get_ufunc_and_otypes', '_vectorize_call', '_vectorize_call_with_signature']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\stride_tricks.py,DummyArray,"Dummy object that just exists to hang __array_interface__ dictionaries
and possibly keep alive a reference to a base array.",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\utils.py,_Deprecate,"Decorator class to deprecate old functions.

Refer to `deprecate` for details.

See Also
--------
deprecate",2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\_datasource.py,_FileOpeners,"Container for different methods to open (un-)compressed files.

`_FileOpeners` contains a dictionary that holds one method for each
supported file format. Attribute lookup is implemented in such a way
that an instance of `_FileOpeners` itself can be indexed with the keys
of that dictionary. Currently uncompressed files as well as files
compressed with ``gzip``, ``bz2`` or ``xz`` compression are supported.

Notes
-----
`_file_openers`, an instance of `_FileOpeners`, is made available for
use in the `_datasource` module.

Examples
--------
>>> import gzip
>>> np.lib._datasource._file_openers.keys()
[None, '.bz2', '.gz', '.xz', '.lzma']
>>> np.lib._datasource._file_openers['.gz'] is gzip.open
True",4,[],0,"['__init__', '_load', 'keys', '__getitem__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\_datasource.py,DataSource,"DataSource(destpath='.')

A generic data source file (file, http, ftp, ...).

DataSources can be local files or remote files/URLs.  The files may
also be compressed or uncompressed. DataSource hides some of the
low-level details of downloading the file, allowing you to simply pass
in a valid file path (or URL) and obtain a file object.

Parameters
----------
destpath : str or None, optional
    Path to the directory where the source file gets downloaded to for
    use.  If `destpath` is None, a temporary directory will be created.
    The default path is the current directory.

Notes
-----
URLs require a scheme string (``http://``) to be used, without it they
will fail::

    >>> repos = np.DataSource()
    >>> repos.exists('www.google.com/index.html')
    False
    >>> repos.exists('http://www.google.com/index.html')
    True

Temporary directories are deleted when the DataSource is deleted.

Examples
--------
::

    >>> ds = np.DataSource('/home/guido')
    >>> urlname = 'http://www.google.com/'
    >>> gfile = ds.open('http://www.google.com/')
    >>> ds.abspath(urlname)
    '/home/guido/www.google.com/index.html'

    >>> ds = np.DataSource(None)  # use with temporary file
    >>> ds.open('/home/guido/foobar.txt')
    <open file '/home/guido.foobar.txt', mode 'r' at 0x91d4430>
    >>> ds.abspath('/home/guido/foobar.txt')
    '/tmp/.../home/guido/foobar.txt'",13,[],0,"['__init__', '__del__', '_iszip', '_iswritemode', '_splitzipext', '_possible_names', '_isurl', '_cache', '_findfile', 'abspath', '_sanitize_relative_path', 'exists', 'open']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\_datasource.py,Repository,"Repository(baseurl, destpath='.')

A data repository where multiple DataSource's share a base
URL/directory.

`Repository` extends `DataSource` by prepending a base URL (or
directory) to all the files it handles. Use `Repository` when you will
be working with multiple files from one base URL.  Initialize
`Repository` with the base URL, then refer to each file by its filename
only.

Parameters
----------
baseurl : str
    Path to the local directory or remote location that contains the
    data files.
destpath : str or None, optional
    Path to the directory where the source file gets downloaded to for
    use.  If `destpath` is None, a temporary directory will be created.
    The default path is the current directory.

Examples
--------
To analyze all files in the repository, do something like this
(note: this is not self-contained code)::

    >>> repos = np.lib._datasource.Repository('/home/user/data/dir/')
    >>> for filename in filelist:
    ...     fp = repos.open(filename)
    ...     fp.analyze()
    ...     fp.close()

Similarly you could use a URL for a repository::

    >>> repos = np.lib._datasource.Repository('http://www.xyz.edu/data')",8,['DataSource'],0,"['__init__', '__del__', '_fullpath', '_findfile', 'abspath', 'exists', 'open', 'listdir']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\_version.py,NumpyVersion,"Parse and compare numpy version strings.

NumPy has the following versioning scheme (numbers given are examples; they
can be > 9 in principle):

- Released version: '1.8.0', '1.8.1', etc.
- Alpha: '1.8.0a1', '1.8.0a2', etc.
- Beta: '1.8.0b1', '1.8.0b2', etc.
- Release candidates: '1.8.0rc1', '1.8.0rc2', etc.
- Development versions: '1.8.0.dev-f1234afa' (git commit hash appended)
- Development versions after a1: '1.8.0a1.dev-f1234afa',
                                 '1.8.0b2.dev-f1234afa',
                                 '1.8.1rc1.dev-f1234afa', etc.
- Development versions (no git hash available): '1.8.0.dev-Unknown'

Comparing needs to be done against a valid version string or other
`NumpyVersion` instance. Note that all development versions of the same
(pre-)release compare equal.

.. versionadded:: 1.9.0

Parameters
----------
vstring : str
    NumPy version string (``np.__version__``).

Examples
--------
>>> from numpy.lib import NumpyVersion
>>> if NumpyVersion(np.__version__) < '1.7.0':
...     print('skip')
>>> # skip

>>> NumpyVersion('1.7')  # raises ValueError, add "".0""
Traceback (most recent call last):
    ...
ValueError: Not a valid numpy version string",11,[],0,"['__init__', '_compare_version', '_compare_pre_release', '_compare', '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\linalg\linalg.py,EigResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\linalg\linalg.py,EighResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\linalg\linalg.py,QRResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\linalg\linalg.py,SlogdetResult,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\linalg\linalg.py,SVDResult,,0,['NamedTuple'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\linalg\linalg.py,LinAlgError,"Generic Python-exception-derived object raised by linalg functions.

General purpose exception class, derived from Python's ValueError
class, programmatically raised in linalg functions when a Linear
Algebra-related condition would prevent further correct execution of the
function.

Parameters
----------
None

Examples
--------
>>> from numpy import linalg as LA
>>> LA.inv(np.zeros((2,2)))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""...linalg.py"", line 350,
    in inv return wrap(solve(a, identity(a.shape[0], dtype=a.dtype)))
  File ""...linalg.py"", line 249,
    in solve
    raise LinAlgError('Singular matrix')
numpy.linalg.LinAlgError: Singular matrix",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\polynomial\polyutils.py,RankWarning,Issued by chebfit when the design matrix is rank deficient.,0,['UserWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\tests\test_warnings.py,ParseCall,,3,['ast.NodeVisitor'],0,"['__init__', 'visit_Attribute', 'visit_Name']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\tests\test_warnings.py,FindFuncs,,2,['ast.NodeVisitor'],0,"['__init__', 'visit_Call']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\_array_like.py,_SupportsArray,,1,['Protocol[_DType_co]'],0,['__array__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\_array_like.py,_SupportsArrayFunc,A protocol class representing `~class.__array_function__`.,1,['Protocol'],0,['__array_function__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\_array_like.py,_UnknownType,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\_nested_sequence.py,_NestedSequence,"A protocol for representing nested sequences.

Warning
-------
`_NestedSequence` currently does not work in combination with typevars,
*e.g.* ``def func(a: _NestedSequnce[T]) -> T: ...``.

See Also
--------
collections.abc.Sequence
    ABCs for read-only and mutable :term:`sequences`.

Examples
--------
.. code-block:: python

    >>> from __future__ import annotations

    >>> from typing import TYPE_CHECKING
    >>> import numpy as np
    >>> from numpy._typing import _NestedSequence

    >>> def get_dtype(seq: _NestedSequence[float]) -> np.dtype[np.float64]:
    ...     return np.asarray(seq).dtype

    >>> a = get_dtype([1.0])
    >>> b = get_dtype([[1.0]])
    >>> c = get_dtype([[[1.0]]])
    >>> d = get_dtype([[[[1.0]]]])

    >>> if TYPE_CHECKING:
    ...     reveal_locals()
    ...     # note: Revealed local types are:
    ...     # note:     a: numpy.dtype[numpy.floating[numpy._typing._64Bit]]
    ...     # note:     b: numpy.dtype[numpy.floating[numpy._typing._64Bit]]
    ...     # note:     c: numpy.dtype[numpy.floating[numpy._typing._64Bit]]
    ...     # note:     d: numpy.dtype[numpy.floating[numpy._typing._64Bit]]",7,['Protocol[_T_co]'],0,"['__len__', '__getitem__', '__contains__', '__iter__', '__reversed__', 'count', 'index']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,NBitBase,"A type representing `numpy.number` precision during static type checking.

Used exclusively for the purpose static type checking, `NBitBase`
represents the base of a hierarchical set of subclasses.
Each subsequent subclass is herein used for representing a lower level
of precision, *e.g.* ``64Bit > 32Bit > 16Bit``.

.. versionadded:: 1.20

Examples
--------
Below is a typical usage example: `NBitBase` is herein used for annotating
a function that takes a float and integer of arbitrary precision
as arguments and returns a new float of whichever precision is largest
(*e.g.* ``np.float16 + np.int64 -> np.float64``).

.. code-block:: python

    >>> from __future__ import annotations
    >>> from typing import TypeVar, TYPE_CHECKING
    >>> import numpy as np
    >>> import numpy.typing as npt

    >>> T1 = TypeVar(""T1"", bound=npt.NBitBase)
    >>> T2 = TypeVar(""T2"", bound=npt.NBitBase)

    >>> def add(a: np.floating[T1], b: np.integer[T2]) -> np.floating[T1 | T2]:
    ...     return a + b

    >>> a = np.float16()
    >>> b = np.int64()
    >>> out = add(a, b)

    >>> if TYPE_CHECKING:
    ...     reveal_locals()
    ...     # note: Revealed local types are:
    ...     # note:     a: numpy.floating[numpy.typing._16Bit*]
    ...     # note:     b: numpy.signedinteger[numpy.typing._64Bit*]
    ...     # note:     out: numpy.floating[numpy.typing._64Bit*]",1,[],0,['__init_subclass__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,_256Bit,,0,['NBitBase'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,_128Bit,,0,['_256Bit'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,_96Bit,,0,['_128Bit'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,_80Bit,,0,['_96Bit'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,_64Bit,,0,['_80Bit'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,_32Bit,,0,['_64Bit'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,_16Bit,,0,['_32Bit'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\_typing\__init__.py,_8Bit,,0,['_16Bit'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_abc.py,TestABC,,5,[],0,"['test_abstract', 'test_floats', 'test_complex', 'test_int', 'test_uint']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestBasic,,7,[],0,"['test_from_object_array', 'test_from_object_array_unicode', 'test_from_string_array', 'test_from_unicode_array', 'test_unicode_upconvert', 'test_from_string', 'test_from_unicode']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestVecString,,7,[],0,"['test_non_existent_method', 'test_non_string_array', 'test_invalid_args_tuple', 'test_invalid_type_descr', 'test_invalid_function_args', 'test_invalid_result_type', 'test_broadcast_error']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestWhitespace,,2,[],0,"['setup_method', 'test1']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestChar,,2,[],0,"['setup_method', 'test_it']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestComparisons,,8,[],0,"['setup_method', 'test_not_equal', 'test_equal', 'test_greater_equal', 'test_less_equal', 'test_greater', 'test_less', 'test_type']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestComparisonsMixed1,Ticket #1276,1,['TestComparisons'],0,['setup_method'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestComparisonsMixed2,Ticket #1276,1,['TestComparisons'],0,['setup_method'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestInformation,,16,[],0,"['setup_method', 'test_len', 'test_count', 'test_endswith', 'test_find', 'test_index', 'test_isalnum', 'test_isalpha', 'test_isdigit', 'test_islower', 'test_isspace', 'test_istitle', 'test_isupper', 'test_rfind', 'test_rindex', 'test_startswith']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestMethods,,24,[],0,"['setup_method', 'test_capitalize', 'test_center', 'test_decode', 'test_encode', 'test_expandtabs', 'test_join', 'test_ljust', 'test_lower', 'test_lstrip', 'test_partition', 'test_replace', 'test_rjust', 'test_rpartition', 'test_rsplit', 'test_rstrip', 'test_strip', 'test_split', 'test_splitlines', 'test_swapcase', 'test_title', 'test_upper', 'test_isnumeric', 'test_isdecimal']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_defchararray.py,TestOperations,,8,[],0,"['setup_method', 'test_add', 'test_radd', 'test_mul', 'test_rmul', 'test_mod', 'test_rmod', 'test_slice']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_getlimits.py,TestPythonFloat,,1,[],0,['test_singleton'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_getlimits.py,TestHalf,,1,[],0,['test_singleton'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_getlimits.py,TestSingle,,1,[],0,['test_singleton'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_getlimits.py,TestDouble,,1,[],0,['test_singleton'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_getlimits.py,TestLongdouble,,1,[],0,['test_singleton'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_getlimits.py,TestFinfo,,3,[],0,"['test_basic', 'test_regression_gh23108', 'test_regression_gh23867']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_getlimits.py,TestIinfo,,2,[],0,"['test_basic', 'test_unsigned_max']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_getlimits.py,TestRepr,,2,[],0,"['test_iinfo_repr', 'test_finfo_repr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_indexerrors.py,TestIndexErrors,Tests to exercise indexerrors not covered by other tests.,8,[],0,"['test_arraytypes_fasttake', 'test_take_from_object', 'test_multiindex_exceptions', 'test_put_exceptions', 'test_iterators_exceptions', 'test_mapping', 'test_mapping_error_message', 'test_methods']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_machar.py,TestMachAr,,2,[],0,"['_run_machar_highprec', 'test_underlow']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_nditer.py,TestIterNested,,8,[],0,"['test_basic', 'test_reorder', 'test_flip_axes', 'test_broadcast', 'test_dtype_copy', 'test_dtype_buffered', 'test_0d', 'test_iter_nested_iters_dtype_buffered']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,A,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,B,,0,"['A', 'np.float64']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,C,,0,['B'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,D,,0,"['C', 'B']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,B0,,0,"['np.float64', 'A']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,C0,,0,['B0'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,HasNew,,1,[],0,['__new__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,B1,,0,"['np.float64', 'HasNew']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,TestInherit,,3,[],0,"['test_init', 'test_init2', 'test_gh_15395']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test_scalarinherit.py,TestCharacter,,2,[],0,"['test_char_radd', 'test_char_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test__exceptions.py,TestArrayMemoryError,,4,[],0,"['test_pickling', 'test_str', 'test__size_to_string', 'test__total_size']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test__exceptions.py,TestUFuncNoLoopError,,1,[],0,['test_pickling'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\core\tests\test__exceptions.py,TestAxisError,,2,[],0,"['test_attr', 'test_pickling']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\f2py\tests\test_symbolic.py,TestSymbolic,,11,['util.F2PyTest'],0,"['test_eliminate_quotes', 'test_sanity', 'test_tostring_fortran', 'test_tostring_c', 'test_operations', 'test_substitute', 'test_fromstring', 'test_traverse', 'test_linear_solve', 'test_as_numer_denom', 'test_polynomial_atoms']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\f2py\_backends\_distutils.py,DistutilsBackend,,2,['Backend'],0,"['__init__', 'compile']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\f2py\_backends\_meson.py,MesonTemplate,Template meson build file generation class.,8,[],0,"['__init__', 'meson_build_template', 'initialize_template', 'sources_substitution', 'deps_substitution', 'libraries_substitution', 'include_substitution', 'generate_meson_build']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\f2py\_backends\_meson.py,MesonBackend,,6,['Backend'],0,"['__init__', '_move_exec_to_root', 'write_meson_build', '_run_subprocess_command', 'run_meson', 'compile']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\fft\tests\test_helper.py,TestFFTShift,,5,[],0,"['test_definition', 'test_inverse', 'test_axes_keyword', 'test_uneven_dims', 'test_equal_to_original']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\fft\tests\test_helper.py,TestFFTFreq,,1,[],0,['test_definition'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\fft\tests\test_helper.py,TestRFFTFreq,,1,[],0,['test_definition'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\fft\tests\test_helper.py,TestIRFFTN,,1,[],0,['test_not_last_axis_success'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_format.py,BytesIOSRandomSize,,1,['BytesIO'],0,['read'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_regression.py,TestRegression,,27,[],0,"['test_poly1d', 'test_cov_parameters', 'test_mem_digitize', 'test_unique_zero_sized', 'test_mem_vectorise', 'test_mgrid_single_element', 'test_refcount_vectorize', 'test_poly1d_nan_roots', 'test_mem_polymul', 'test_mem_string_concat', 'test_poly_div', 'test_poly_eq', 'test_polyfit_build', 'test_polydiv_type', 'test_histogramdd_too_many_bins', 'test_polyint_type', 'test_ndenumerate_crash', 'test_asfarray_none', 'test_large_fancy_indexing', 'test_void_coercion', 'test_who_with_0dim_array', 'test_include_dirs', 'test_polyder_return_type', 'test_append_fields_dtype_list', 'test_loadtxt_fields_subarrays', 'test_nansum_with_boolean', 'test_py3_compat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_stride_tricks.py,TestSlidingWindowView,,8,[],0,"['test_1d', 'test_2d', 'test_2d_with_axis', 'test_2d_repeated_axis', 'test_2d_without_axis', 'test_errors', 'test_writeable', 'test_subok']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_stride_tricks.py,VerySimpleSubClass,,1,['np.ndarray'],0,['__new__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_stride_tricks.py,SimpleSubClass,,2,['VerySimpleSubClass'],0,"['__new__', '__array_finalize__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestCommonType,,1,[],0,['test_basic'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestMintypecode,,3,[],0,"['test_default_1', 'test_default_2', 'test_default_3']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIsscalar,,1,[],0,['test_basic'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestReal,,2,[],0,"['test_real', 'test_cmplx']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestImag,,2,[],0,"['test_real', 'test_cmplx']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIscomplex,,2,[],0,"['test_fail', 'test_pass']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIsreal,,2,[],0,"['test_pass', 'test_fail']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIscomplexobj,,6,[],0,"['test_basic', 'test_scalar', 'test_list', 'test_duck', 'test_pandas_duck', 'test_custom_dtype_duck']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIsrealobj,,1,[],0,['test_basic'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIsnan,,7,[],0,"['test_goodvalues', 'test_posinf', 'test_neginf', 'test_ind', 'test_integer', 'test_complex', 'test_complex1']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIsfinite,,7,[],0,"['test_goodvalues', 'test_posinf', 'test_neginf', 'test_ind', 'test_integer', 'test_complex', 'test_complex1']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIsinf,,6,[],0,"['test_goodvalues', 'test_posinf', 'test_posinf_scalar', 'test_neginf', 'test_neginf_scalar', 'test_ind']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIsposinf,,1,[],0,['test_generic'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestIsneginf,,1,[],0,['test_generic'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestNanToNum,,8,[],0,"['test_generic', 'test_array', 'test_integer', 'test_float', 'test_complex_good', 'test_complex_bad', 'test_complex_bad2', 'test_do_not_rewrite_previous_keyword']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestRealIfClose,,1,[],0,['test_basic'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_type_check.py,TestArrayConversion,,1,[],0,['test_asfarray'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_ufunclike.py,TestUfunclike,,5,[],0,"['test_isposinf', 'test_isneginf', 'test_fix', 'test_fix_with_subclass', 'test_scalar']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test_utils.py,TestByteBounds,,4,[],0,"['test_byte_bounds', 'test_unusual_order_positive_stride', 'test_unusual_order_negative_stride', 'test_strided']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__datasource.py,TestDataSourceOpen,,9,[],0,"['setup_method', 'teardown_method', 'test_ValidHTTP', 'test_InvalidHTTP', 'test_InvalidHTTPCacheURLError', 'test_ValidFile', 'test_InvalidFile', 'test_ValidGzipFile', 'test_ValidBz2File']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__datasource.py,TestDataSourceExists,,6,[],0,"['setup_method', 'teardown_method', 'test_ValidHTTP', 'test_InvalidHTTP', 'test_ValidFile', 'test_InvalidFile']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__datasource.py,TestDataSourceAbspath,,8,[],0,"['setup_method', 'teardown_method', 'test_ValidHTTP', 'test_ValidFile', 'test_InvalidHTTP', 'test_InvalidFile', 'test_sandboxing', 'test_windows_os_sep']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__datasource.py,TestRepositoryAbspath,,5,[],0,"['setup_method', 'teardown_method', 'test_ValidHTTP', 'test_sandboxing', 'test_windows_os_sep']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__datasource.py,TestRepositoryExists,,6,[],0,"['setup_method', 'teardown_method', 'test_ValidFile', 'test_InvalidFile', 'test_RemoveHTTPFile', 'test_CachedHTTPFile']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__datasource.py,TestOpenFunc,,3,[],0,"['setup_method', 'teardown_method', 'test_DataSourceOpen']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__iotools.py,TestLineSplitter,Tests the LineSplitter class.,6,[],0,"['test_no_delimiter', 'test_space_delimiter', 'test_tab_delimiter', 'test_other_delimiter', 'test_constant_fixed_width', 'test_variable_fixed_width']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__iotools.py,TestNameValidator,,5,[],0,"['test_case_sensitivity', 'test_excludelist', 'test_missing_names', 'test_validate_nb_names', 'test_validate_wo_names']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__iotools.py,TestStringConverter,Test StringConverter,10,[],0,"['test_creation', 'test_upgrade', 'test_missing', 'test_upgrademapper', 'test_string_to_object', 'test_keep_default', 'test_keep_default_zero', 'test_keep_missing_values', 'test_int64_dtype', 'test_uint64_dtype']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\lib\tests\test__iotools.py,TestMiscFunctions,,3,[],0,"['test_has_nested_dtype', 'test_easy_dtype', 'test_flatten_dtype']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\linalg\tests\test_regression.py,TestRegression,,9,[],0,"['test_eig_build', 'test_eigh_build', 'test_svd_build', 'test_norm_vector_badarg', 'test_lapack_endian', 'test_large_svd_32bit', 'test_svd_no_uv', 'test_norm_object_array', 'test_lstsq_complex_larger_rhs']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\ma\tests\test_deprecations.py,TestArgsort,gh-8701 ,4,[],0,"['_test_base', 'test_function_ndarray', 'test_function_maskedarray', 'test_method']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\ma\tests\test_deprecations.py,TestMinimumMaximum,,1,[],0,['test_axis_default'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\ma\tests\test_deprecations.py,TestFromtextfile,,1,[],0,['test_fromtextfile_delimitor'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\ma\tests\test_regression.py,TestRegression,,14,[],0,"['test_masked_array_create', 'test_masked_array', 'test_mem_masked_where', 'test_masked_array_multiply', 'test_masked_array_repeat', 'test_masked_array_repr_unicode', 'test_atleast_2d', 'test_set_fill_value_unicode_py3', 'test_var_sets_maskedarray_scalar', 'test_ddof_corrcoef', 'test_mask_not_backmangled', 'test_empty_list_on_structured', 'test_masked_array_tobytes_fortran', 'test_structured_array']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\matrixlib\tests\test_interaction.py,TestConcatenatorMatrix,,3,[],0,"['test_matrix', 'test_matrix_scalar', 'test_matrix_builder']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\matrixlib\tests\test_multiarray.py,TestView,,2,[],0,"['test_type', 'test_keywords']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\matrixlib\tests\test_numeric.py,TestDot,,1,[],0,['test_matscalar'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\matrixlib\tests\test_regression.py,TestRegression,,4,[],0,"['test_kron_matrix', 'test_matrix_properties', 'test_matrix_multiply_by_1d_vector', 'test_matrix_std_argmax']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\polynomial\tests\test_classes.py,TestInterpolate,,4,[],0,"['f', 'test_raises', 'test_dimensions', 'test_approximation']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\polynomial\tests\test_polyutils.py,TestMisc,,6,[],0,"['test_trimseq', 'test_as_series', 'test_trimcoef', 'test_vander_nd_exception', 'test_div_zerodiv', 'test_pow_too_large']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\polynomial\tests\test_polyutils.py,TestDomain,,3,[],0,"['test_getdomain', 'test_mapdomain', 'test_mapparms']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\random\tests\test_generator_mt19937_regressions.py,TestRegression,,16,[],0,"['setup_method', 'test_vonmises_range', 'test_hypergeometric_range', 'test_logseries_convergence', 'test_shuffle_mixed_dimension', 'test_call_within_randomstate', 'test_multivariate_normal_size_types', 'test_beta_small_parameters', 'test_beta_very_small_parameters', 'test_beta_ridiculously_small_parameters', 'test_choice_sum_of_probs_tolerance', 'test_shuffle_of_array_of_different_length_strings', 'test_shuffle_of_array_of_objects', 'test_permutation_subclass', 'test_gamma_0', 'test_geometric_tiny_prob']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\random\tests\test_regression.py,TestRegression,,11,[],0,"['test_VonMises_range', 'test_hypergeometric_range', 'test_logseries_convergence', 'test_shuffle_mixed_dimension', 'test_call_within_randomstate', 'test_multivariate_normal_size_types', 'test_beta_small_parameters', 'test_choice_sum_of_probs_tolerance', 'test_shuffle_of_array_of_different_length_strings', 'test_shuffle_of_array_of_objects', 'test_permutation_subclass']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\test_isfile.py,TestIsFile,,1,[],0,['test_isfile'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\test_runtime.py,TypeTup,,0,['NamedTuple'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\test_runtime.py,TestRuntimeProtocol,,2,[],0,"['test_isinstance', 'test_issubclass']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\arithmetic.py,Object,,9,[],0,"['__array__', '__sub__', '__rsub__', '__floordiv__', '__rfloordiv__', '__mul__', '__rmul__', '__pow__', '__rpow__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\array_constructors.py,Index,,1,[],0,['__index__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\array_constructors.py,SubClass,,0,['np.ndarray'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\array_like.py,A,,1,[],0,['__array__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\ndarray_misc.py,SubClass,,0,['np.ndarray'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\numeric.py,SubClass,,0,['np.ndarray'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\scalars.py,D,,1,[],0,['__index__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\scalars.py,C,,1,[],0,['__complex__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\scalars.py,B,,1,[],0,['__int__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\scalars.py,A,,1,[],0,['__float__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\ufunclike.py,Object,,4,[],0,"['__ceil__', '__floor__', '__ge__', '__array__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\ufunc_config.py,Write1,,1,[],0,['write'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\ufunc_config.py,Write2,,1,[],0,['write'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\numpy\typing\tests\data\pass\ufunc_config.py,Write3,,1,[],0,['write'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pbr\tests\base.py,DiveDir,"Dive into given directory and return back on cleanup.

:ivar path: The target directory.",2,['fixtures.Fixture'],0,"['__init__', 'setUp']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pbr\tests\base.py,BaseTestCase,,5,"['testtools.TestCase', 'testresources.ResourcedTestCase']",0,"['setUp', '_discard_testpackage', 'run_pbr', 'run_setup', '_run_cmd']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pbr\tests\base.py,CapturedSubprocess,"Run a process and capture its output.

:attr stdout: The output (a string).
:attr stderr: The standard error (a string).
:attr returncode: The return code of the process.

Note that stdout and stderr are decoded from the bytestrings subprocess
returns using error=replace.",2,['fixtures.Fixture'],0,"['__init__', 'setUp']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pbr\tests\test_commands.py,TestCommands,,4,['base.BaseTestCase'],0,"['test_custom_build_py_command', 'test_custom_deb_version_py_command', 'test_custom_rpm_version_py_command', 'test_freeze_command']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pbr\tests\test_files.py,FilesConfigTest,,8,['base.BaseTestCase'],0,"['setUp', 'test_implicit_auto_package', 'test_auto_package', 'test_data_files_globbing', 'test_data_files_with_spaces', 'test_data_files_with_spaces_subdirectories', 'test_data_files_with_spaces_quoted_components', 'test_data_files_globbing_source_prefix_in_directory_name']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pbr\tests\test_version.py,TestSemanticVersion,,31,['base.BaseTestCase'],0,"['test_ordering', 'test_from_pip_string_legacy_alpha', 'test_from_pip_string_legacy_postN', 'test_from_pip_string_v_version', 'test_from_pip_string_legacy_nonzero_lead_in', 'test_from_pip_string_legacy_short_nonzero_lead_in', 'test_from_pip_string_legacy_no_0_prerelease', 'test_from_pip_string_legacy_no_0_prerelease_2', 'test_from_pip_string_legacy_non_440_beta', 'test_from_pip_string_pure_git_hash', 'test_from_pip_string_non_digit_start', 'test_final_version', 'test_parsing_short_forms', 'test_dev_version', 'test_dev_no_git_version', 'test_dev_zero_version', 'test_alpha_dev_version', 'test_alpha_version', 'test_alpha_zero_version', 'test_alpha_major_zero_version', 'test_alpha_default_version', 'test_beta_dev_version', 'test_beta_version', 'test_decrement_nonrelease', 'test_decrement_nonrelease_zero', 'test_decrement_release', 'test_increment_nonrelease', 'test_increment_release', 'test_rc_dev_version', 'test_rc_version', 'test_to_dev']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\cli\base_command.py,Command,,7,['CommandContextMixIn'],2,"['__init__', 'add_options', 'handle_pip_version_check', 'run', 'parse_args', 'main', '_main']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\cli\spinners.py,SpinnerInterface,,2,[],0,"['spin', 'finish']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\cli\spinners.py,InteractiveSpinner,,4,['SpinnerInterface'],0,"['__init__', '_write', 'spin', 'finish']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\cli\spinners.py,NonInteractiveSpinner,,4,['SpinnerInterface'],0,"['__init__', '_update', 'spin', 'finish']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\cli\spinners.py,RateLimiter,,3,[],0,"['__init__', 'ready', 'reset']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\metadata\__init__.py,Backend,,0,['Protocol'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\cache.py,SafeFileCache,"A file based cache which is safe to use even when the target directory may
not be accessible or writable.

There is a race condition when two processes try to write and/or read the
same entry at the same time, since each entry consists of two separate
files (https://github.com/psf/cachecontrol/issues/324).  We therefore have
additional logic that makes sure that both files to be present before
returning an entry; this fixes the read side of the race condition.

For the write side, we assume that the server will only ever return the
same data for the same URL, which ought to be the case for files pip is
downloading.  PyPI does not have a mechanism to swap out a wheel for
another wheel, for example.  If this assumption is not true, the
CacheControl issue will need to be fixed.",8,['SeparateBodyBaseCache'],0,"['__init__', '_get_cache_path', 'get', '_write', 'set', 'delete', 'get_body', 'set_body']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\download.py,Downloader,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\download.py,BatchDownloader,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\session.py,LocalFSAdapter,,2,['BaseAdapter'],0,"['send', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\session.py,_SSLContextAdapterMixin,"Mixin to add the ``ssl_context`` constructor argument to HTTP adapters.

The additional argument is forwarded directly to the pool manager. This allows us
to dynamically decide what SSL store to use at runtime, which is used to implement
the optional ``truststore`` backend.",2,[],0,"['__init__', 'init_poolmanager']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\session.py,HTTPAdapter,,0,"['_SSLContextAdapterMixin', '_BaseHTTPAdapter']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\session.py,CacheControlAdapter,,0,"['_SSLContextAdapterMixin', '_BaseCacheControlAdapter']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\session.py,InsecureHTTPAdapter,,1,['HTTPAdapter'],0,['cert_verify'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\session.py,InsecureCacheControlAdapter,,1,['CacheControlAdapter'],0,['cert_verify'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\session.py,PipSession,,6,['requests.Session'],1,"['__init__', 'update_index_urls', 'add_trusted_host', 'iter_secure_origins', 'is_secure_origin', 'request']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\network\xmlrpc.py,PipXmlrpcTransport,"Provide a `xmlrpclib.Transport` implementation via a `PipSession`
object.",2,['xmlrpc.client.Transport'],0,"['__init__', 'request']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\operations\check.py,PackageDetails,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\operations\prepare.py,File,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\operations\prepare.py,RequirementPreparer,Prepares a Requirement,14,[],0,"['__init__', '_log_preparing_link', '_ensure_link_req_src_dir', '_get_linked_req_hashes', '_fetch_metadata_only', '_fetch_metadata_using_link_data_attr', '_fetch_metadata_using_lazy_wheel', '_complete_partial_requirements', 'prepare_linked_requirement', 'prepare_linked_requirements_more', '_prepare_linked_requirement', 'save_linked_requirement', 'prepare_editable_requirement', 'prepare_installed_requirement']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\req\constructors.py,RequirementParts,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\req\req_file.py,ParsedRequirement,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\req\req_file.py,ParsedLine,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\req\req_file.py,RequirementsFileParser,,4,[],0,"['__init__', 'parse', '_parse_and_recurse', '_parse_file']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\req\req_file.py,OptionParsingError,,1,['Exception'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\req\__init__.py,InstallationResult,,2,[],0,"['__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\resolution\base.py,BaseResolver,,2,[],0,"['resolve', 'get_installation_order']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\utils\deprecation.py,PipDeprecationWarning,,0,['Warning'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\utils\_log.py,VerboseLogger,"Custom Logger, defining a verbose log-level

VERBOSE is between INFO and DEBUG.",1,['logging.Logger'],0,['verbose'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\operations\install\wheel.py,ZipBackedFile,,3,[],0,"['__init__', '_getinfo', 'save']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\operations\install\wheel.py,ScriptFile,,2,[],0,"['__init__', 'save']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\operations\install\wheel.py,MissingCallableSuffix,,1,['InstallationError'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\operations\install\wheel.py,PipScriptMaker,,1,['ScriptMaker'],0,['make'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\resolution\resolvelib\reporter.py,PipReporter,,2,['BaseReporter'],0,"['__init__', 'rejecting_candidate']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_internal\resolution\resolvelib\reporter.py,PipDebuggingReporter,A reporter that does an info log for every event it sees.,7,['BaseReporter'],0,"['starting', 'starting_round', 'ending_round', 'ending', 'adding_requirement', 'rejecting_candidate', 'pinning']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\cachecontrol\cache.py,BaseCache,,4,[],0,"['get', 'set', 'delete', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\cachecontrol\cache.py,DictCache,,4,['BaseCache'],0,"['__init__', 'get', 'set', 'delete']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\cachecontrol\cache.py,SeparateBodyBaseCache,"In this variant, the body is not stored mixed in with the metadata, but is
passed in (as a bytes-like object) in a separate call to ``set_body()``.

That is, the expected interaction pattern is::

    cache.set(key, serialized_metadata)
    cache.set_body(key)

Similarly, the body should be loaded separately via ``get_body()``.",2,['BaseCache'],0,"['set_body', 'get_body']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\cachecontrol\filewrapper.py,CallbackFileWrapper,"Small wrapper around a fp object which will tee everything read into a
buffer, and when that file is closed it will execute a callback with the
contents of that buffer.

All attributes are proxied to the underlying file object.

This class uses members with a double underscore (__) leading prefix so as
not to accidentally shadow an attribute.

The data is stored in a temporary file until it is all available.  As long
as the temporary files directory is disk-based (sometimes it's a
memory-backed-``tmpfs`` on Linux), data will be unloaded to disk if memory
pressure is high.  For small files the disk usually won't be used at all,
it'll all be in the filesystem memory cache, so there should be no
performance impact.",6,[],0,"['__init__', '__getattr__', '__is_fp_closed', '_close', 'read', '_safe_read']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\chardet\mbcharsetprober.py,MultiByteCharSetProber,MultiByteCharSetProber,4,['CharSetProber'],0,"['__init__', 'reset', 'feed', 'get_confidence']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\chardet\mbcsgroupprober.py,MBCSGroupProber,,1,['CharSetGroupProber'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\chardet\sbcsgroupprober.py,SBCSGroupProber,,1,['CharSetGroupProber'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\distlib\manifest.py,Manifest,"A list of files built by exploring the filesystem and filtered by applying various
patterns to what we find there.",12,['object'],0,"['__init__', 'findall', 'add', 'add_many', 'sorted', 'clear', 'process_directive', '_parse_directive', '_include_pattern', '_exclude_pattern', '_translate_pattern', '_glob_to_re']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\distlib\__init__.py,DistlibException,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\codec.py,Codec,,2,['codecs.Codec'],0,"['encode', 'decode']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\codec.py,IncrementalEncoder,,1,['codecs.BufferedIncrementalEncoder'],0,['_buffer_encode'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\codec.py,IncrementalDecoder,,1,['codecs.BufferedIncrementalDecoder'],0,['_buffer_decode'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\codec.py,StreamWriter,,0,"['Codec', 'codecs.StreamWriter']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\codec.py,StreamReader,,0,"['Codec', 'codecs.StreamReader']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\core.py,IDNAError,Base exception for all IDNA-encoding related problems ,0,['UnicodeError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\core.py,IDNABidiError,Exception when bidirectional requirements are not satisfied ,0,['IDNAError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\core.py,InvalidCodepoint,Exception when a disallowed or unallocated codepoint is used ,0,['IDNAError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\idna\core.py,InvalidCodepointContext,Exception when the codepoint is not valid in the context it is used ,0,['IDNAError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\msgpack\exceptions.py,UnpackException,"Base class for some exceptions raised while unpacking.

NOTE: unpack may raise exception other than subclass of
UnpackException.  If you want to catch all error, catch
Exception instead.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\msgpack\exceptions.py,BufferFull,,0,['UnpackException'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\msgpack\exceptions.py,OutOfData,,0,['UnpackException'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\msgpack\exceptions.py,FormatError,Invalid msgpack format,0,"['ValueError', 'UnpackException']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\msgpack\exceptions.py,StackError,Too nested,0,"['ValueError', 'UnpackException']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\msgpack\exceptions.py,ExtraData,"ExtraData is raised when there is trailing data.

This exception is raised while only one-shot (not streaming)
unpack.",2,['UnpackValueError'],0,"['__init__', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,InvalidMarker,"An invalid marker was found, users should refer to PEP 508.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,UndefinedComparison,An invalid operation was attempted on a value that doesn't support it.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,UndefinedEnvironmentName,"A name was attempted to be used that does not exist inside of the
environment.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,Node,,4,[],0,"['__init__', '__str__', '__repr__', 'serialize']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,Variable,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,Value,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,Op,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,Undefined,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\markers.py,Marker,,4,[],0,"['__init__', '__str__', '__repr__', 'evaluate']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\requirements.py,InvalidRequirement,"An invalid requirement was found, users should refer to PEP 508.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\requirements.py,Requirement,"Parse a requirement.

Parse a given requirement string into its parts, such as name, specifier,
URL, and extras. Raises InvalidRequirement on a badly-formed requirement
string.",3,[],0,"['__init__', '__str__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\utils.py,InvalidWheelFilename,"An invalid wheel filename was found, users should refer to PEP 427.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\utils.py,InvalidSdistFilename,"An invalid sdist filename was found, users should refer to the packaging user guide.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\_musllinux.py,_MuslVersion,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\_structures.py,InfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\packaging\_structures.py,NegativeInfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pygments\cmdline.py,HelpFormatter,,1,['argparse.HelpFormatter'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pygments\util.py,ClassNotFound,Raised if one of the lookup functions didn't find a matching class.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pygments\util.py,OptionError,"This exception will be raised by all option processing functions if
the type or value of the argument is not correct.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pygments\util.py,Future,"Generic class to defer some work.

Handled specially in RegexLexerMeta, to support regex string construction at
first use.",1,[],0,['get'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pygments\util.py,UnclosingTextIOWrapper,,1,['TextIOWrapper'],0,['close'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pyparsing\actions.py,OnlyOnce,"Wrapper for parse actions, to ensure they are only called once.",3,[],0,"['__init__', '__call__', 'reset']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\auth.py,AuthBase,Base class that all auth implementations derive from,1,[],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\auth.py,HTTPBasicAuth,Attaches HTTP Basic Authentication to the given Request object.,4,['AuthBase'],0,"['__init__', '__eq__', '__ne__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\auth.py,HTTPProxyAuth,Attaches HTTP Proxy Authentication to a given Request object.,1,['HTTPBasicAuth'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\auth.py,HTTPDigestAuth,Attaches HTTP Digest Authentication to the given Request object.,8,['AuthBase'],0,"['__init__', 'init_per_thread_state', 'build_digest_header', 'handle_redirect', 'handle_401', '__call__', '__eq__', '__ne__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,RequestException,"There was an ambiguous exception that occurred while handling your
request.",1,['IOError'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,InvalidJSONError,A JSON error occurred.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,JSONDecodeError,Couldn't decode the text into json,1,"['InvalidJSONError', 'CompatJSONDecodeError']",0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,HTTPError,An HTTP error occurred.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,ConnectionError,A Connection error occurred.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,ProxyError,A proxy error occurred.,0,['ConnectionError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,SSLError,An SSL error occurred.,0,['ConnectionError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,Timeout,"The request timed out.

Catching this error will catch both
:exc:`~requests.exceptions.ConnectTimeout` and
:exc:`~requests.exceptions.ReadTimeout` errors.",0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,ConnectTimeout,"The request timed out while trying to connect to the remote server.

Requests that produced this error are safe to retry.",0,"['ConnectionError', 'Timeout']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,ReadTimeout,The server did not send any data in the allotted amount of time.,0,['Timeout'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,URLRequired,A valid URL is required to make a request.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,TooManyRedirects,Too many redirects.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,MissingSchema,The URL scheme (e.g. http or https) is missing.,0,"['RequestException', 'ValueError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,InvalidSchema,The URL scheme provided is either invalid or unsupported.,0,"['RequestException', 'ValueError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,InvalidURL,The URL provided was somehow invalid.,0,"['RequestException', 'ValueError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,InvalidHeader,The header value provided was somehow invalid.,0,"['RequestException', 'ValueError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,InvalidProxyURL,The proxy URL provided is invalid.,0,['InvalidURL'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,ChunkedEncodingError,The server declared chunked encoding but sent an invalid chunk.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,ContentDecodingError,Failed to decode response content.,0,"['RequestException', 'BaseHTTPError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,StreamConsumedError,The content for this response was already consumed.,0,"['RequestException', 'TypeError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,RetryError,Custom retries logic failed,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,UnrewindableBodyError,Requests encountered an error when trying to rewind a body.,0,['RequestException'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,RequestsWarning,Base warning for Requests.,0,['Warning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,FileModeWarning,"A file was opened in text mode, but Requests determined its binary length.",0,"['RequestsWarning', 'DeprecationWarning']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\exceptions.py,RequestsDependencyWarning,An imported dependency doesn't match the expected version range.,0,['RequestsWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\structures.py,CaseInsensitiveDict,"A case-insensitive ``dict``-like object.

Implements all methods and operations of
``MutableMapping`` as well as dict's ``copy``. Also
provides ``lower_items``.

All keys are expected to be strings. The structure remembers the
case of the last key to be set, and ``iter(instance)``,
``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``
will contain case-sensitive keys. However, querying and contains
testing is case insensitive::

    cid = CaseInsensitiveDict()
    cid['Accept'] = 'application/json'
    cid['aCCEPT'] == 'application/json'  # True
    list(cid) == ['Accept']  # True

For example, ``headers['content-encoding']`` will return the
value of a ``'Content-Encoding'`` response header, regardless
of how the header name was originally stored.

If the constructor, ``.update``, or equality comparison
operations are given keys that have equal ``.lower()``s, the
behavior is undefined.",10,['MutableMapping'],0,"['__init__', '__setitem__', '__getitem__', '__delitem__', '__iter__', '__len__', 'lower_items', '__eq__', 'copy', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\requests\structures.py,LookupDict,Dictionary lookup object.,4,['dict'],0,"['__init__', '__repr__', '__getitem__', 'get']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\resolvelib\reporters.py,BaseReporter,Delegate class to provider progress reporting for the resolver.,8,['object'],0,"['starting', 'starting_round', 'ending_round', 'ending', 'adding_requirement', 'resolving_conflicts', 'rejecting_candidate', 'pinning']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\ansi.py,_AnsiToken,Result of ansi tokenized string.,0,['NamedTuple'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\ansi.py,AnsiDecoder,Translate ANSI code in to styled Text.,3,[],0,"['__init__', 'decode', 'decode_line']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\bar.py,Bar,"Renders a solid block bar.

Args:
    size (float): Value for the end of the bar.
    begin (float): Begin point (between 0 and size, inclusive).
    end (float): End point (between 0 and size, inclusive).
    width (int, optional): Width of the bar, or ``None`` for maximum width. Defaults to None.
    color (Union[Color, str], optional): Color of the bar. Defaults to ""default"".
    bgcolor (Union[Color, str], optional): Color of bar background. Defaults to ""default"".",4,['JupyterMixin'],0,"['__init__', '__repr__', '__rich_console__', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\box.py,Box,"Defines characters to render boxes.

┌─┬┐ top
│ ││ head
├─┼┤ head_row
│ ││ mid
├─┼┤ row
├─┼┤ foot_row
│ ││ foot
└─┴┘ bottom

Args:
    box (str): Characters making up box.
    ascii (bool, optional): True if this box uses ascii characters only. Default is False.",8,[],0,"['__init__', '__repr__', '__str__', 'substitute', 'get_plain_headed_box', 'get_top', 'get_row', 'get_bottom']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\columns.py,Columns,"Display renderables in neat columns.

Args:
    renderables (Iterable[RenderableType]): Any number of Rich renderables (including str).
    width (int, optional): The desired width of the columns, or None to auto detect. Defaults to None.
    padding (PaddingDimensions, optional): Optional padding around cells. Defaults to (0, 1).
    expand (bool, optional): Expand columns to full width. Defaults to False.
    equal (bool, optional): Arrange in to equal sized columns. Defaults to False.
    column_first (bool, optional): Align items from top to bottom (rather than left to right). Defaults to False.
    right_to_left (bool, optional): Start column from right hand side. Defaults to False.
    align (str, optional): Align value (""left"", ""right"", or ""center"") or None for default. Defaults to None.
    title (TextType, optional): Optional title for Columns.",3,['JupyterMixin'],0,"['__init__', 'add_renderable', '__rich_console__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\constrain.py,Constrain,"Constrain the width of a renderable to a given number of characters.

Args:
    renderable (RenderableType): A renderable object.
    width (int, optional): The maximum width (in characters) to render. Defaults to 80.",3,['JupyterMixin'],0,"['__init__', '__rich_console__', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,ConsoleError,An error in console operation.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,StyleError,An error in styles.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,StyleSyntaxError,Style was badly formatted.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,MissingStyle,No such style.,0,['StyleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,StyleStackError,Style stack is invalid.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,NotRenderableError,Object is not renderable.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,MarkupError,Markup was badly formatted.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,LiveError,Error related to Live display.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\errors.py,NoAltScreen,Alt screen mode was required.,0,['ConsoleError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\live_render.py,LiveRender,"Creates a renderable that may be updated.

Args:
    renderable (RenderableType): Any renderable object.
    style (StyleType, optional): An optional style to apply to the renderable. Defaults to """".",5,[],0,"['__init__', 'set_renderable', 'position_cursor', 'restore_cursor', '__rich_console__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\logging.py,RichHandler,"A logging handler that renders output with Rich. The time / level / message and file are displayed in columns.
The level is color coded, and the message is syntax highlighted.

Note:
    Be careful when enabling console markup in log messages if you have configured logging for libraries not
    under your control. If a dependency writes messages containing square brackets, it may not produce the intended output.

Args:
    level (Union[int, str], optional): Log level. Defaults to logging.NOTSET.
    console (:class:`~rich.console.Console`, optional): Optional console instance to write logs.
        Default will use a global console instance writing to stdout.
    show_time (bool, optional): Show a column for the time. Defaults to True.
    omit_repeated_times (bool, optional): Omit repetition of the same time. Defaults to True.
    show_level (bool, optional): Show a column for the level. Defaults to True.
    show_path (bool, optional): Show the path to the original log call. Defaults to True.
    enable_link_path (bool, optional): Enable terminal link of path column to file. Defaults to True.
    highlighter (Highlighter, optional): Highlighter to style log messages, or None to use ReprHighlighter. Defaults to None.
    markup (bool, optional): Enable console markup in log messages. Defaults to False.
    rich_tracebacks (bool, optional): Enable rich tracebacks with syntax highlighting and formatting. Defaults to False.
    tracebacks_width (Optional[int], optional): Number of characters used to render tracebacks, or None for full width. Defaults to None.
    tracebacks_extra_lines (int, optional): Additional lines of code to render tracebacks, or None for full width. Defaults to None.
    tracebacks_theme (str, optional): Override pygments theme used in traceback.
    tracebacks_word_wrap (bool, optional): Enable word wrapping of long tracebacks lines. Defaults to True.
    tracebacks_show_locals (bool, optional): Enable display of locals in tracebacks. Defaults to False.
    tracebacks_suppress (Sequence[Union[str, ModuleType]]): Optional sequence of modules or paths to exclude from traceback.
    locals_max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.
        Defaults to 10.
    locals_max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to 80.
    log_time_format (Union[str, TimeFormatterCallable], optional): If ``log_time`` is enabled, either string for strftime or callable that formats the time. Defaults to ""[%x %X] "".
    keywords (List[str], optional): List of words to highlight instead of ``RichHandler.KEYWORDS``.",5,['Handler'],2,"['__init__', 'get_level_text', 'emit', 'render_message', 'render']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\region.py,Region,Defines a rectangular region of the screen.,0,['NamedTuple'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\repr.py,ReprError,An error occurred when attempting to build a repr.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\rule.py,Rule,"A console renderable to draw a horizontal rule (line).

Args:
    title (Union[str, Text], optional): Text to render in the rule. Defaults to """".
    characters (str, optional): Character(s) used to draw the line. Defaults to ""─"".
    style (StyleType, optional): Style of Rule. Defaults to ""rule.line"".
    end (str, optional): Character at end of Rule. defaults to ""\\n""
    align (str, optional): How to align the title, one of ""left"", ""center"", or ""right"". Defaults to ""center"".",5,['JupyterMixin'],0,"['__init__', '__repr__', '__rich_console__', '_rule_line', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\screen.py,Screen,"A renderable that fills the terminal screen and crops excess.

Args:
    renderable (RenderableType): Child renderable.
    style (StyleType, optional): Optional background style. Defaults to None.",2,[],1,"['__init__', '__rich_console__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\spinner.py,Spinner,"A spinner animation.

Args:
    name (str): Name of spinner (run python -m rich.spinner).
    text (RenderableType, optional): A renderable to display at the right of the spinner (str or Text typically). Defaults to """".
    style (StyleType, optional): Style for spinner animation. Defaults to None.
    speed (float, optional): Speed factor for animation. Defaults to 1.0.

Raises:
    KeyError: If name isn't one of the supported spinner animations.",5,[],0,"['__init__', '__rich_console__', '__rich_measure__', 'render', 'update']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\styled.py,Styled,"Apply a style to a renderable.

Args:
    renderable (RenderableType): Any renderable.
    style (StyleType): A style to apply across the entire renderable.",3,[],0,"['__init__', '__rich_console__', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\terminal_theme.py,TerminalTheme,"A color theme used when exporting console content.

Args:
    background (Tuple[int, int, int]): The background color.
    foreground (Tuple[int, int, int]): The foreground (text) color.
    normal (List[Tuple[int, int, int]]): A list of 8 normal intensity colors.
    bright (List[Tuple[int, int, int]], optional): A list of 8 bright colors, or None
        to repeat normal intensity. Defaults to None.",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\tree.py,Tree,"A renderable for a tree structure.

Args:
    label (RenderableType): The renderable or str for the tree label.
    style (StyleType, optional): Style of this tree. Defaults to ""tree"".
    guide_style (StyleType, optional): Style of the guide lines. Defaults to ""tree.line"".
    expanded (bool, optional): Also display children. Defaults to True.
    highlight (bool, optional): Highlight renderable (if str). Defaults to False.",4,['JupyterMixin'],0,"['__init__', 'add', '__rich_console__', '__rich_measure__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\_inspect.py,Inspect,"A renderable to inspect any Python Object.

Args:
    obj (Any): An object to inspect.
    title (str, optional): Title to display over inspect result, or None use type. Defaults to None.
    help (bool, optional): Show full help text rather than just first paragraph. Defaults to False.
    methods (bool, optional): Enable inspection of callables. Defaults to False.
    docs (bool, optional): Also render doc strings. Defaults to True.
    private (bool, optional): Show private attributes (beginning with underscore). Defaults to False.
    dunder (bool, optional): Show attributes starting with double underscore. Defaults to False.
    sort (bool, optional): Sort attributes alphabetically. Defaults to True.
    all (bool, optional): Show all attributes. Defaults to False.
    value (bool, optional): Pretty print value of object. Defaults to True.",6,['JupyterMixin'],0,"['__init__', '_make_title', '__rich__', '_get_signature', '_render', '_get_formatted_doc']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\_log_render.py,LogRender,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\_null_file.py,NullFile,,19,['IO[str]'],0,"['close', 'isatty', 'read', 'readable', 'readline', 'readlines', 'seek', 'seekable', 'tell', 'truncate', 'writable', 'writelines', '__next__', '__iter__', '__enter__', '__exit__', 'write', 'flush', 'fileno']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\_ratio.py,Edge,Any object that defines an edge (such as Layout).,0,['Protocol'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\_windows.py,WindowsConsoleFeatures,Windows features available.,0,[],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\rich\__main__.py,ColorBox,,2,[],0,"['__rich_console__', '__rich_measure__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\tenacity\nap.py,sleep_using_event,Sleep strategy that waits on an event to be set.,2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\tenacity\_asyncio.py,AsyncRetrying,,4,['BaseRetrying'],1,"['__init__', '__iter__', '__aiter__', 'wraps']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\webencodings\x_user_defined.py,Codec,,2,['codecs.Codec'],0,"['encode', 'decode']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\webencodings\x_user_defined.py,IncrementalEncoder,,1,['codecs.IncrementalEncoder'],0,['encode'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\webencodings\x_user_defined.py,IncrementalDecoder,,1,['codecs.IncrementalDecoder'],0,['decode'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\webencodings\x_user_defined.py,StreamWriter,,0,"['Codec', 'codecs.StreamWriter']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\webencodings\x_user_defined.py,StreamReader,,0,"['Codec', 'codecs.StreamReader']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\webencodings\__init__.py,Encoding,"Reresents a character encoding such as UTF-8,
that can be used for decoding or encoding.

.. attribute:: name

    Canonical name of the encoding

.. attribute:: codec_info

    The actual implementation of the encoding,
    a stdlib :class:`~codecs.CodecInfo` object.
    See :func:`codecs.register`.",2,['object'],0,"['__init__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\webencodings\__init__.py,IncrementalDecoder,"“Push”-based decoder.

:param fallback_encoding:
    An :class:`Encoding` object or a label string.
    The encoding to use if :obj:`input` does note have a BOM.
:param errors: Type of error handling. See :func:`codecs.register`.
:raises: :exc:`~exceptions.LookupError` for an unknown encoding label.",2,['object'],0,"['__init__', 'decode']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\webencodings\__init__.py,IncrementalEncoder,"“Push”-based encoder.

:param encoding: An :class:`Encoding` object or a label string.
:param errors: Type of error handling. See :func:`codecs.register`.
:raises: :exc:`~exceptions.LookupError` for an unknown encoding label.

.. method:: encode(input, final=False)

    :param input: An Unicode string.
    :param final:
        Indicate that no more input is available.
        Must be :obj:`True` if this is the last call.
    :returns: A byte string.",1,['object'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\cachecontrol\caches\redis_cache.py,RedisCache,,6,['BaseCache'],0,"['__init__', 'get', 'set', 'delete', 'clear', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\chardet\metadata\languages.py,Language,"Metadata about a language useful for training models

:ivar name: The human name for the language, in English.
:type name: str
:ivar iso_code: 2-letter ISO 639-1 if possible, 3-letter ISO code otherwise,
                or use another catalog as a last resort.
:type iso_code: str
:ivar use_ascii: Whether or not ASCII letters should be included in trained
                 models.
:type use_ascii: bool
:ivar charsets: The charsets we want to support and create data for.
:type charsets: list of str
:ivar alphabet: The characters in the language's alphabet. If `use_ascii` is
                `True`, you only need to add those not in the ASCII set.
:type alphabet: str
:ivar wiki_start_pages: The Wikipedia pages to start from if we're crawling
                        Wikipedia for training data.
:type wiki_start_pages: list of str",2,[],0,"['__init__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\colorama\tests\ansi_test.py,AnsiTest,,5,['TestCase'],0,"['setUp', 'tearDown', 'testForeAttributes', 'testBackAttributes', 'testStyleAttributes']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\colorama\tests\isatty_test.py,IsattyTest,,7,['TestCase'],0,"['test_TTY', 'test_nonTTY', 'test_withPycharm', 'test_withPycharmTTYOverride', 'test_withPycharmNonTTYOverride', 'test_withPycharmNoneOverride', 'test_withPycharmStreamWrapped']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\colorama\tests\utils.py,StreamTTY,,1,['StringIO'],0,['isatty'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\colorama\tests\utils.py,StreamNonTTY,,1,['StringIO'],0,['isatty'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pygments\formatters\__init__.py,_automodule,Automatically import formatters.,1,['types.ModuleType'],0,['__getattr__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pygments\lexers\__init__.py,_automodule,Automatically import lexers.,1,['types.ModuleType'],0,['__getattr__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py,BackendUnavailable,Raised if we cannot import the backend,1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py,BackendInvalid,Raised if the backend is invalid,1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py,HookMissing,Raised if a hook is missing and we are not executing the fallback,1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py,_DummyException,Nothing should ever raise this exception,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py,GotUnsupportedOperation,For internal use when backend raises UnsupportedOperation,1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\urllib3\contrib\appengine.py,AppEnginePlatformWarning,,0,['HTTPWarning'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\urllib3\contrib\appengine.py,AppEnginePlatformError,,0,['HTTPError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\urllib3\contrib\appengine.py,AppEngineManager,"Connection manager for Google App Engine sandbox applications.

This manager uses the URLFetch service directly instead of using the
emulated httplib, and is subject to URLFetch limitations as described in
the App Engine documentation `here
<https://cloud.google.com/appengine/docs/python/urlfetch>`_.

Notably it will raise an :class:`AppEnginePlatformError` if:
    * URLFetch is not available.
    * If you attempt to use this on App Engine Flexible, as full socket
      support is available.
    * If a request size is more than 10 megabytes.
    * If a response size is more than 32 megabytes.
    * If you use an unsupported request method such as OPTIONS.

Beyond those cases, it will raise normal urllib3 errors.",7,['RequestMethods'],0,"['__init__', '__enter__', '__exit__', 'urlopen', '_urlfetch_response_to_http_response', '_get_absolute_timeout', '_get_retries']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\urllib3\util\queue.py,LifoQueue,,4,['queue.Queue'],0,"['_init', '_qsize', '_put', '_get']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\urllib3\util\ssl_match_hostname.py,CertificateError,,0,['ValueError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pip\_vendor\urllib3\util\wait.py,NoWayToWaitForSocketError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pkg_resources\tests\test_working_set.py,FakeInstaller,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\deprecated\config.py,_ConfigMetaclass,,1,['type'],0,['__getattr__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\deprecated\config.py,BaseConfig,"This class is only retained for backwards compatibility.

!!! Warning ""Deprecated""
    BaseConfig is deprecated. Use the [`pydantic.ConfigDict`][pydantic.ConfigDict] instead.",2,[],0,"['__getattr__', '__init_subclass__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\deprecated\config.py,_ExtraMeta,,1,['type'],0,['__getattribute__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\deprecated\config.py,Extra,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\deprecated\decorator.py,ValidatedFunction,,6,[],0,"['__init__', 'init_model_instance', 'call', 'build_values', 'execute', 'create_model']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\plugin\__init__.py,SchemaTypePath,"Path defining where `schema_type` was defined, or where `TypeAdapter` was called.",0,['NamedTuple'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\plugin\__init__.py,PydanticPluginProtocol,Protocol defining the interface for Pydantic plugins.,1,['Protocol'],0,['new_schema_validator'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\plugin\__init__.py,BaseValidateHandlerProtocol,"Base class for plugin callbacks protocols.

You shouldn't implement this protocol directly, instead use one of the subclasses with adds the correctly
typed `on_error` method.",3,['Protocol'],1,"['on_success', 'on_error', 'on_exception']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\plugin\__init__.py,ValidatePythonHandlerProtocol,Event handler for `SchemaValidator.validate_python`.,1,"['BaseValidateHandlerProtocol', 'Protocol']",0,['on_enter'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\plugin\__init__.py,ValidateJsonHandlerProtocol,Event handler for `SchemaValidator.validate_json`.,1,"['BaseValidateHandlerProtocol', 'Protocol']",0,['on_enter'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\plugin\__init__.py,ValidateStringsHandlerProtocol,Event handler for `SchemaValidator.validate_strings`.,1,"['BaseValidateHandlerProtocol', 'Protocol']",0,['on_enter'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\v1\decorator.py,ValidatedFunction,,6,[],0,"['__init__', 'init_model_instance', 'call', 'build_values', 'execute', 'create_model']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\v1\schema.py,SkipField,Utility exception used to exclude fields from schema.,1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\v1\validators.py,IfConfig,,2,[],0,"['__init__', 'check']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_core_utils.py,_WalkCoreSchema,,24,[],0,"['__init__', '_build_schema_type_to_method', 'walk', '_walk', '_handle_other_schemas', '_handle_ser_schemas', 'handle_definitions_schema', 'handle_list_schema', 'handle_set_schema', 'handle_frozenset_schema', 'handle_generator_schema', 'handle_tuple_schema', 'handle_dict_schema', 'handle_function_schema', 'handle_union_schema', 'handle_tagged_union_schema', 'handle_chain_schema', 'handle_lax_or_strict_schema', 'handle_json_or_python_schema', 'handle_model_fields_schema', 'handle_typed_dict_schema', 'handle_dataclass_args_schema', 'handle_arguments_schema', 'handle_call_schema']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_decorators_v1.py,V1OnlyValueValidator,"A simple validator, supported for V1 validators and V2 validators.",1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_decorators_v1.py,V1ValidatorWithValues,"A validator with `values` argument, supported for V1 validators and V2 validators.",1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_decorators_v1.py,V1ValidatorWithValuesKwOnly,"A validator with keyword only `values` argument, supported for V1 validators and V2 validators.",1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_decorators_v1.py,V1ValidatorWithKwargs,"A validator with `kwargs` argument, supported for V1 validators and V2 validators.",1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_decorators_v1.py,V1ValidatorWithValuesAndKwargs,"A validator with `values` and `kwargs` arguments, supported for V1 validators and V2 validators.",1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_decorators_v1.py,V1RootValidatorFunction,"A simple root validator, supported for V1 validators and V2 validators.",1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_decorators_v1.py,V2CoreBeforeRootValidator,V2 validator with mode='before'.,1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_decorators_v1.py,V2CoreAfterRootValidator,V2 validator with mode='after'.,1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_discriminated_union.py,MissingDefinitionForUnionRef,"Raised when applying a discriminated union discriminator to a schema
requires a definition that is not yet defined",1,['Exception'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_discriminated_union.py,_ApplyInferredDiscriminator,"This class is used to convert an input schema containing a union schema into one where that union is
replaced with a tagged-union, with all the associated debugging and performance benefits.

This is done by:
* Validating that the input schema is compatible with the provided discriminator
* Introspecting the schema to determine which discriminator values should map to which union choices
* Handling various edge cases such as 'definitions', 'default', 'nullable' schemas, and more

I have chosen to implement the conversion algorithm in this class, rather than a function,
to make it easier to maintain state while recursively walking the provided CoreSchema.",12,[],0,"['__init__', 'apply', '_apply_to_root', '_handle_choice', '_is_discriminator_shared', '_infer_discriminator_values_for_choice', '_infer_discriminator_values_for_typed_dict_choice', '_infer_discriminator_values_for_model_choice', '_infer_discriminator_values_for_dataclass_choice', '_infer_discriminator_values_for_field', '_infer_discriminator_values_for_inner_schema', '_set_unique_choice_for_values']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_docs_extraction.py,DocstringVisitor,,4,['ast.NodeVisitor'],0,"['__init__', 'visit', 'visit_AnnAssign', 'visit_Expr']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_generics.py,PydanticGenericMetadata,,0,['typing_extensions.TypedDict'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_std_types_schema.py,InnerSchemaValidator,"Use a fixed CoreSchema, avoiding interference from outward annotations.",2,[],4,"['__get_pydantic_json_schema__', '__get_pydantic_core_schema__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_std_types_schema.py,DequeValidator,,1,[],2,['__get_pydantic_core_schema__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pydantic\_internal\_std_types_schema.py,MappingValidator,,2,[],6,"['serialize_mapping_via_dict', '__get_pydantic_core_schema__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\chimp.py,Fist,"moves a clenched fist on the screen, following the mouse",4,['pg.sprite.Sprite'],0,"['__init__', 'update', 'punch', 'unpunch']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\chimp.py,Chimp,"moves a monkey critter across the screen. it can spin the
monkey when it is punched.",5,['pg.sprite.Sprite'],0,"['__init__', 'update', '_walk', '_spin', 'punched']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\glcube.py,Rotation,Data class that stores rotation angles in three axes.,1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\go_over_there.py,Ball,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\grid.py,Player,,3,[],0,"['__init__', 'draw', 'move']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\grid.py,Game,,3,[],0,"['__init__', 'main', 'grid_loop']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\joystick.py,TextPrint,,5,[],0,"['__init__', 'tprint', 'reset', 'indent', 'unindent']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\mask.py,Sprite,Moving Sprite demonstrating pixel-perfect collisions between pg.mask.Mask objects,3,[],0,"['__init__', 'collide', 'update']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\moveit.py,GameObject,,2,[],0,"['__init__', 'move']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\sprite_texture.py,Something,,1,['pg.sprite.Sprite'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\testsprite.py,Thingy,,2,['pg.sprite.DirtySprite'],1,"['__init__', 'update']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\examples\testsprite.py,Static,,1,['pg.sprite.DirtySprite'],1,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\blit_test.py,BlitTest,,2,['unittest.TestCase'],0,"['test_SRCALPHA', 'test_BLEND']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\blit_test.py,BlitsTest,Tests for pygame.Surface.blits,11,['unittest.TestCase'],0,"['setUp', 'make_blit_list', 'custom_blits', 'test_custom_blits_performance', 'test_blits_performance', 'test_blits_correctness', 'test_blits_doreturn', 'test_blits_not_sequence', 'test_blits_wrong_length', 'test_blits_bad_surf_args', 'test_blits_bad_dest']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\cursors_test.py,CursorsModuleTest,,3,['unittest.TestCase'],0,"['test_compile', 'test_load_xbm', 'test_Cursor']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\image__save_gl_surface_test.py,GL_ImageSave,,1,['unittest.TestCase'],0,['test_image_save_works_with_opengl_surfaces'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\locals_test.py,LocalsTest,,1,['unittest.TestCase'],0,['test_locals_has_all_constants'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\rwobject_test.py,RWopsEncodeStringTest,,13,['unittest.TestCase'],0,"['test_obj_None', 'test_returns_bytes', 'test_obj_bytes', 'test_encode_unicode', 'test_error_fowarding', 'test_errors', 'test_encoding_error', 'test_check_defaults', 'test_etype', 'test_etype__invalid', 'test_string_with_null_bytes', 'test_smp', 'test_pathlib_obj']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\rwobject_test.py,RWopsEncodeFilePathTest,,5,['unittest.TestCase'],0,"['test_encoding', 'test_error_fowarding', 'test_path_with_null_bytes', 'test_etype', 'test_etype__invalid']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\surflock_test.py,SurfaceLockTest,,4,['unittest.TestCase'],0,"['test_lock', 'test_subsurface_lock', 'test_pxarray_ref', 'test_buffer']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\threads_test.py,WorkerQueueTypeTest,,5,['unittest.TestCase'],0,"['test_usage_with_different_functions', 'test_do', 'test_stop', 'test_threadloop', 'test_wait']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\threads_test.py,ThreadsModuleTest,,7,['unittest.TestCase'],0,"['test_benchmark_workers', 'test_init', 'test_quit', 'test_tmap', 'todo_test_tmap__None_func_and_multiple_sequences', 'test_tmap__wait', 'test_FuncResult']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\transform_test.py,TransformModuleTest,,40,['unittest.TestCase'],0,"['test_scale__alpha', 'test_scale__destination', 'test_scale__vector2', 'test_scale__zero_surface_transform', 'test_scale_by', 'test_smoothscale_by', 'test_grayscale', 'test_threshold__honors_third_surface', 'test_threshold_dest_surf_not_change', 'test_threshold_dest_surf_all_changed', 'test_threshold_count', 'test_threshold_search_surf', 'test_threshold_inverse_set', 'test_threshold_non_src_alpha', 'test_threshold__uneven_colors', 'test_threshold_set_behavior2', 'test_threshold_set_behavior0', 'test_threshold_from_surface', 'test_threshold__surface', 'test_threshold__subclassed_surface', 'test_laplacian', 'test_laplacian__24_big_endian', 'test_average_surfaces', 'test_average_surfaces__24', 'test_average_surfaces__24_big_endian', 'test_average_surfaces__subclassed_surfaces', 'test_average_surfaces__subclassed_destination_surface', 'test_average_color', 'test_average_color_considering_alpha_all_pixels_opaque', 'test_average_color_considering_alpha', 'test_rotate', 'test_rotate_of_0_sized_surface', 'test_rotate__lossless_at_90_degrees', 'test_scale2x', 'test_scale2xraw', 'test_get_smoothscale_backend', 'test_set_smoothscale_backend', 'test_chop', 'test_rotozoom', 'test_smoothscale']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\transform_test.py,TransformDisplayModuleTest,,4,['unittest.TestCase'],0,"['setUp', 'tearDown', 'test_flip', 'test_flip_alpha']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\threads\__init__.py,WorkerQueue,,6,[],0,"['__init__', '_setup_workers', 'do', 'stop', 'threadloop', 'wait']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\threads\__init__.py,FuncResult,"Used for wrapping up a function call so that the results are stored
inside the instances result attribute.",2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\test_utils\async_sub.py,Popen,,7,['subprocess.Popen'],0,"['recv', 'recv_err', 'send_recv', 'read_async', 'send_all', 'get_conn_maxsize', '_close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\test_utils\async_sub.py,AsyncTest,,1,['unittest.TestCase'],0,['test_proc_in_time_or_kill'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\test_utils\test_machinery.py,PygameTestLoader,,2,['unittest.TestLoader'],0,"['__init__', 'getTestCaseNames']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\test_utils\test_machinery.py,TestTags,,3,[],0,"['__init__', 'get_parent_module', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\test_utils\__init__.py,SurfaceSubclass,A subclassed Surface to test inheritance.,1,['pygame.Surface'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\all_ok\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\all_ok\fake_3_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\all_ok\fake_4_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\all_ok\fake_5_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\all_ok\fake_6_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\all_ok\no_assertions__ret_code_of_1__test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\all_ok\zero_tests_test.py,KeyModuleTest,,0,['unittest.TestCase'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\everything\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\everything\incomplete_todo_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'todo_test_get_pressed', 'test_name', 'todo_test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\everything\magic_tag_test.py,KeyModuleTest,,5,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\everything\sleep_test.py,KeyModuleTest,,1,['unittest.TestCase'],0,['test_get_focused'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\exclude\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\exclude\invisible_tag_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\exclude\magic_tag_test.py,KeyModuleTest,,5,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\failures1\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\failures1\fake_3_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\failures1\fake_4_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\incomplete\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'todo_test_get_pressed', 'test_name', 'todo_test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\incomplete\fake_3_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\incomplete_todo\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'todo_test_get_pressed', 'test_name', 'todo_test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\incomplete_todo\fake_3_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\infinite_loop\fake_1_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\infinite_loop\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\print_stderr\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\print_stderr\fake_3_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\print_stderr\fake_4_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\print_stdout\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\print_stdout\fake_3_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\print_stdout\fake_4_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\timeout\fake_2_test.py,KeyModuleTest,,6,['unittest.TestCase'],0,"['test_get_focused', 'test_get_mods', 'test_get_pressed', 'test_name', 'test_set_mods', 'test_set_repeat']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygame\tests\run_tests__tests\timeout\sleep_test.py,KeyModuleTest,,1,['unittest.TestCase'],0,['test_get_focused'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygments\formatters\__init__.py,_automodule,Automatically import formatters.,1,['types.ModuleType'],0,['__getattr__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\pygments\lexers\__init__.py,_automodule,Automatically import lexers.,1,['types.ModuleType'],0,['__getattr__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\adapters\host_header_ssl.py,HostHeaderSSLAdapter,"A HTTPS Adapter for Python Requests that sets the hostname for certificate
verification based on the Host header.

This allows requesting the IP address directly via HTTPS without getting
a ""hostname doesn't match"" exception.

Example usage:

    >>> s.mount('https://', HostHeaderSSLAdapter())
    >>> s.get(""https://93.184.216.34"", headers={""Host"": ""example.org""})",1,['HTTPAdapter'],0,['send'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\adapters\socket_options.py,SocketOptionsAdapter,"An adapter for requests that allows users to specify socket options.

Since version 2.4.0 of requests, it is possible to specify a custom list
of socket options that need to be set before establishing the connection.

Example usage::

    >>> import socket
    >>> import requests
    >>> from requests_toolbelt.adapters import socket_options
    >>> s = requests.Session()
    >>> opts = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 0)]
    >>> adapter = socket_options.SocketOptionsAdapter(socket_options=opts)
    >>> s.mount('http://', adapter)

You can also take advantage of the list of default options on this class
to keep using the original options in addition to your custom options. In
that case, ``opts`` might look like::

    >>> opts = socket_options.SocketOptionsAdapter.default_options + opts",2,['adapters.HTTPAdapter'],0,"['__init__', 'init_poolmanager']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\adapters\socket_options.py,TCPKeepAliveAdapter,"An adapter for requests that turns on TCP Keep-Alive by default.

The adapter sets 4 socket options:

- ``SOL_SOCKET`` ``SO_KEEPALIVE`` - This turns on TCP Keep-Alive
- ``IPPROTO_TCP`` ``TCP_KEEPINTVL`` 20 - Sets the keep alive interval
- ``IPPROTO_TCP`` ``TCP_KEEPCNT`` 5 - Sets the number of keep alive probes
- ``IPPROTO_TCP`` ``TCP_KEEPIDLE`` 60 - Sets the keep alive time if the
  socket library has the ``TCP_KEEPIDLE`` constant

The latter three can be overridden by keyword arguments (respectively):

- ``interval``
- ``count``
- ``idle``

You can use this adapter like so::

   >>> from requests_toolbelt.adapters import socket_options
   >>> tcp = socket_options.TCPKeepAliveAdapter(idle=120, interval=10)
   >>> s = requests.Session()
   >>> s.mount('http://', tcp)",1,['SocketOptionsAdapter'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\adapters\source.py,SourceAddressAdapter,"A Source Address Adapter for Python Requests that enables you to choose the
local address to bind to. This allows you to send your HTTP requests from a
specific interface and IP address.

Two address formats are accepted. The first is a string: this will set the
local IP address to the address given in the string, and will also choose a
semi-random high port for the local port number.

The second is a two-tuple of the form (ip address, port): for example,
``('10.10.10.10', 8999)``. This will set the local IP address to the first
element, and the local port to the second element. If ``0`` is used as the
port number, a semi-random high port will be selected.

.. warning:: Setting an explicit local port can have negative interactions
             with connection-pooling in Requests: in particular, it risks
             the possibility of getting ""Address in use"" errors. The
             string-only argument is generally preferred to the tuple-form.

Example usage:

.. code-block:: python

    import requests
    from requests_toolbelt.adapters.source import SourceAddressAdapter

    s = requests.Session()
    s.mount('http://', SourceAddressAdapter('10.10.10.10'))
    s.mount('https://', SourceAddressAdapter(('10.10.10.10', 8999)))",3,['HTTPAdapter'],0,"['__init__', 'init_poolmanager', 'proxy_manager_for']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\adapters\x509.py,X509Adapter,"Adapter for use with X.509 certificates.

Provides an interface for Requests sessions to contact HTTPS urls and
authenticate  with an X.509 cert by implementing the Transport Adapter
interface. This class will need to be manually instantiated and mounted
to the session

:param pool_connections: The number of urllib3 connection pools to
       cache.
:param pool_maxsize: The maximum number of connections to save in the
        pool.
:param max_retries: The maximum number of retries each connection
    should attempt. Note, this applies only to failed DNS lookups,
    socket connections and connection timeouts, never to requests where
    data has made it to the server. By default, Requests does not retry
    failed connections. If you need granular control over the
    conditions under which we retry a request, import urllib3's
    ``Retry`` class and pass that instead.
:param pool_block: Whether the connection pool should block for
        connections.

:param bytes cert_bytes:
    bytes object containing contents of a cryptography.x509Certificate
    object using the encoding specified by the ``encoding`` parameter.
:param bytes pk_bytes:
    bytes object containing contents of a object that implements
    ``cryptography.hazmat.primitives.serialization.PrivateFormat``
    using the encoding specified by the ``encoding`` parameter.
:param password:
    string or utf8 encoded bytes containing the passphrase used for the
    private key. None if unencrypted. Defaults to None.
:param encoding:
    Enumeration detailing the encoding method used on the ``cert_bytes``
    parameter. Can be either PEM or DER. Defaults to PEM.
:type encoding:
    :class: `cryptography.hazmat.primitives.serialization.Encoding`

Usage::

  >>> import requests
  >>> from requests_toolbelt.adapters.x509 import X509Adapter
  >>> s = requests.Session()
  >>> a = X509Adapter(max_retries=3,
            cert_bytes=b'...', pk_bytes=b'...', encoding='...'
  >>> s.mount('https://', a)",5,['HTTPAdapter'],0,"['__init__', 'init_poolmanager', 'proxy_manager_for', '_import_pyopensslcontext', '_check_version']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\auth\guess.py,GuessAuth,Guesses the auth type by the WWW-Authentication header.,5,['auth.AuthBase'],0,"['__init__', '_handle_basic_auth_401', '_handle_digest_auth_401', 'handle_401', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\auth\guess.py,GuessProxyAuth,"Guesses the auth type by WWW-Authentication and Proxy-Authentication
headers",5,['GuessAuth'],0,"['__init__', '_handle_basic_auth_407', '_handle_digest_auth_407', 'handle_407', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\cookies\forgetful.py,ForgetfulCookieJar,,1,['RequestsCookieJar'],0,['set_cookie'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\threaded\thread.py,SessionThread,,6,['object'],0,"['__init__', '_create_worker', '_handle_request', '_make_request', 'is_alive', 'join']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\requests_toolbelt\utils\dump.py,PrefixSettings,,1,['_PrefixSettings'],0,['__new__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\command\bdist_rpm.py,bdist_rpm,"Override the default bdist_rpm behavior to do the following:

1. Run egg_info to ensure the name and version are properly calculated.
2. Always run 'install' using --single-version-externally-managed to
   disable eggs in RPM distributions.",2,['orig.bdist_rpm'],1,"['run', '_make_spec_file']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\command\build_clib.py,build_clib,"Override the default build_clib behaviour to do the following:

1. Implement a rudimentary timestamp-based dependency system
   so 'compile()' doesn't run every time.
2. Add more keys to the 'build_info' dictionary:
    * obj_deps - specify dependencies for each object compiled.
                 this should be a dictionary mapping a key
                 with the source filename to a list of
                 dependencies. Use an empty string for global
                 dependencies.
    * cflags   - specify a list of additional flags to pass to
                 the compiler.",1,['orig.build_clib'],1,['build_libraries'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\command\install_scripts.py,install_scripts,"Do normal script install, plus any egg_info wrapper scripts",4,['orig.install_scripts'],1,"['initialize_options', 'run', '_install_ep_scripts', 'write_script']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_build.py,Subcommand,Dummy command to be used in tests,3,['Command'],0,"['initialize_options', 'finalize_options', 'run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_build_ext.py,TestBuildExt,,6,[],0,"['test_get_ext_filename', 'test_abi3_filename', 'test_ext_suffix_override', 'dist_with_example', 'test_get_outputs', 'test_get_output_mapping_with_stub']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_build_ext.py,TestBuildExtInplace,,4,[],0,"['get_build_ext_cmd', 'get_log_messages', 'test_optional', 'test_non_optional']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_depends.py,TestGetModuleConstant,,1,[],0,['test_basic'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_namespaces.py,TestNamespaces,,4,[],0,"['test_mixed_site_and_non_site', 'test_pkg_resources_import', 'test_namespace_package_installed_and_cwd', 'test_packages_in_the_same_namespace_installed_and_cwd']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_packageindex.py,TestPackageIndex,,13,[],0,"['test_regex', 'test_bad_url_bad_port', 'test_bad_url_typo', 'test_bad_url_bad_status_line', 'test_bad_url_double_scheme', 'test_url_ok', 'test_parse_bdist_wininst', 'test__vcs_split_rev_from_url', 'test_local_index', 'test_egg_fragment', 'test_download_git_with_rev', 'test_download_git_no_rev', 'test_download_svn']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_packageindex.py,TestContentCheckers,,5,[],0,"['test_md5', 'test_other_fragment', 'test_blank_md5', 'test_get_hash_name_md5', 'test_report']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_packageindex.py,TestPyPIConfig,,1,[],0,['test_percent_in_password'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\test_wheel.py,Record,,2,[],0,"['__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsError,The root of all Distutils evil.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsModuleError,"Unable to load an expected module, or to find an expected class
within some module (in particular, command modules and classes).",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsClassError,"Some command class (or possibly distribution class, if anyone
feels a need to subclass Distribution) is found not to be holding
up its end of the bargain, ie. implementing some part of the
""command ""interface.",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsGetoptError,The option table provided to 'fancy_getopt()' is bogus.,0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsArgError,"Raised by fancy_getopt in response to getopt.error -- ie. an
error in the command line usage.",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsFileError,"Any problems in the filesystem: expected file not found, etc.
Typically this is for problems that we detect before OSError
could be raised.",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsOptionError,"Syntactic/semantic errors in command options, such as use of
mutually conflicting options, or inconsistent options,
badly-spelled values, etc.  No distinction is made between option
values originating in the setup script, the command line, config
files, or what-have-you -- but if we *know* something originated in
the setup script, we'll raise DistutilsSetupError instead.",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsSetupError,"For errors that can be definitely blamed on the setup script,
such as invalid keyword arguments to 'setup()'.",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsPlatformError,"We don't know how to do something on the current platform (but
we do know how to do it on some platform) -- eg. trying to compile
C files on a platform not supported by a CCompiler subclass.",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsExecError,"Any problems executing an external program (such as the C
compiler, when compiling C files).",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsInternalError,"Internal inconsistencies or impossibilities (obviously, this
should never be seen if the code is working!).",0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsTemplateError,Syntax error in a file list template.,0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,DistutilsByteCompileError,Byte compile error.,0,['DistutilsError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,CCompilerError,Some compile/link operation failed.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,PreprocessError,Failure to preprocess one or more C/C++ files.,0,['CCompilerError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,CompileError,Failure to compile one or more C/C++ source files.,0,['CCompilerError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,LibError,"Failure to create a static library from one or more C/C++ object
files.",0,['CCompilerError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,LinkError,"Failure to link one or more C/C++ object files into an executable
or shared library file.",0,['CCompilerError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\errors.py,UnknownFileError,Attempt to process an unknown file type.,0,['CCompilerError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\extension.py,Extension,"Just a collection of attributes that describes an extension
module and everything needed to build it (hopefully in a portable
way, but there are hooks that let you be as unportable as you need).

Instance attributes:
  name : string
    the full name of the extension, including any packages -- ie.
    *not* a filename or pathname, but Python dotted name
  sources : [string | os.PathLike]
    list of source filenames, relative to the distribution root
    (where the setup script lives), in Unix form (slash-separated)
    for portability.  Source files may be C, C++, SWIG (.i),
    platform-specific resource files, or whatever else is recognized
    by the ""build_ext"" command as source for a Python extension.
  include_dirs : [string]
    list of directories to search for C/C++ header files (in Unix
    form for portability)
  define_macros : [(name : string, value : string|None)]
    list of macros to define; each macro is defined using a 2-tuple,
    where 'value' is either the string to define it to or None to
    define it without a particular value (equivalent of ""#define
    FOO"" in source or -DFOO on Unix C compiler command line)
  undef_macros : [string]
    list of macros to undefine explicitly
  library_dirs : [string]
    list of directories to search for C/C++ libraries at link time
  libraries : [string]
    list of library names (not filenames or paths) to link against
  runtime_library_dirs : [string]
    list of directories to search for C/C++ libraries at run time
    (for shared extensions, this is when the extension is loaded)
  extra_objects : [string]
    list of extra files to link with (eg. object files not implied
    by 'sources', static library that must be explicitly specified,
    binary resource files, etc.)
  extra_compile_args : [string]
    any extra platform- and compiler-specific information to use
    when compiling the source files in 'sources'.  For platforms and
    compilers where ""command line"" makes sense, this is typically a
    list of command-line arguments, but for other platforms it could
    be anything.
  extra_link_args : [string]
    any extra platform- and compiler-specific information to use
    when linking object files together to create the extension (or
    to create a new static Python interpreter).  Similar
    interpretation as for 'extra_compile_args'.
  export_symbols : [string]
    list of symbols to be exported from a shared extension.  Not
    used on all platforms, and not generally necessary for Python
    extensions, which typically export exactly one symbol: ""init"" +
    extension_name.
  swig_opts : [string]
    any extra options to pass to SWIG if a source file has the .i
    extension.
  depends : [string]
    list of files that the extension depends on
  language : string
    extension language (i.e. ""c"", ""c++"", ""objc""). Will be detected
    from the source extensions if not provided.
  optional : boolean
    specifies that a build failure in the extension should not abort the
    build process, but simply not install the failing extension.",2,[],0,"['__init__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\fancy_getopt.py,FancyGetopt,"Wrapper around the standard 'getopt()' module that provides some
handy extra functionality:
  * short and long options are tied together
  * options have help strings, and help text can be assembled
    from them
  * options set attributes of a passed-in object
  * boolean options can have ""negative aliases"" -- eg. if
    --quiet is the ""negative alias"" of --verbose, then ""--quiet""
    on the command line sets 'verbose' to false",14,[],0,"['__init__', '_build_index', 'set_option_table', 'add_option', 'has_option', 'get_attr_name', '_check_alias_dict', 'set_aliases', 'set_negative_aliases', '_grok_option_table', 'getopt', 'get_option_order', 'generate_help', 'print_help']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\fancy_getopt.py,OptionDummy,"Dummy class just used as a place to hold command-line option
values as instance attributes.",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\versionpredicate.py,VersionPredicate,"Parse and test package version predicates.

>>> v = VersionPredicate('pyepat.abc (>1.0, <3333.3a1, !=1555.1b3)')

The `name` attribute provides the full dotted name that is given::

>>> v.name
'pyepat.abc'

The str() of a `VersionPredicate` provides a normalized
human-readable version of the expression::

>>> print(v)
pyepat.abc (> 1.0, < 3333.3a1, != 1555.1b3)

The `satisfied_by()` method can be used to determine with a given
version number is included in the set described by the version
restrictions::

>>> v.satisfied_by('1.1')
True
>>> v.satisfied_by('1.4')
True
>>> v.satisfied_by('1.0')
False
>>> v.satisfied_by('4444.4')
False
>>> v.satisfied_by('1555.1b3')
False

`VersionPredicate` is flexible in accepting extra whitespace::

>>> v = VersionPredicate(' pat( ==  0.1  )  ')
>>> v.name
'pat'
>>> v.satisfied_by('0.1')
True
>>> v.satisfied_by('0.2')
False

If any version numbers passed in do not conform to the
restrictions of `StrictVersion`, a `ValueError` is raised::

>>> v = VersionPredicate('p1.p2.p3.p4(>=1.0, <=1.3a1, !=1.2zb3)')
Traceback (most recent call last):
  ...
ValueError: invalid version number '1.2zb3'

It the module or package name given does not conform to what's
allowed as a legal module or package name, `ValueError` is
raised::

>>> v = VersionPredicate('foo-bar')
Traceback (most recent call last):
  ...
ValueError: expected parenthesized list: '-bar'

>>> v = VersionPredicate('foo bar (12.21)')
Traceback (most recent call last):
  ...
ValueError: expected parenthesized list: 'bar (12.21)'",3,[],0,"['__init__', '__str__', 'satisfied_by']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\config\_validate_pyproject\formats.py,_TroveClassifier,"The ``trove_classifiers`` package is the official way of validating classifiers,
however this package might not be always available.
As a workaround we can still download a list from PyPI.
We also don't want to be over strict about it, so simply skipping silently is an
option (classifiers will be validated anyway during the upload to PyPI).",3,[],1,"['__init__', '_disable_download', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\tests\integration\helpers.py,Archive,Compatibility layer for ZipFile/Info and TarFile/Info,4,[],0,"['__init__', '__iter__', 'get_name', 'get_content']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\compat\py39.py,UnequalIterablesError,,1,['ValueError'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\support.py,TempdirManager,Mix-in class that handles temporary directories for test cases.,3,[],0,"['mkdtemp', 'write_file', 'create_dist']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\support.py,DummyCommand,Class to store options for retrieval via set_undefined_options().,2,[],0,"['__init__', 'ensure_finalized']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_bdist.py,TestBuild,,2,['support.TempdirManager'],0,"['test_formats', 'test_skip_build']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_build.py,TestBuild,,1,['support.TempdirManager'],0,['test_finalize_options'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_check.py,TestCheck,,7,['support.TempdirManager'],0,"['_run', 'test_check_metadata', 'test_check_author_maintainer', 'test_check_document', 'test_check_restructuredtext', 'test_check_restructuredtext_with_syntax_highlight', 'test_check_all']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_clean.py,TestClean,,1,['support.TempdirManager'],0,['test_simple_run'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_cmd.py,MyCmd,,1,['Command'],0,['initialize_options'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_cmd.py,TestCommand,,7,[],0,"['test_ensure_string_list', 'test_make_file', 'test_dump_options', 'test_ensure_string', 'test_ensure_filename', 'test_ensure_dirname', 'test_debug_print']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_core.py,TestCore,,7,[],0,"['test_run_setup_provides_file', 'test_run_setup_preserves_sys_argv', 'test_run_setup_defines_subclass', 'test_run_setup_uses_current_dir', 'test_run_setup_within_if_main', 'test_run_commands', 'test_debug_mode']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_extension.py,TestExtension,,2,[],0,"['test_read_setup_file', 'test_extension_init']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_file_util.py,TestFileUtil,,5,[],0,"['test_move_file_verbosity', 'test_move_file_exception_unpacking_rename', 'test_move_file_exception_unpacking_unlink', 'test_copy_file_hard_link', 'test_copy_file_hard_link_failure']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_install_data.py,TestInstallData,,1,['support.TempdirManager'],0,['test_simple_run'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_install_headers.py,TestInstallHeaders,,1,['support.TempdirManager'],0,['test_simple_run'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_install_scripts.py,TestInstallScripts,,2,['support.TempdirManager'],0,"['test_default_settings', 'test_installation']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_log.py,TestLog,,1,[],0,['test_non_ascii'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_modified.py,TestDepUtil,,7,['support.TempdirManager'],0,"['test_newer', '_setup_1234', 'test_newer_pairwise', 'test_newer_pairwise_mismatch', 'test_newer_pairwise_empty', 'test_newer_pairwise_fresh', 'test_newer_group']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_text_file.py,TestTextFile,,1,['support.TempdirManager'],0,['test_class'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_distutils\tests\test_version.py,TestVersion,,3,[],0,"['test_prerelease', 'test_cmp_strict', 'test_cmp']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\autocommand\automain.py,AutomainRequiresModuleError,,0,"['AutocommandError', 'TypeError']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\autocommand\autoparse.py,AnnotationError,"Annotation error: annotation must be a string, type, or tuple of both",0,['AutocommandError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\autocommand\autoparse.py,PositionalArgError,Postional Arg Error: autocommand can't handle postional-only parameters,0,['AutocommandError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\autocommand\autoparse.py,KWArgError,kwarg Error: autocommand can't handle a **kwargs parameter,0,['AutocommandError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\autocommand\autoparse.py,DocstringError,Docstring error,0,['AutocommandError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\autocommand\autoparse.py,TooManySplitsError,"The docstring had too many ---- section splits. Currently we only support
using up to a single split, to split the docstring into description and
epilog parts.",0,['DocstringError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\autocommand\errors.py,AutocommandError,Base class for autocommand exceptions,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\more_itertools\recipes.py,UnequalIterablesError,,1,['ValueError'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\markers.py,InvalidMarker,"An invalid marker was found, users should refer to PEP 508.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\markers.py,UndefinedComparison,An invalid operation was attempted on a value that doesn't support it.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\markers.py,UndefinedEnvironmentName,"A name was attempted to be used that does not exist inside of the
environment.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\markers.py,Environment,,0,['TypedDict'],11,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\markers.py,Marker,,6,[],0,"['__init__', '__str__', '__repr__', '__hash__', '__eq__', 'evaluate']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\requirements.py,InvalidRequirement,"An invalid requirement was found, users should refer to PEP 508.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\requirements.py,Requirement,"Parse a requirement.

Parse a given requirement string into its parts, such as name, specifier,
URL, and extras. Raises InvalidRequirement on a badly-formed requirement
string.",6,[],0,"['__init__', '_iter_parts', '__str__', '__repr__', '__hash__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\utils.py,InvalidName,An invalid distribution name; users should refer to the packaging user guide.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\utils.py,InvalidWheelFilename,"An invalid wheel filename was found, users should refer to PEP 427.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\utils.py,InvalidSdistFilename,"An invalid sdist filename was found, users should refer to the packaging user guide.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_manylinux.py,_GLibCVersion,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_musllinux.py,_MuslVersion,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_parser.py,Node,,4,[],0,"['__init__', '__str__', '__repr__', 'serialize']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_parser.py,Variable,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_parser.py,Value,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_parser.py,Op,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_parser.py,ParsedRequirement,,0,['NamedTuple'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_structures.py,InfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\packaging\_structures.py,NegativeInfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\typeguard\_exceptions.py,TypeHintWarning,"A warning that is emitted when a type hint in string form could not be resolved to
an actual type.",0,['UserWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\typeguard\_exceptions.py,TypeCheckWarning,Emitted by typeguard's type checkers when a type mismatch is detected.,1,['UserWarning'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\typeguard\_exceptions.py,InstrumentationWarning,Emitted when there's a problem with instrumenting a function for type checks.,1,['UserWarning'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\typeguard\_exceptions.py,TypeCheckError,Raised by typeguard's type checkers when a type mismatch is detected.,3,['Exception'],0,"['__init__', 'append_path_element', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\typeguard\_union_transformer.py,UnionTransformer,,2,['NodeTransformer'],0,"['__init__', 'visit_BinOp']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\zipp\glob.py,Translator,">>> Translator('xyz')
Traceback (most recent call last):
...
AssertionError: Invalid separators

>>> Translator('')
Traceback (most recent call last):
...
AssertionError: Invalid separators",7,[],1,"['__init__', 'translate', 'extend', 'translate_core', 'replace', 'restrict_rglob', 'star_not_empty']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\importlib_resources\future\adapters.py,TraversableResourcesLoader,"Adapt loaders to provide TraversableResources and other
compatibility.

Ensures the readers from importlib_resources are preferred
over stdlib readers.",5,['_adapters.TraversableResourcesLoader'],0,"['get_resource_reader', '_standard_reader', '_zip_reader', '_namespace_reader', '_file_reader']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\importlib_resources\tests\test_custom.py,SimpleLoader,A simple loader that only implements a resource reader.,2,[],0,"['__init__', 'get_resource_reader']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\importlib_resources\tests\test_custom.py,MagicResources,Magically returns the resources at path.,2,['TraversableResources'],0,"['__init__', 'files']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\importlib_resources\tests\test_custom.py,CustomTraversableResourcesTests,,2,['unittest.TestCase'],0,"['setUp', 'test_custom_loader']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\jaraco\functools\__init__.py,Throttler,Rate-limit a function (or other callable).,5,[],0,"['__init__', 'reset', '__call__', '_wait', '__get__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\cli\__init__.py,WheelError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\markers.py,InvalidMarker,"An invalid marker was found, users should refer to PEP 508.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\markers.py,UndefinedComparison,An invalid operation was attempted on a value that doesn't support it.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\markers.py,UndefinedEnvironmentName,"A name was attempted to be used that does not exist inside of the
environment.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\markers.py,Marker,,6,[],0,"['__init__', '__str__', '__repr__', '__hash__', '__eq__', 'evaluate']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\requirements.py,InvalidRequirement,"An invalid requirement was found, users should refer to PEP 508.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\requirements.py,Requirement,"Parse a requirement.

Parse a given requirement string into its parts, such as name, specifier,
URL, and extras. Raises InvalidRequirement on a badly-formed requirement
string.",6,[],0,"['__init__', '_iter_parts', '__str__', '__repr__', '__hash__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\utils.py,InvalidName,An invalid distribution name; users should refer to the packaging user guide.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\utils.py,InvalidWheelFilename,"An invalid wheel filename was found, users should refer to PEP 427.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\utils.py,InvalidSdistFilename,"An invalid sdist filename was found, users should refer to the packaging user guide.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_manylinux.py,_GLibCVersion,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_musllinux.py,_MuslVersion,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_parser.py,Node,,4,[],0,"['__init__', '__str__', '__repr__', 'serialize']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_parser.py,Variable,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_parser.py,Value,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_parser.py,Op,,1,['Node'],0,['serialize'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_parser.py,ParsedRequirement,,0,['NamedTuple'],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_structures.py,InfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\setuptools\_vendor\wheel\vendored\packaging\_structures.py,NegativeInfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\connectors\__init__.py,Connector,"Base class for dialect mixins, for DBAPIs that work
across entirely different database backends.

Currently the only such mixin is pyodbc.",0,['Dialect'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\engine\mock.py,MockConnection,,6,[],3,"['__init__', 'connect', 'schema_for_object', 'execution_options', '_run_ddl_visitor', 'execute']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\ext\compiler.py,_dispatcher,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\ext\serializer.py,Serializer,,1,['pickle.Pickler'],0,['persistent_id'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\ext\serializer.py,Deserializer,,3,['pickle.Unpickler'],0,"['__init__', 'get_engine', 'persistent_load']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\orm\identity.py,IdentityMap,,21,[],3,"['__init__', '_kill', 'all_states', 'contains_state', '__contains__', 'safe_discard', '__getitem__', 'get', 'fast_get_state', 'keys', 'values', 'replace', 'add', '_fast_discard', '_add_unpresent', '_manage_incoming_state', '_manage_removed_state', '_dirty_states', 'check_modified', 'has_key', '__len__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\orm\identity.py,WeakInstanceDict,,15,['IdentityMap'],1,"['__getitem__', '__contains__', 'contains_state', 'replace', 'add', '_add_unpresent', 'fast_get_state', 'get', 'items', 'values', '__iter__', 'all_states', '_fast_discard', 'discard', 'safe_discard']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\orm\_typing.py,_OrmKnownExecutionOptions,,0,['_CoreKnownExecutionOptions'],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\orm\_typing.py,_ORMAdapterProto,"protocol for the :class:`.AliasedInsp._orm_adapt_element` method
which is a synonym for :class:`.AliasedInsp._adapt_element`.",1,['Protocol'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\orm\_typing.py,_LoaderCallable,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\sql\naming.py,ConventionDict,,10,[],0,"['__init__', '_key_table_name', '_column_X', '_key_constraint_name', '_key_column_X_key', '_key_column_X_name', '_key_column_X_label', '_key_referred_table_name', '_key_referred_column_X_name', '__getitem__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\engines.py,ConnectionKiller,,17,[],0,"['__init__', 'add_pool', '_add_conn', '_remove_conn', 'add_engine', '_safe', 'rollback_all', 'checkin_all', 'close_all', 'prepare_for_drop_tables', '_drop_testing_engines', 'after_test', 'after_test_outside_fixtures', 'stop_test_class_inside_fixtures', 'stop_test_class_outside_fixtures', 'final_cleanup', 'assert_all_closed']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\engines.py,ReconnectFixture,,6,[],0,"['__init__', '__getattr__', 'connect', '_safe', 'shutdown', 'restart']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\engines.py,DBAPIProxyCursor,"Proxy a DBAPI cursor.

Tests can provide subclasses of this to intercept
DBAPI-level cursor operations.",5,[],0,"['__init__', 'execute', 'executemany', '__iter__', '__getattr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\engines.py,DBAPIProxyConnection,"Proxy a DBAPI connection.

Tests can provide subclasses of this to intercept
DBAPI-level connection operations.",4,[],0,"['__init__', 'cursor', 'close', '__getattr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\entities.py,BasicEntity,,2,[],0,"['__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\entities.py,ComparableMixin,,2,[],0,"['__ne__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\entities.py,ComparableEntity,,1,"['ComparableMixin', 'BasicEntity']",0,['__hash__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\schema.py,eq_type_affinity,"Helper to compare types inside of datastructures based on affinity.

E.g.::

    eq_(
        inspect(connection).get_columns(""foo""),
        [
            {
                ""name"": ""id"",
                ""type"": testing.eq_type_affinity(sqltypes.INTEGER),
                ""nullable"": False,
                ""default"": None,
                ""autoincrement"": False,
            },
            {
                ""name"": ""data"",
                ""type"": testing.eq_type_affinity(sqltypes.NullType),
                ""nullable"": True,
                ""default"": None,
                ""autoincrement"": False,
            },
        ],
    )",3,[],0,"['__init__', '__eq__', '__ne__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\schema.py,eq_compile_type,similar to eq_type_affinity but uses compile,3,[],0,"['__init__', '__eq__', '__ne__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\testing\schema.py,eq_clause_element,Helper to compare SQL structures based on compare(),3,[],0,"['__init__', '__eq__', '__ne__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\compat.py,FullArgSpec,,0,['typing.NamedTuple'],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\concurrency.py,_AsyncUtil,Asyncio util for test suite/ util only,4,[],0,"['__init__', 'run', 'run_in_greenlet', 'close']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\preloaded.py,_ModuleRegistry,"Registry of modules to load in a package init file.

To avoid potential thread safety issues for imports that are deferred
in a function, like https://bugs.python.org/issue38884, these modules
are added to the system module cache by importing them after the packages
has finished initialization.

A global instance is provided under the name :attr:`.preloaded`. Use
the function :func:`.preload_module` to register modules to load and
:meth:`.import_prefix` to load all the modules that start with the
given path.

While the modules are loaded in the global module cache, it's advisable
to access them using :attr:`.preloaded` to ensure that it was actually
registered. Each registered module is added to the instance ``__dict__``
in the form `<package>_<module>`, omitting ``sqlalchemy`` from the package
name. Example: ``sqlalchemy.sql.util`` becomes ``preloaded.sql_util``.",3,[],0,"['__init__', 'preload_module', 'import_prefix']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\typing.py,ArgsTypeProcotol,"protocol for types that have ``__args__``

there's no public interface for this AFAIK",0,['Protocol'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\typing.py,GenericProtocol,"protocol for generic types.

this since Python.typing _GenericAlias is private",0,['Protocol[_T]'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\typing.py,SupportsKeysAndGetItem,,2,"['Protocol[_KT, _VT_co]']",0,"['keys', '__getitem__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\typing.py,DescriptorProto,,3,['Protocol'],0,"['__get__', '__set__', '__delete__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\typing.py,DescriptorReference,"a descriptor that refers to a descriptor.

used for cases where we need to have an instance variable referring to an
object that is itself a descriptor, which typically confuses typing tools
as they don't know when they should use ``__get__`` or not when referring
to the descriptor assignment as an instance variable. See
sqlalchemy.orm.interfaces.PropComparator.prop",0,['Generic[_DESC]'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\typing.py,RODescriptorReference,"a descriptor that refers to a descriptor.

same as :class:`.DescriptorReference` but is read-only, so that subclasses
can define a subtype as the generically contained element",0,['Generic[_DESC_co]'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\util\typing.py,CallableReference,"a descriptor that refers to a callable.

works around mypy's limitation of not allowing callables assigned
as instance variables",0,['Generic[_FN]'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\mssql\json.py,JSON,"MSSQL JSON type.

MSSQL supports JSON-formatted data as of SQL Server 2016.

The :class:`_mssql.JSON` datatype at the DDL level will represent the
datatype as ``NVARCHAR(max)``, but provides for JSON-level comparison
functions as well as Python coercion behavior.

:class:`_mssql.JSON` is used automatically whenever the base
:class:`_types.JSON` datatype is used against a SQL Server backend.

.. seealso::

    :class:`_types.JSON` - main documentation for the generic
    cross-platform JSON datatype.

The :class:`_mssql.JSON` type supports persistence of JSON values
as well as the core index operations provided by :class:`_types.JSON`
datatype, by adapting the operations to render the ``JSON_VALUE``
or ``JSON_QUERY`` functions at the database level.

The SQL Server :class:`_mssql.JSON` type necessarily makes use of the
``JSON_QUERY`` and ``JSON_VALUE`` functions when querying for elements
of a JSON object.   These two functions have a major restriction in that
they are **mutually exclusive** based on the type of object to be returned.
The ``JSON_QUERY`` function **only** returns a JSON dictionary or list,
but not an individual string, numeric, or boolean element; the
``JSON_VALUE`` function **only** returns an individual string, numeric,
or boolean element.   **both functions either return NULL or raise
an error if they are not used against the correct expected value**.

To handle this awkward requirement, indexed access rules are as follows:

1. When extracting a sub element from a JSON that is itself a JSON
   dictionary or list, the :meth:`_types.JSON.Comparator.as_json` accessor
   should be used::

        stmt = select(
            data_table.c.data[""some key""].as_json()
        ).where(
            data_table.c.data[""some key""].as_json() == {""sub"": ""structure""}
        )

2. When extracting a sub element from a JSON that is a plain boolean,
   string, integer, or float, use the appropriate method among
   :meth:`_types.JSON.Comparator.as_boolean`,
   :meth:`_types.JSON.Comparator.as_string`,
   :meth:`_types.JSON.Comparator.as_integer`,
   :meth:`_types.JSON.Comparator.as_float`::

        stmt = select(
            data_table.c.data[""some key""].as_string()
        ).where(
            data_table.c.data[""some key""].as_string() == ""some string""
        )

.. versionadded:: 1.4",0,['sqltypes.JSON'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\mssql\json.py,_FormatTypeMixin,,3,[],0,"['_format_value', 'bind_processor', 'literal_processor']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\mssql\json.py,JSONIndexType,,1,"['_FormatTypeMixin', 'sqltypes.JSON.JSONIndexType']",0,['_format_value'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\mssql\json.py,JSONPathType,,1,"['_FormatTypeMixin', 'sqltypes.JSON.JSONPathType']",0,['_format_value'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\mysql\json.py,JSON,"MySQL JSON type.

MySQL supports JSON as of version 5.7.
MariaDB supports JSON (as an alias for LONGTEXT) as of version 10.2.

:class:`_mysql.JSON` is used automatically whenever the base
:class:`_types.JSON` datatype is used against a MySQL or MariaDB backend.

.. seealso::

    :class:`_types.JSON` - main documentation for the generic
    cross-platform JSON datatype.

The :class:`.mysql.JSON` type supports persistence of JSON values
as well as the core index operations provided by :class:`_types.JSON`
datatype, by adapting the operations to render the ``JSON_EXTRACT``
function at the database level.",0,['sqltypes.JSON'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\mysql\json.py,_FormatTypeMixin,,3,[],0,"['_format_value', 'bind_processor', 'literal_processor']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\mysql\json.py,JSONIndexType,,1,"['_FormatTypeMixin', 'sqltypes.JSON.JSONIndexType']",0,['_format_value'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\mysql\json.py,JSONPathType,,1,"['_FormatTypeMixin', 'sqltypes.JSON.JSONPathType']",0,['_format_value'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\sqlite\json.py,JSON,"SQLite JSON type.

SQLite supports JSON as of version 3.9 through its JSON1_ extension. Note
that JSON1_ is a
`loadable extension <https://www.sqlite.org/loadext.html>`_ and as such
may not be available, or may require run-time loading.

:class:`_sqlite.JSON` is used automatically whenever the base
:class:`_types.JSON` datatype is used against a SQLite backend.

.. seealso::

    :class:`_types.JSON` - main documentation for the generic
    cross-platform JSON datatype.

The :class:`_sqlite.JSON` type supports persistence of JSON values
as well as the core index operations provided by :class:`_types.JSON`
datatype, by adapting the operations to render the ``JSON_EXTRACT``
function wrapped in the ``JSON_QUOTE`` function at the database level.
Extracted values are quoted in order to ensure that the results are
always JSON string values.


.. versionadded:: 1.3


.. _JSON1: https://www.sqlite.org/json1.html",0,['sqltypes.JSON'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\sqlite\json.py,_FormatTypeMixin,,3,[],0,"['_format_value', 'bind_processor', 'literal_processor']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\sqlite\json.py,JSONIndexType,,1,"['_FormatTypeMixin', 'sqltypes.JSON.JSONIndexType']",0,['_format_value'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\dialects\sqlite\json.py,JSONPathType,,1,"['_FormatTypeMixin', 'sqltypes.JSON.JSONPathType']",0,['_format_value'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\ext\asyncio\exc.py,AsyncMethodRequired,"an API can't be used because its result would not be
compatible with async",0,['exc.InvalidRequestError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\ext\asyncio\exc.py,AsyncContextNotStarted,a startable context manager has not been started.,0,['exc.InvalidRequestError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\ext\asyncio\exc.py,AsyncContextAlreadyStarted,a startable context manager is already started.,0,['exc.InvalidRequestError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sqlalchemy\ext\mypy\plugin.py,SQLAlchemyPlugin,,7,['Plugin'],0,"['get_dynamic_class_hook', 'get_customize_class_mro_hook', 'get_class_decorator_hook', 'get_metaclass_hook', 'get_base_class_hook', 'get_attribute_hook', 'get_additional_deps']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\example\simple.py,Simple,A very basic formatter.,1,['base.FormatterBase'],0,['format'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\example2\fields.py,FieldList,"Format values as a reStructuredText field list.

For example::

  : name1 : value
  : name2 : value
  : name3 : a long value
      will be wrapped with
      a hanging indent",1,['base.FormatterBase'],0,['format'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\manager.py,TestExtensionManager,"ExtensionManager that is explicitly initialized for tests.

.. deprecated:: 0.13

   Use the :func:`make_test_instance` class method of the class
   being replaced by the test instance instead of using this class
   directly.

:param extensions: Pre-configured Extension instances to use
                   instead of loading them from entry points.
:type extensions: list of :class:`~stevedore.extension.Extension`
:param namespace: The namespace for the entry points.
:type namespace: str
:param invoke_on_load: Boolean controlling whether to invoke the
    object returned by the entry point after the driver is loaded.
:type invoke_on_load: bool
:param invoke_args: Positional arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_args: tuple
:param invoke_kwds: Named arguments to pass when invoking
    the object returned by the entry point. Only used if invoke_on_load
    is True.
:type invoke_kwds: dict",2,['extension.ExtensionManager'],0,"['__init__', '_load_plugins']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_dispatch.py,TestDispatch,,6,['utils.TestCase'],0,"['check_dispatch', 'test_dispatch', 'test_dispatch_map_method', 'test_name_dispatch', 'test_name_dispatch_ignore_missing', 'test_name_dispatch_map_method']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_driver.py,TestCallback,,7,['utils.TestCase'],0,"['test_detect_plugins', 'test_call', 'test_driver_property_not_invoked_on_load', 'test_driver_property_invoked_on_load', 'test_no_drivers', 'test_bad_driver', 'test_multiple_drivers']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_enabled.py,TestEnabled,,2,['utils.TestCase'],0,"['test_enabled', 'test_enabled_after_load']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_example_fields.py,TestExampleFields,,2,['utils.TestCase'],0,"['test_simple_items', 'test_long_item']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_example_simple.py,TestExampleSimple,,1,['utils.TestCase'],0,['test_simple_items'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_extension.py,FauxExtension,,2,['object'],0,"['__init__', 'get_args_and_data']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_extension.py,BrokenExtension,,1,['object'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_extension.py,TestCallback,,18,['utils.TestCase'],0,"['test_detect_plugins', 'test_get_by_name', 'test_list_entry_points', 'test_list_entry_points_names', 'test_contains_by_name', 'test_get_by_name_missing', 'test_load_multiple_times_entry_points', 'test_load_multiple_times_plugins', 'test_use_cache', 'test_iterable', 'test_invoke_on_load', 'test_map_return_values', 'test_map_arguments', 'test_map_eats_errors', 'test_map_propagate_exceptions', 'test_map_errors_when_no_plugins', 'test_map_method', 'test_items']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_extension.py,TestLoadRequirementsNewSetuptools,,3,['utils.TestCase'],0,"['setUp', 'test_verify_requirements', 'test_no_verify_requirements']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_extension.py,TestLoadRequirementsOldSetuptools,,3,['utils.TestCase'],0,"['setUp', 'test_verify_requirements', 'test_no_verify_requirements']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_extension.py,TestExtensionProperties,,4,['utils.TestCase'],0,"['setUp', 'test_module_name', 'test_attr', 'test_entry_point_target']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_hook.py,TestHook,,3,['utils.TestCase'],0,"['test_hook', 'test_get_by_name', 'test_get_by_name_missing']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_named.py,TestNamed,,4,['utils.TestCase'],0,"['test_named', 'test_enabled_before_load', 'test_extensions_listed_in_name_order', 'test_load_fail_ignored_when_sorted']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_sphinxext.py,TestSphinxExt,,6,['utils.TestCase'],0,"['setUp', 'test_simple_list', 'test_simple_list_no_docstring', 'test_detailed_list', 'test_detailed_list_format', 'test_detailed_list_no_docstring']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\test_test_manager.py,TestTestManager,,32,['utils.TestCase'],0,"['test_instance_should_use_supplied_extensions', 'test_instance_should_have_default_namespace', 'test_instance_should_use_supplied_namespace', 'test_extension_name_should_be_listed', 'test_iterator_should_yield_extension', 'test_manager_should_allow_name_access', 'test_manager_should_call', 'test_manager_should_call_all', 'test_manager_return_values', 'test_manager_should_eat_exceptions', 'test_manager_should_propagate_exceptions', 'test_named_manager_should_use_supplied_extensions', 'test_named_manager_should_have_default_namespace', 'test_named_manager_should_use_supplied_namespace', 'test_named_manager_should_populate_names', 'test_hook_manager_should_use_supplied_extensions', 'test_hook_manager_should_be_first_extension_name', 'test_hook_manager_should_have_default_namespace', 'test_hook_manager_should_use_supplied_namespace', 'test_hook_manager_should_return_named_extensions', 'test_driver_manager_should_use_supplied_extension', 'test_driver_manager_should_have_default_namespace', 'test_driver_manager_should_use_supplied_namespace', 'test_instance_should_use_driver_name', 'test_instance_call', 'test_instance_driver_property', 'test_enabled_instance_should_use_supplied_extensions', 'test_dispatch_instance_should_use_supplied_extensions', 'test_dispatch_map_should_invoke_filter_for_extensions', 'test_name_dispatch_instance_should_use_supplied_extensions', 'test_name_dispatch_instance_should_build_extension_name_map', 'test_named_dispatch_map_should_invoke_filter_for_extensions']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\stevedore\tests\utils.py,TestCase,,0,['unittest.TestCase'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\assumptions\sathandlers.py,ClassFactRegistry,"Register handlers against classes.

Explanation
===========

``register`` method registers the handler function for a class. Here,
handler function should return a single fact. ``multiregister`` method
registers the handler function for multiple classes. Here, handler function
should return a container of multiple facts.

``registry(expr)`` returns a set of facts for *expr*.

Examples
========

Here, we register the facts for ``Abs``.

>>> from sympy import Abs, Equivalent, Q
>>> from sympy.assumptions.sathandlers import ClassFactRegistry
>>> reg = ClassFactRegistry()
>>> @reg.register(Abs)
... def f1(expr):
...     return Q.nonnegative(expr)
>>> @reg.register(Abs)
... def f2(expr):
...     arg = expr.args[0]
...     return Equivalent(~Q.zero(arg), ~Q.zero(expr))

Calling the registry with expression returns the defined facts for the
expression.

>>> from sympy.abc import x
>>> reg(Abs(x))
{Q.nonnegative(Abs(x)), Equivalent(~Q.zero(x), ~Q.zero(Abs(x)))}

Multiple facts can be registered at once by ``multiregister`` method.

>>> reg2 = ClassFactRegistry()
>>> @reg2.multiregister(Abs)
... def _(expr):
...     arg = expr.args[0]
...     return [Q.even(arg) >> Q.even(expr), Q.odd(arg) >> Q.odd(expr)]
>>> reg2(Abs(x))
{Implies(Q.even(x), Q.even(Abs(x))), Implies(Q.odd(x), Q.odd(Abs(x)))}",5,[],0,"['__init__', 'register', 'multiregister', '__getitem__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\codegen\abstract_nodes.py,List,Represents a (frozen) (Python) list (for code printing purposes).,2,['Tuple'],0,"['__eq__', '__hash__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\codegen\approximations.py,SumApprox,"Approximates sum by neglecting small terms.

Explanation
===========

If terms are expressions which can be determined to be monotonic, then
bounds for those expressions are added.

Parameters
==========

bounds : dict
    Mapping expressions to length 2 tuple of bounds (low, high).
reltol : number
    Threshold for when to ignore a term. Taken relative to the largest
    lower bound among bounds.

Examples
========

>>> from sympy import exp
>>> from sympy.abc import x, y, z
>>> from sympy.codegen.rewriting import optimize
>>> from sympy.codegen.approximations import SumApprox
>>> bounds = {x: (-1, 1), y: (1000, 2000), z: (-10, 3)}
>>> sum_approx3 = SumApprox(bounds, reltol=1e-3)
>>> sum_approx2 = SumApprox(bounds, reltol=1e-2)
>>> sum_approx1 = SumApprox(bounds, reltol=1e-1)
>>> expr = 3*(x + y + exp(z))
>>> optimize(expr, [sum_approx3])
3*(x + y + exp(z))
>>> optimize(expr, [sum_approx2])
3*y + 3*exp(z)
>>> optimize(expr, [sum_approx1])
3*y",4,['Optimization'],0,"['__init__', '__call__', 'query', 'value']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\codegen\approximations.py,SeriesApprox,"Approximates functions by expanding them as a series.

Parameters
==========

bounds : dict
    Mapping expressions to length 2 tuple of bounds (low, high).
reltol : number
    Threshold for when to ignore a term. Taken relative to the largest
    lower bound among bounds.
max_order : int
    Largest order to include in series expansion
n_point_checks : int (even)
    The validity of an expansion (with respect to reltol) is checked at
    discrete points (linearly spaced over the bounds of the variable). The
    number of points used in this numerical check is given by this number.

Examples
========

>>> from sympy import sin, pi
>>> from sympy.abc import x, y
>>> from sympy.codegen.rewriting import optimize
>>> from sympy.codegen.approximations import SeriesApprox
>>> bounds = {x: (-.1, .1), y: (pi-1, pi+1)}
>>> series_approx2 = SeriesApprox(bounds, reltol=1e-2)
>>> series_approx3 = SeriesApprox(bounds, reltol=1e-3)
>>> series_approx8 = SeriesApprox(bounds, reltol=1e-8)
>>> expr = sin(x)*sin(y)
>>> optimize(expr, [series_approx2])
x*(-y + (y - pi)**3/6 + pi)
>>> optimize(expr, [series_approx3])
(-x**3/6 + x)*sin(y)
>>> optimize(expr, [series_approx8])
sin(x)*sin(y)",4,['Optimization'],0,"['__init__', '__call__', 'query', 'value']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\codegen\rewriting.py,Optimization,"Abstract base class for rewriting optimization.

Subclasses should implement ``__call__`` taking an expression
as argument.

Parameters
==========
cost_function : callable returning number
priority : number",2,[],0,"['__init__', 'cheapest']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\codegen\rewriting.py,ReplaceOptim,"Rewriting optimization calling replace on expressions.

Explanation
===========

The instance can be used as a function on expressions for which
it will apply the ``replace`` method (see
:meth:`sympy.core.basic.Basic.replace`).

Parameters
==========

query :
    First argument passed to replace.
value :
    Second argument passed to replace.

Examples
========

>>> from sympy import Symbol
>>> from sympy.codegen.rewriting import ReplaceOptim
>>> from sympy.codegen.cfunctions import exp2
>>> x = Symbol('x')
>>> exp2_opt = ReplaceOptim(lambda p: p.is_Pow and p.base == 2,
...     lambda p: exp2(p.exp))
>>> exp2_opt(2**x)
exp2(x)",2,['Optimization'],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\codegen\rewriting.py,FuncMinusOneOptim,"Specialization of ReplaceOptim for functions evaluating ""f(x) - 1"".

Explanation
===========

Numerical functions which go toward one as x go toward zero is often best
implemented by a dedicated function in order to avoid catastrophic
cancellation. One such example is ``expm1(x)`` in the C standard library
which evaluates ``exp(x) - 1``. Such functions preserves many more
significant digits when its argument is much smaller than one, compared
to subtracting one afterwards.

Parameters
==========

func :
    The function which is subtracted by one.
func_m_1 :
    The specialized function evaluating ``func(x) - 1``.
opportunistic : bool
    When ``True``, apply the transformation as long as the magnitude of the
    remaining number terms decreases. When ``False``, only apply the
    transformation if it completely eliminates the number term.

Examples
========

>>> from sympy import symbols, exp
>>> from sympy.codegen.rewriting import FuncMinusOneOptim
>>> from sympy.codegen.cfunctions import expm1
>>> x, y = symbols('x y')
>>> expm1_opt = FuncMinusOneOptim(exp, expm1)
>>> expm1_opt(exp(x) + 2*exp(5*y) - 3)
expm1(x) + 2*expm1(5*y)",4,['ReplaceOptim'],0,"['__init__', '_group_Add_terms', 'replace_in_Add', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\combinatorics\homomorphisms.py,GroupHomomorphism,"A class representing group homomorphisms. Instantiate using `homomorphism()`.

References
==========

.. [1] Holt, D., Eick, B. and O'Brien, E. (2005). Handbook of computational group theory.",15,[],0,"['__init__', '_invs', 'invert', 'kernel', '_compute_kernel', 'image', '_apply', '__call__', 'is_injective', 'is_surjective', 'is_isomorphism', 'is_trivial', 'compose', 'restrict_to', 'invert_subgroup']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\combinatorics\rewritingsystem_fsm.py,State,"A representation of a state managed by a ``StateMachine``.

Attributes:
    name (instance of FreeGroupElement or string) -- State name which is also assigned to the Machine.
    transisitons (OrderedDict) -- Represents all the transitions of the state object.
    state_type (string) -- Denotes the type (accept/start/dead) of the state.
    rh_rule (instance of FreeGroupElement) -- right hand rule for dead state.
    state_machine (instance of StateMachine object) -- The finite state machine that the state belongs to.",2,[],0,"['__init__', 'add_transition']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\combinatorics\rewritingsystem_fsm.py,StateMachine,"Representation of a finite state machine the manages the states and the transitions of the automaton.

Attributes:
    states (dictionary) -- Collection of all registered `State` objects.
    name (str) -- Name of the state machine.",3,[],0,"['__init__', 'add_state', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\cache.py,_cache,List of cached functions ,2,['list'],0,"['print_cache', 'clear_cache']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\coreerrors.py,BaseCoreError,Base class for core related exceptions. ,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\coreerrors.py,NonCommutativeExpression,Raised when expression didn't have commutative property. ,0,['BaseCoreError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\decorators.py,_SympifyWrapper,Internal class used by sympify_return and sympify_method_args,2,[],0,"['__init__', 'make_wrapped']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\multidimensional.py,vectorize,"Generalizes a function taking scalars to accept multidimensional arguments.

Examples
========

>>> from sympy import vectorize, diff, sin, symbols, Function
>>> x, y, z = symbols('x y z')
>>> f, g, h = list(map(Function, 'fgh'))

>>> @vectorize(0)
... def vsin(x):
...     return sin(x)

>>> vsin([1, x, y])
[sin(1), sin(x), sin(y)]

>>> @vectorize(0, 1)
... def vdiff(f, y):
...     return diff(f, y)

>>> vdiff([f(x, y, z), g(x, y, z), h(x, y, z)], [x, y, z])
[[Derivative(f(x, y, z), x), Derivative(f(x, y, z), y), Derivative(f(x, y, z), z)], [Derivative(g(x, y, z), x), Derivative(g(x, y, z), y), Derivative(g(x, y, z), z)], [Derivative(h(x, y, z), x), Derivative(h(x, y, z), y), Derivative(h(x, y, z), z)]]",2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\parameters.py,_global_parameters,"Thread-local global parameters.

Explanation
===========

This class generates thread-local container for SymPy's global parameters.
Every global parameters must be passed as keyword argument when generating
its instance.
A variable, `global_parameters` is provided as default instance for this class.

WARNING! Although the global parameters are thread-local, SymPy's cache is not
by now.
This may lead to undesired result in multi-threading operations.

Examples
========

>>> from sympy.abc import x
>>> from sympy.core.cache import clear_cache
>>> from sympy.core.parameters import global_parameters as gp

>>> gp.evaluate
True
>>> x+x
2*x

>>> log = []
>>> def f():
...     clear_cache()
...     gp.evaluate = False
...     log.append(x+x)
...     clear_cache()
>>> import threading
>>> thread = threading.Thread(target=f)
>>> thread.start()
>>> thread.join()

>>> print(log)
[x + x]

>>> gp.evaluate
True
>>> x+x
2*x

References
==========

.. [1] https://docs.python.org/3/library/threading.html",2,['local'],0,"['__init__', '__setattr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\parameters.py,evaluate,"Control automatic evaluation

Explanation
===========

This context manager controls whether or not all SymPy functions evaluate
by default.

Note that much of SymPy expects evaluated expressions.  This functionality
is experimental and is unlikely to function as intended on large
expressions.

Examples
========

>>> from sympy import evaluate
>>> from sympy.abc import x
>>> print(x + x)
2*x
>>> with evaluate(False):
...     print(x + x)
x + x",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\rules.py,Transform,"Immutable mapping that can be used as a generic transformation rule.

Parameters
==========

transform : callable
    Computes the value corresponding to any key.

filter : callable, optional
    If supplied, specifies which objects are in the mapping.

Examples
========

>>> from sympy.core.rules import Transform
>>> from sympy.abc import x

This Transform will return, as a value, one more than the key:

>>> add1 = Transform(lambda x: x + 1)
>>> add1[1]
2
>>> add1[x]
x + 1

By default, all values are considered to be in the dictionary. If a filter
is supplied, only the objects for which it returns True are considered as
being in the dictionary:

>>> add1_odd = Transform(lambda x: x + 1, lambda x: x%2 == 1)
>>> 2 in add1_odd
False
>>> add1_odd.get(2, 0)
0
>>> 3 in add1_odd
True
>>> add1_odd[3]
4
>>> add1_odd.get(3, 0)
4",4,[],0,"['__init__', '__contains__', '__getitem__', 'get']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\traversal.py,preorder_traversal,"Do a pre-order traversal of a tree.

This iterator recursively yields nodes that it has visited in a pre-order
fashion. That is, it yields the current node then descends through the
tree breadth-first to yield all of a node's children's pre-order
traversal.


For an expression, the order of the traversal depends on the order of
.args, which in many cases can be arbitrary.

Parameters
==========
node : SymPy expression
    The expression to traverse.
keys : (default None) sort key(s)
    The key(s) used to sort args of Basic objects. When None, args of Basic
    objects are processed in arbitrary order. If key is defined, it will
    be passed along to ordered() as the only key(s) to use to sort the
    arguments; if ``key`` is simply True then the default keys of ordered
    will be used.

Yields
======
subtree : SymPy expression
    All of the subtrees in the tree.

Examples
========

>>> from sympy import preorder_traversal, symbols
>>> x, y, z = symbols('x y z')

The nodes are returned in the order that they are encountered unless key
is given; simply passing key=True will guarantee that the traversal is
unique.

>>> list(preorder_traversal((x + y)*z, keys=None)) # doctest: +SKIP
[z*(x + y), z, x + y, y, x]
>>> list(preorder_traversal((x + y)*z, keys=True))
[z*(x + y), z, x + y, x, y]",5,[],0,"['__init__', '_preorder_traversal', 'skip', '__next__', '__iter__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\crypto\crypto.py,NonInvertibleCipherWarning,A warning raised if the cipher is not invertible.,3,['RuntimeWarning'],0,"['__init__', '__str__', 'warn']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\geometry\exceptions.py,GeometryError,An exception raised by classes in the geometry module.,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\holonomic\holonomicerrors.py,BaseHolonomicError,,1,['Exception'],0,['new'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\holonomic\holonomicerrors.py,NotPowerSeriesError,,2,['BaseHolonomicError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\holonomic\holonomicerrors.py,NotHolonomicError,,2,['BaseHolonomicError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\holonomic\holonomicerrors.py,SingularityError,,2,['BaseHolonomicError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\holonomic\holonomicerrors.py,NotHyperSeriesError,,2,['BaseHolonomicError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\integrals\heurisch.py,BesselTable,"Derivatives of Bessel functions of orders n and n-1
in terms of each other.

See the docstring of DiffCache.",4,[],0,"['__init__', '_create_table', 'diffs', 'has']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\integrals\heurisch.py,DiffCache,"Store for derivatives of expressions.

Explanation
===========

The standard form of the derivative of a Bessel function of order n
contains two Bessel functions of orders n-1 and n+1, respectively.
Such forms cannot be used in parallel Risch algorithm, because
there is a linear recurrence relation between the three functions
while the algorithm expects that functions and derivatives are
represented in terms of algebraically independent transcendentals.

The solution is to take two of the functions, e.g., those of orders
n and n-1, and to express the derivatives in terms of the pair.
To guarantee that the proper form is used the two derivatives are
cached as soon as one is encountered.

Derivatives of other functions are also cached at no extra cost.
All derivatives are with respect to the same variable `x`.",2,[],0,"['__init__', 'get_diff']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\integrals\meijerint.py,_CoeffExpValueError,"Exception raised by _get_coeff_exp, for internal use only.",0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\cartan_type.py,CartanType_generator,Constructor for actually creating things,1,[],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\cartan_type.py,Standard_Cartan,"Concrete base class for Cartan types such as A4, etc",3,['Atom'],0,"['__new__', 'rank', 'series']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\root_system.py,RootSystem,"Represent the root system of a simple Lie algebra

Every simple Lie algebra has a unique root system.  To find the root
system, we first consider the Cartan subalgebra of g, which is the maximal
abelian subalgebra, and consider the adjoint action of g on this
subalgebra.  There is a root system associated with this action. Now, a
root system over a vector space V is a set of finite vectors Phi (called
roots), which satisfy:

1.  The roots span V
2.  The only scalar multiples of x in Phi are x and -x
3.  For every x in Phi, the set Phi is closed under reflection
    through the hyperplane perpendicular to x.
4.  If x and y are roots in Phi, then the projection of y onto
    the line through x is a half-integral multiple of x.

Now, there is a subset of Phi, which we will call Delta, such that:
1.  Delta is a basis of V
2.  Each root x in Phi can be written x = sum k_y y for y in Delta

The elements of Delta are called the simple roots.
Therefore, we see that the simple roots span the root space of a given
simple Lie algebra.

References
==========

.. [1] https://en.wikipedia.org/wiki/Root_system
.. [2] Lie Algebras and Representation Theory - Humphreys",8,['Atom'],0,"['__new__', 'simple_roots', 'all_roots', 'root_space', 'add_simple_roots', 'add_as_roots', 'cartan_matrix', 'dynkin_diagram']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\type_a.py,TypeA,"This class contains the information about
the A series of simple Lie algebras.
====",11,['Standard_Cartan'],0,"['__new__', 'dimension', 'basic_root', 'simple_root', 'positive_roots', 'highest_root', 'roots', 'cartan_matrix', 'basis', 'lie_algebra', 'dynkin_diagram']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\type_b.py,TypeB,,10,['Standard_Cartan'],0,"['__new__', 'dimension', 'basic_root', 'simple_root', 'positive_roots', 'roots', 'cartan_matrix', 'basis', 'lie_algebra', 'dynkin_diagram']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\type_c.py,TypeC,,10,['Standard_Cartan'],0,"['__new__', 'dimension', 'basic_root', 'simple_root', 'positive_roots', 'roots', 'cartan_matrix', 'basis', 'lie_algebra', 'dynkin_diagram']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\type_d.py,TypeD,,10,['Standard_Cartan'],0,"['__new__', 'dimension', 'basic_root', 'simple_root', 'positive_roots', 'roots', 'cartan_matrix', 'basis', 'lie_algebra', 'dynkin_diagram']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\type_e.py,TypeE,,9,['Standard_Cartan'],0,"['__new__', 'dimension', 'basic_root', 'simple_root', 'positive_roots', 'roots', 'cartan_matrix', 'basis', 'dynkin_diagram']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\type_f.py,TypeF,,9,['Standard_Cartan'],0,"['__new__', 'dimension', 'basic_root', 'simple_root', 'positive_roots', 'roots', 'cartan_matrix', 'basis', 'dynkin_diagram']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\type_g.py,TypeG,,8,['Standard_Cartan'],0,"['__new__', 'dimension', 'simple_root', 'positive_roots', 'roots', 'cartan_matrix', 'basis', 'dynkin_diagram']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\liealgebras\weyl_group.py,WeylGroup,"For each semisimple Lie group, we have a Weyl group.  It is a subgroup of
the isometry group of the root system.  Specifically, it's the subgroup
that is generated by reflections through the hyperplanes orthogonal to
the roots.  Therefore, Weyl groups are reflection groups, and so a Weyl
group is a finite Coxeter group.",8,['Atom'],0,"['__new__', 'generators', 'group_order', 'group_name', 'element_order', 'delete_doubles', 'matrix_form', 'coxeter_diagram']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\matrices\exceptions.py,MatrixError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\matrices\exceptions.py,ShapeError,Wrong matrix shape,0,"['ValueError', 'MatrixError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\matrices\exceptions.py,NonSquareMatrixError,,0,['ShapeError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\matrices\exceptions.py,NonInvertibleMatrixError,The matrix in not invertible (division by multidimensional zero error).,0,"['ValueError', 'MatrixError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\matrices\exceptions.py,NonPositiveDefiniteMatrixError,The matrix is not a positive-definite matrix.,0,"['ValueError', 'MatrixError']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\matrices\kind.py,MatrixKind,"Kind for all matrices in SymPy.

Basic class for this kind is ``MatrixBase`` and ``MatrixExpr``,
but any expression representing the matrix can have this.

Parameters
==========

element_kind : Kind
    Kind of the element. Default is
    :class:`sympy.core.kind.NumberKind`,
    which means that the matrix contains only numbers.

Examples
========

Any instance of matrix class has kind ``MatrixKind``:

>>> from sympy import MatrixSymbol
>>> A = MatrixSymbol('A', 2, 2)
>>> A.kind
MatrixKind(NumberKind)

An expression representing a matrix may not be an instance of
the Matrix class, but it will have kind ``MatrixKind``:

>>> from sympy import MatrixExpr, Integral
>>> from sympy.abc import x
>>> intM = Integral(A, x)
>>> isinstance(intM, MatrixExpr)
False
>>> intM.kind
MatrixKind(NumberKind)

Use ``isinstance()`` to check for ``MatrixKind`` without specifying the
element kind. Use ``is`` to check the kind including the element kind:

>>> from sympy import Matrix
>>> from sympy.core import NumberKind
>>> from sympy.matrices import MatrixKind
>>> M = Matrix([1, 2])
>>> isinstance(M.kind, MatrixKind)
True
>>> M.kind is MatrixKind(NumberKind)
True

See Also
========

sympy.core.kind.NumberKind
sympy.core.kind.UndefinedKind
sympy.core.containers.TupleKind
sympy.sets.sets.SetKind",2,['Kind'],0,"['__new__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\matrices\utilities.py,DotProdSimpState,,1,['local'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\conflict.py,AmbiguityWarning,,0,['Warning'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\ntheory\ecm.py,Point,"Montgomery form of Points in an elliptic curve.
In this form, the addition and doubling of points
does not need any y-coordinate information thus
decreasing the number of operations.
Using Montgomery form we try to perform point addition
and doubling in least amount of multiplications.

The elliptic curve used here is of the form
(E : b*y**2*z = x**3 + a*x**2*z + x*z**2).
The a_24 parameter is equal to (a + 2)/4.

References
==========

.. [1] Kris Gaj, Soonhak Kwon, Patrick Baier, Paul Kohlbrenner, Hoang Le, Mohammed Khaleeluddin, Ramakrishna Bachimanchi,
       Implementing the Elliptic Curve Method of Factoring in Reconfigurable Hardware,
       Cryptographic Hardware and Embedded Systems - CHES 2006 (2006), pp. 119-133,
       https://doi.org/10.1007/11894063_10
       https://www.hyperelliptic.org/tanja/SHARCS/talks06/Gaj.pdf",5,[],0,"['__init__', '__eq__', 'add', 'double', 'mont_ladder']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\ntheory\generate.py,Sieve,"A list of prime numbers, implemented as a dynamically
growing sieve of Eratosthenes. When a lookup is requested involving
an odd number that has not been sieved, the sieve is automatically
extended up to that number. Implementation details limit the number of
primes to ``2^32-1``.

Examples
========

>>> from sympy import sieve
>>> sieve._reset() # this line for doctest only
>>> 25 in sieve
False
>>> sieve._list
array('L', [2, 3, 5, 7, 11, 13, 17, 19, 23])",13,[],0,"['__init__', '__repr__', '_reset', 'extend', '_primerange', 'extend_to_no', 'primerange', 'totientrange', 'mobiusrange', 'search', '__contains__', '__iter__', '__getitem__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\ntheory\qs.py,SievePolynomial,,2,[],0,"['__init__', 'eval']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\ntheory\qs.py,FactorBaseElem,"This class stores an element of the `factor_base`.
    ",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\parsing\ast_parser.py,Transform,,4,['NodeTransformer'],0,"['__init__', 'visit_Constant', 'visit_Name', 'visit_Lambda']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\parsing\maxima.py,MaximaHelpers,,7,[],0,"['maxima_expand', 'maxima_float', 'maxima_trigexpand', 'maxima_sum', 'maxima_product', 'maxima_csc', 'maxima_sec']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\parsing\sym_expr.py,SymPyExpression,"Class to store and handle SymPy expressions

This class will hold SymPy Expressions and handle the API for the
conversion to and from different languages.

It works with the C and the Fortran Parser to generate SymPy expressions
which are stored here and which can be converted to multiple language's
source code.

Notes
=====

The module and its API are currently under development and experimental
and can be changed during development.

The Fortran parser does not support numeric assignments, so all the
variables have been Initialized to zero.

The module also depends on external dependencies:

- LFortran which is required to use the Fortran parser
- Clang which is required for the C parser

Examples
========

Example of parsing C code:

>>> from sympy.parsing.sym_expr import SymPyExpression
>>> src = '''
... int a,b;
... float c = 2, d =4;
... '''
>>> a = SymPyExpression(src, 'c')
>>> a.return_expr()
[Declaration(Variable(a, type=intc)),
Declaration(Variable(b, type=intc)),
Declaration(Variable(c, type=float32, value=2.0)),
Declaration(Variable(d, type=float32, value=4.0))]

An example of variable definition:

>>> from sympy.parsing.sym_expr import SymPyExpression
>>> src2 = '''
... integer :: a, b, c, d
... real :: p, q, r, s
... '''
>>> p = SymPyExpression()
>>> p.convert_to_expr(src2, 'f')
>>> p.convert_to_c()
['int a = 0', 'int b = 0', 'int c = 0', 'int d = 0', 'double p = 0.0', 'double q = 0.0', 'double r = 0.0', 'double s = 0.0']

An example of Assignment:

>>> from sympy.parsing.sym_expr import SymPyExpression
>>> src3 = '''
... integer :: a, b, c, d, e
... d = a + b - c
... e = b * d + c * e / a
... '''
>>> p = SymPyExpression(src3, 'f')
>>> p.convert_to_python()
['a = 0', 'b = 0', 'c = 0', 'd = 0', 'e = 0', 'd = a + b - c', 'e = b*d + c*e/a']

An example of function definition:

>>> from sympy.parsing.sym_expr import SymPyExpression
>>> src = '''
... integer function f(a,b)
... integer, intent(in) :: a, b
... integer :: r
... end function
... '''
>>> a = SymPyExpression(src, 'f')
>>> a.convert_to_python()
['def f(a, b):\n   f = 0\n    r = 0\n    return f']",6,[],0,"['__init__', 'convert_to_expr', 'convert_to_python', 'convert_to_c', 'convert_to_fortran', 'return_expr']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\wigner.py,Wigner3j,,1,['Function'],0,['doit'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,BasePolynomialError,Base class for polynomial related exceptions. ,1,['Exception'],0,['new'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,ExactQuotientFailed,,3,['BasePolynomialError'],0,"['__init__', '__str__', 'new']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,PolynomialDivisionFailed,,2,['BasePolynomialError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,OperationNotSupported,,2,['BasePolynomialError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,HeuristicGCDFailed,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,ModularGCDFailed,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,HomomorphismFailed,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,IsomorphismFailed,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,ExtraneousFactors,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,EvaluationFailed,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,RefinementFailed,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,CoercionFailed,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,NotInvertible,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,NotReversible,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,NotAlgebraic,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,DomainError,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,PolynomialError,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,UnificationFailed,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,UnsolvableFactorError,"Raised if ``roots`` is called with strict=True and a polynomial
having a factor whose solutions are not expressible in radicals
is encountered.",0,['BasePolynomialError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,GeneratorsError,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,GeneratorsNeeded,,0,['GeneratorsError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,ComputationFailed,,2,['BasePolynomialError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,UnivariatePolynomialError,,0,['PolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,MultivariatePolynomialError,,0,['PolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,PolificationFailed,,2,['PolynomialError'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,OptionError,,0,['BasePolynomialError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\polyerrors.py,FlagError,,0,['OptionError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\llvmjitcode.py,LLVMJitPrinter,Convert expressions to LLVM IR,10,['Printer'],0,"['__init__', '_add_tmp_var', '_print_Number', '_print_Integer', '_print_Symbol', '_print_Pow', '_print_Mul', '_print_Add', '_print_Function', 'emptyPrinter']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\llvmjitcode.py,LLVMJitCallbackPrinter,,3,['LLVMJitPrinter'],0,"['__init__', '_print_Indexed', '_print_Symbol']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\llvmjitcode.py,LLVMJitCode,,9,[],0,"['__init__', '_from_ctype', '_create_args', '_create_function_base', '_create_param_dict', '_create_function', '_wrap_return', '_convert_expr', '_compile_function']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\llvmjitcode.py,LLVMJitCodeCallback,,3,['LLVMJitCode'],0,"['__init__', '_create_param_dict', '_create_function']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\llvmjitcode.py,CodeSignature,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\python.py,PythonPrinter,A printer which converts an expression into its Python interpretation.,4,"['ReprPrinter', 'StrPrinter']",0,"['__init__', '_print_Function', '_print_Symbol', '_print_module']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\tableform.py,TableForm,"Create a nice table representation of data.

Examples
========

>>> from sympy import TableForm
>>> t = TableForm([[5, 7], [4, 2], [10, 3]])
>>> print(t)
5  7
4  2
10 3

You can use the SymPy's printing system to produce tables in any
format (ascii, latex, html, ...).

>>> print(t.as_latex())
\begin{tabular}{l l}
$5$ & $7$ \\
$4$ & $2$ \\
$10$ & $3$ \\
\end{tabular}",8,[],0,"['__init__', '__repr__', '__str__', 'as_matrix', 'as_str', 'as_latex', '_sympystr', '_latex']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\series\gruntz.py,SubsSet,"Stores (expr, dummy) pairs, and how to rewrite expr-s.

Explanation
===========

The gruntz algorithm needs to rewrite certain expressions in term of a new
variable w. We cannot use subs, because it is just too smart for us. For
example::

    > Omega=[exp(exp(_p - exp(-_p))/(1 - 1/_p)), exp(exp(_p))]
    > O2=[exp(-exp(_p) + exp(-exp(-_p))*exp(_p)/(1 - 1/_p))/_w, 1/_w]
    > e = exp(exp(_p - exp(-_p))/(1 - 1/_p)) - exp(exp(_p))
    > e.subs(Omega[0],O2[0]).subs(Omega[1],O2[1])
    -1/w + exp(exp(p)*exp(-exp(-p))/(1 - 1/p))

is really not what we want!

So we do it the hard way and keep track of all the things we potentially
want to substitute by dummy variables. Consider the expression::

    exp(x - exp(-x)) + exp(x) + x.

The mrv set is {exp(x), exp(-x), exp(x - exp(-x))}.
We introduce corresponding dummy variables d1, d2, d3 and rewrite::

    d3 + d1 + x.

This class first of all keeps track of the mapping expr->variable, i.e.
will at this stage be a dictionary::

    {exp(x): d1, exp(-x): d2, exp(x - exp(-x)): d3}.

[It turns out to be more convenient this way round.]
But sometimes expressions in the mrv set have other expressions from the
mrv set as subexpressions, and we need to keep track of that as well. In
this case, d3 is really exp(x - d2), so rewrites at this stage is::

    {d3: exp(x-d2)}.

The function rewrite uses all this information to correctly rewrite our
expression in terms of w. In this case w can be chosen to be exp(-x),
i.e. d2. The correct rewriting then is::

    exp(-w)/w + 1/w + x.",7,['dict'],0,"['__init__', '__repr__', '__getitem__', 'do_subs', 'meets', 'union', 'copy']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\simplify\sqrtdenest.py,SqrtdenestStopIteration,,0,['StopIteration'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\solvers\polysys.py,SolveFailed,Raised when solver's conditions were not met. ,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\solvers\simplex.py,UnboundedLPError,"A linear programing problem is said to be unbounded if its objective
function can assume arbitrarily large values.

Example
=======

Suppose you want to maximize
    2x
subject to
    x >= 0

There's no upper limit that 2x can take.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\solvers\simplex.py,InfeasibleLPError,"A linear programing problem is considered infeasible if its
constraint set is empty. That is, if the set of all vectors
satisfying the contraints is empty, then the problem is infeasible.

Example
=======

Suppose you want to maximize
    x
subject to
    x >= 10
    x <= 9

No x can satisfy those constraints.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\solvers\solveset.py,NonlinearError,Raised when unexpectedly encountering nonlinear equations,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\solvers\solveset.py,_SolveTrig1Error,Raised when _solve_trig1 heuristics do not apply,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\tensor\index_methods.py,IndexConformanceException,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\unify\core.py,Compound,"A little class to represent an interior node in the tree

This is analogous to SymPy.Basic for non-Atoms",4,[],0,"['__init__', '__eq__', '__hash__', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\unify\core.py,Variable,A Wild token ,4,[],0,"['__init__', '__eq__', '__hash__', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\unify\core.py,CondVariable,"A wild token that matches conditionally.

arg   - a wild token.
valid - an additional constraining function on a match.",4,[],0,"['__init__', '__eq__', '__hash__', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\utilities\decorator.py,no_attrs_in_subclass,"Don't 'inherit' certain attributes from a base class

>>> from sympy.utilities.decorator import no_attrs_in_subclass

>>> class A(object):
...     x = 'test'

>>> A.x = no_attrs_in_subclass(A, A.x)

>>> class B(A):
...     pass

>>> hasattr(A, 'x')
True
>>> hasattr(B, 'x')
False",2,[],0,"['__init__', '__get__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\utilities\iterables.py,NotIterable,"Use this as mixin when creating a class which is not supposed to
return true when iterable() is called on its instances because
calling list() on the instance, for example, would result in
an infinite loop.",0,[],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\utilities\misc.py,Undecidable,,0,['ValueError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\vector\operators.py,Gradient,"Represents unevaluated Gradient.

Examples
========

>>> from sympy.vector import CoordSys3D, Gradient
>>> R = CoordSys3D('R')
>>> s = R.x*R.y*R.z
>>> Gradient(s)
Gradient(R.x*R.y*R.z)",2,['Expr'],0,"['__new__', 'doit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\vector\operators.py,Divergence,"Represents unevaluated Divergence.

Examples
========

>>> from sympy.vector import CoordSys3D, Divergence
>>> R = CoordSys3D('R')
>>> v = R.y*R.z*R.i + R.x*R.z*R.j + R.x*R.y*R.k
>>> Divergence(v)
Divergence(R.y*R.z*R.i + R.x*R.z*R.j + R.x*R.y*R.k)",2,['Expr'],0,"['__new__', 'doit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\vector\operators.py,Curl,"Represents unevaluated Curl.

Examples
========

>>> from sympy.vector import CoordSys3D, Curl
>>> R = CoordSys3D('R')
>>> v = R.y*R.z*R.i + R.x*R.z*R.j + R.x*R.y*R.k
>>> Curl(v)
Curl(R.y*R.z*R.i + R.x*R.z*R.j + R.x*R.y*R.k)",2,['Expr'],0,"['__new__', 'doit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\vector\operators.py,Laplacian,"Represents unevaluated Laplacian.

Examples
========

>>> from sympy.vector import CoordSys3D, Laplacian
>>> R = CoordSys3D('R')
>>> v = 3*R.x**3*R.y**2*R.z**3
>>> Laplacian(v)
Laplacian(3*R.x**3*R.y**2*R.z**3)",2,['Expr'],0,"['__new__', 'doit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\tests\test_constructor_postprocessor.py,SymbolInMulOnce,,0,['Symbol'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\tests\test_constructor_postprocessor.py,SymbolRemovesOtherSymbols,,0,['Symbol'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\tests\test_constructor_postprocessor.py,SubclassSymbolInMulOnce,,0,['SymbolInMulOnce'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\core\tests\test_constructor_postprocessor.py,SubclassSymbolRemovesOtherSymbols,,0,['SymbolRemovesOtherSymbols'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\integrals\tests\test_risch.py,_TestingException,Dummy Exception class for testing.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\matrices\expressions\dotproduct.py,DotProduct,"Dot product of vector matrices

The input should be two 1 x n or n x 1 matrices. The output represents the
scalar dotproduct.

This is similar to using MatrixElement and MatMul, except DotProduct does
not require that one vector to be a row vector and the other vector to be
a column vector.

>>> from sympy import MatrixSymbol, DotProduct
>>> A = MatrixSymbol('A', 1, 3)
>>> B = MatrixSymbol('B', 1, 3)
>>> DotProduct(A, B)
DotProduct(A, B)
>>> DotProduct(A, B).doit()
A[0, 0]*B[0, 0] + A[0, 1]*B[0, 1] + A[0, 2]*B[0, 2]",2,['Expr'],0,"['__new__', 'doit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\tests\test_conflict.py,A,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\tests\test_conflict.py,B,,0,['A'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\tests\test_conflict.py,C,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\tests\test_core.py,A,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\tests\test_core.py,B,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\tests\test_core.py,C,,0,['A'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\tests\test_core.py,D,,0,['C'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\multipledispatch\tests\test_core.py,E,,0,['C'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\parsing\latex\errors.py,LaTeXParsingError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\parsing\tests\test_custom_latex.py,CustomTransformer,,1,['TransformToSymPyExpr'],0,['number'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\parsing\autolev\_antlr\autolevlistener.py,AutolevListener,,90,['ParseTreeListener'],0,"['enterProg', 'exitProg', 'enterStat', 'exitStat', 'enterVecAssign', 'exitVecAssign', 'enterIndexAssign', 'exitIndexAssign', 'enterRegularAssign', 'exitRegularAssign', 'enterEquals', 'exitEquals', 'enterIndex', 'exitIndex', 'enterDiff', 'exitDiff', 'enterFunctionCall', 'exitFunctionCall', 'enterVarDecl', 'exitVarDecl', 'enterVarType', 'exitVarType', 'enterVarDecl2', 'exitVarDecl2', 'enterRanges', 'exitRanges', 'enterMassDecl', 'exitMassDecl', 'enterMassDecl2', 'exitMassDecl2', 'enterInertiaDecl', 'exitInertiaDecl', 'enterMatrix', 'exitMatrix', 'enterMatrixInOutput', 'exitMatrixInOutput', 'enterCodeCommands', 'exitCodeCommands', 'enterSettings', 'exitSettings', 'enterUnits', 'exitUnits', 'enterInputs', 'exitInputs', 'enterId_diff', 'exitId_diff', 'enterInputs2', 'exitInputs2', 'enterOutputs', 'exitOutputs', 'enterOutputs2', 'exitOutputs2', 'enterCodegen', 'exitCodegen', 'enterCommands', 'exitCommands', 'enterVec', 'exitVec', 'enterParens', 'exitParens', 'enterVectorOrDyadic', 'exitVectorOrDyadic', 'enterExponent', 'exitExponent', 'enterMulDiv', 'exitMulDiv', 'enterAddSub', 'exitAddSub', 'enterFloat', 'exitFloat', 'enterInt', 'exitInt', 'enterIdEqualsExpr', 'exitIdEqualsExpr', 'enterNegativeOne', 'exitNegativeOne', 'enterFunction', 'exitFunction', 'enterRangess', 'exitRangess', 'enterColon', 'exitColon', 'enterId', 'exitId', 'enterExp', 'exitExp', 'enterMatrices', 'exitMatrices', 'enterIndexing', 'exitIndexing']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\parsing\latex\lark\latex_parser.py,LarkLaTeXParser,"Class for converting input `\mathrm{\LaTeX}` strings into SymPy Expressions.
It holds all the necessary internal data for doing so, and exposes hooks for
customizing its behavior.

Parameters
==========

print_debug_output : bool, optional

    If set to ``True``, prints debug output to the logger. Defaults to ``False``.

transform : bool, optional

    If set to ``True``, the class runs the Transformer class on the parse tree
    generated by running ``Lark.parse`` on the input string. Defaults to ``True``.

    Setting it to ``False`` can help with debugging the `\mathrm{\LaTeX}` grammar.

grammar_file : str, optional

    The path to the grammar file that the parser should use. If set to ``None``,
    it uses the default grammar, which is in ``grammar/latex.lark``, relative to
    the ``sympy/parsing/latex/lark/`` directory.

transformer : str, optional

    The name of the Transformer class to use. If set to ``None``, it uses the
    default transformer class, which is :py:func:`TransformToSymPyExpr`.",2,[],0,"['__init__', 'doparse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\mechanics\linearize.py,Linearizer,"This object holds the general model form for a dynamic system. This
model is used for computing the linearized form of the system, while
properly dealing with constraints leading to  dependent coordinates and
speeds. The notation and method is described in [1]_.

Attributes
==========

f_0, f_1, f_2, f_3, f_4, f_c, f_v, f_a : Matrix
    Matrices holding the general system form.
q, u, r : Matrix
    Matrices holding the generalized coordinates, speeds, and
    input vectors.
q_i, u_i : Matrix
    Matrices of the independent generalized coordinates and speeds.
q_d, u_d : Matrix
    Matrices of the dependent generalized coordinates and speeds.
perm_mat : Matrix
    Permutation matrix such that [q_ind, u_ind]^T = perm_mat*[q, u]^T

References
==========

.. [1] D. L. Peterson, G. Gede, and M. Hubbard, ""Symbolic linearization of
       equations of motion of constrained multibody systems,"" Multibody
       Syst Dyn, vol. 33, no. 2, pp. 143-161, Feb. 2015, doi:
       10.1007/s11044-014-9436-5.",6,[],0,"['__init__', '_setup', '_form_permutation_matrices', '_form_coefficient_matrices', '_form_block_matrices', 'linearize']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\dagger.py,Dagger,"General Hermitian conjugate operation.

Explanation
===========

Take the Hermetian conjugate of an argument [1]_. For matrices this
operation is equivalent to transpose and complex conjugate [2]_.

Parameters
==========

arg : Expr
    The SymPy expression that we want to take the dagger of.
evaluate : bool
    Whether the resulting expression should be directly evaluated.

Examples
========

Daggering various quantum objects:

    >>> from sympy.physics.quantum.dagger import Dagger
    >>> from sympy.physics.quantum.state import Ket, Bra
    >>> from sympy.physics.quantum.operator import Operator
    >>> Dagger(Ket('psi'))
    <psi|
    >>> Dagger(Bra('phi'))
    |phi>
    >>> Dagger(Operator('A'))
    Dagger(A)

Inner and outer products::

    >>> from sympy.physics.quantum import InnerProduct, OuterProduct
    >>> Dagger(InnerProduct(Bra('a'), Ket('b')))
    <b|a>
    >>> Dagger(OuterProduct(Ket('a'), Bra('b')))
    |b><a|

Powers, sums and products::

    >>> A = Operator('A')
    >>> B = Operator('B')
    >>> Dagger(A*B)
    Dagger(B)*Dagger(A)
    >>> Dagger(A+B)
    Dagger(A) + Dagger(B)
    >>> Dagger(A**2)
    Dagger(A)**2

Dagger also seamlessly handles complex numbers and matrices::

    >>> from sympy import Matrix, I
    >>> m = Matrix([[1,I],[2,I]])
    >>> m
    Matrix([
    [1, I],
    [2, I]])
    >>> Dagger(m)
    Matrix([
    [ 1,  2],
    [-I, -I]])

References
==========

.. [1] https://en.wikipedia.org/wiki/Hermitian_adjoint
.. [2] https://en.wikipedia.org/wiki/Hermitian_transpose",2,['adjoint'],0,"['__new__', '__mul__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\matrixcache.py,MatrixCache,"A cache for small matrices in different formats.

This class takes small matrices in the standard ``sympy.Matrix`` format,
and then converts these to both ``numpy.matrix`` and
``scipy.sparse.csr_matrix`` matrices. These matrices are then stored for
future recovery.",7,[],0,"['__init__', 'cache_matrix', 'get_matrix', '_store_matrix', '_sympy_matrix', '_numpy_matrix', '_scipy_sparse_matrix']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\qasm.py,Qasm,"Class to form objects from Qasm lines

>>> from sympy.physics.quantum.qasm import Qasm
>>> q = Qasm('qubit q0', 'qubit q1', 'h q0', 'cnot q0,q1')
>>> q.get_circuit()
CNOT(1,0)*H(1)
>>> q = Qasm('qubit q0', 'qubit q1', 'cnot q0,q1', 'cnot q1,q0', 'cnot q0,q1')
>>> q.get_circuit()
CNOT(1,0)*CNOT(0,1)*CNOT(1,0)",23,[],0,"['__init__', 'add', 'get_circuit', 'get_labels', 'plot', 'qubit', 'indices', 'index', 'nop', 'x', 'z', 'h', 's', 't', 'measure', 'cnot', 'swap', 'cphase', 'toffoli', 'cx', 'cz', 'defbox', 'qdef']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\vector\printing.py,VectorStrPrinter,String Printer for vector expressions. ,2,['StrPrinter'],0,"['_print_Derivative', '_print_Function']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\vector\printing.py,VectorStrReprPrinter,String repr printer for vector expressions.,1,['VectorStrPrinter'],0,['_print_str'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\vector\printing.py,VectorLatexPrinter,Latex Printer for vector expressions. ,2,['LatexPrinter'],0,"['_print_Function', '_print_Derivative']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\vector\printing.py,VectorPrettyPrinter,Pretty Printer for vectorialexpressions. ,2,['PrettyPrinter'],0,"['_print_Derivative', '_print_Function']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\tests\test_anticommutator.py,Foo,,1,['Operator'],0,['_eval_anticommutator_Bar'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\tests\test_anticommutator.py,Bar,,0,['Operator'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\tests\test_anticommutator.py,Tam,,1,['Operator'],0,['_eval_anticommutator_Foo'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\tests\test_commutator.py,Foo,,1,['Operator'],0,['_eval_commutator_Bar'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\tests\test_commutator.py,Bar,,0,['Operator'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\tests\test_commutator.py,Tam,,1,['Operator'],0,['_eval_commutator_Foo'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\tests\test_dagger.py,Foo,,1,['Expr'],0,['_eval_adjoint'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\physics\quantum\tests\test_qapply.py,Foo,,1,['Operator'],0,['_apply_operator_JzKet'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\plotting\pygletplot\plot_axes.py,PlotAxes,,8,['PlotObject'],0,"['__init__', 'reset_resources', 'reset_bounding_box', 'draw', 'adjust_bounds', '_recalculate_axis_ticks', 'toggle_visible', 'toggle_colors']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\plotting\pygletplot\plot_axes.py,PlotAxesBase,,6,['PlotObject'],0,"['__init__', 'draw', 'draw_background', 'draw_axis', 'draw_text', 'draw_line']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\plotting\pygletplot\plot_axes.py,PlotAxesOrdinate,,6,['PlotAxesBase'],0,"['__init__', 'draw_axis', 'draw_axis_line', 'draw_axis_line_labels', 'draw_tick_line', 'draw_tick_line_label']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\plotting\pygletplot\plot_axes.py,PlotAxesFrame,,3,['PlotAxesBase'],0,"['__init__', 'draw_background', 'draw_axis']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\plotting\pygletplot\plot_window.py,PlotWindow,,6,['ManagedWindow'],0,"['__init__', 'setup', 'on_resize', 'update', 'draw', 'update_caption']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\plotting\tests\test_plot.py,DummyBackendNotOk,"Used to verify if users can create their own backends.
This backend is meant to raise NotImplementedError for methods `show`,
`save`, `close`.",1,['Plot'],0,['__new__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\plotting\tests\test_plot.py,DummyBackendOk,"Used to verify if users can create their own backends.
This backend is meant to pass all tests.",4,['Plot'],0,"['__new__', 'show', 'save', 'close']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\plotting\backends\textbackend\text.py,TextBackend,,3,['base_backend.Plot'],0,"['__init__', 'show', 'close']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\domains\groundtypes.py,_GMPYInteger,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\domains\groundtypes.py,_GMPYRational,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMError,Base class for errors raised by DomainMatrix,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMBadInputError,list of lists is inconsistent with shape,0,['DMError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMDomainError,domains do not match,0,['DMError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMNotAField,domain is not a field,0,['DMDomainError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMFormatError,mixed dense/sparse not supported,0,['DMError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMNonInvertibleMatrixError,The matrix in not invertible,0,['DMError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMRankError,matrix does not have expected rank,0,['DMError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMShapeError,shapes are inconsistent,0,['DMError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMNonSquareMatrixError,The matrix is not square,0,['DMShapeError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\exceptions.py,DMValueError,The value passed is invalid,0,['DMError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\_typing.py,RingElement,"A ring element.

Must support ``+``, ``-``, ``*``, ``**`` and ``-``.",5,['Protocol'],0,"['__add__', '__sub__', '__mul__', '__pow__', '__neg__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\numberfields\exceptions.py,ClosureFailure,"Signals that a :py:class:`ModuleElement` which we tried to represent in a
certain :py:class:`Module` cannot in fact be represented there.

Examples
========

>>> from sympy.polys import Poly, cyclotomic_poly, ZZ
>>> from sympy.polys.matrices import DomainMatrix
>>> from sympy.polys.numberfields.modules import PowerBasis, to_col
>>> T = Poly(cyclotomic_poly(5))
>>> A = PowerBasis(T)
>>> B = A.submodule_from_matrix(2 * DomainMatrix.eye(4, ZZ))

Because we are in a cyclotomic field, the power basis ``A`` is an integral
basis, and the submodule ``B`` is just the ideal $(2)$. Therefore ``B`` can
represent an element having all even coefficients over the power basis:

>>> a1 = A(to_col([2, 4, 6, 8]))
>>> print(B.represent(a1))
DomainMatrix([[1], [2], [3], [4]], (4, 1), ZZ)

but ``B`` cannot represent an element with an odd coefficient:

>>> a2 = A(to_col([1, 2, 2, 2]))
>>> B.represent(a2)
Traceback (most recent call last):
...
ClosureFailure: Element in QQ-span but not ZZ-span of this basis.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\numberfields\exceptions.py,StructureError,"Represents cases in which an algebraic structure was expected to have a
certain property, or be of a certain type, but was not.",0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\numberfields\exceptions.py,MissingUnityError,Structure should contain a unity element but does not.,0,['StructureError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\numberfields\galoisgroups.py,MaxTriesException,,0,['GaloisGroupException'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\numberfields\utilities.py,AlgIntPowers,"Compute the powers of an algebraic integer.

Explanation
===========

Given an algebraic integer $\theta$ by its monic irreducible polynomial
``T`` over :ref:`ZZ`, this class computes representations of arbitrarily
high powers of $\theta$, as :ref:`ZZ`-linear combinations over
$\{1, \theta, \ldots, \theta^{n-1}\}$, where $n = \deg(T)$.

The representations are computed using the linear recurrence relations for
powers of $\theta$, derived from the polynomial ``T``. See [1], Sec. 4.2.2.

Optionally, the representations may be reduced with respect to a modulus.

Examples
========

>>> from sympy import Poly, cyclotomic_poly
>>> from sympy.polys.numberfields.utilities import AlgIntPowers
>>> T = Poly(cyclotomic_poly(5))
>>> zeta_pow = AlgIntPowers(T)
>>> print(zeta_pow[0])
[1, 0, 0, 0]
>>> print(zeta_pow[1])
[0, 1, 0, 0]
>>> print(zeta_pow[4])  # doctest: +SKIP
[-1, -1, -1, -1]
>>> print(zeta_pow[24])  # doctest: +SKIP
[-1, -1, -1, -1]

References
==========

.. [1] Cohen, H. *A Course in Computational Algebraic Number Theory.*",6,[],0,"['__init__', 'red', '__rmod__', 'compute_up_through', 'get', '__getitem__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\polys\matrices\tests\test_xxm.py,_Sliced,,1,[],0,['__getitem__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\tests\test_lambdarepr.py,CustomPrintedObject,,5,['Expr'],0,"['_lambdacode', '_tensorflowcode', '_numpycode', '_numexprcode', '_mpmathcode']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\tests\test_latex.py,lowergamma,,0,['sym.lowergamma'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\tests\test_pycode.py,CustomPrintedObject,,2,['Expr'],0,"['_numpycode', '_mpmathcode']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\printing\pretty\tests\test_pretty.py,lowergamma,,0,['sym.lowergamma'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\solvers\ode\systems.py,ODEOrderError,Raised by linear_ode_to_matrix if the system has the wrong order,0,['ValueError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\solvers\ode\systems.py,ODENonlinearError,Raised by linear_ode_to_matrix if the system is nonlinear,0,['NonlinearError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\strategies\tests\test_traverse.py,Basic2,,0,['Basic'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\tensor\array\arrayop.py,Flatten,"Flatten an iterable object to a list in a lazy-evaluation way.

Notes
=====

This class is an iterator with which the memory cost can be economised.
Optimisation has been considered to ameliorate the performance for some
specific data types like DenseNDimArray and SparseNDimArray.

Examples
========

>>> from sympy.tensor.array.arrayop import Flatten
>>> from sympy.tensor.array import Array
>>> A = Array(range(6)).reshape(2, 3)
>>> Flatten(A)
Flatten([[0, 1, 2], [3, 4, 5]])
>>> [i for i in Flatten(A)]
[0, 1, 2, 3, 4, 5]",5,['Printable'],0,"['__init__', '__iter__', '__next__', 'next', '_sympystr']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\tensor\array\mutable_ndim_array.py,MutableNDimArray,,3,['NDimArray'],0,"['as_immutable', 'as_mutable', '_sympy_']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\tensor\array\expressions\from_array_to_indexed.py,_ConvertArrayToIndexed,,2,[],0,"['__init__', 'do_convert']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\testing\tests\test_code_quality.py,_Visit,"return the line number corresponding to the
line on which a bare expression appears if it is a binary op
or a comparison that is not in a with block.

EXAMPLES
========

>>> import ast
>>> class _Visit(ast.NodeVisitor):
...     def visit_Expr(self, node):
...         if isinstance(node.value, (ast.BinOp, ast.Compare)):
...             print(node.lineno)
...     def visit_With(self, node):
...         pass  # no checking there
...
>>> code='''x = 1    # line 1
... for i in range(3):
...     x == 2       # <-- 3
... if x == 2:
...     x == 3       # <-- 5
...     x + 1        # <-- 6
...     x = 1
...     if x == 1:
...         print(1)
... while x != 1:
...     x == 1       # <-- 11
... with raises(TypeError):
...     c == 1
...     raise TypeError
... assert x == 1
... '''
>>> _Visit().visit(ast.parse(code))
3
5
6
11",2,['ast.NodeVisitor'],0,"['visit_Expr', 'visit_With']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\utilities\_compilation\util.py,CompilerNotFoundError,,0,['FileNotFoundError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\sympy\utilities\_compilation\util.py,CompileError,Failure to compile one or more C/C++ source files.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\tenacity\asyncio\retry.py,async_retry_base,Abstract base class for async retry strategies.,4,['retry_base'],0,"['__and__', '__rand__', '__or__', '__ror__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\tenacity\asyncio\retry.py,retry_if_exception,Retry strategy that retries if an exception verifies a predicate.,1,['async_retry_base'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\tenacity\asyncio\retry.py,retry_if_result,Retries if the result verifies a predicate.,1,['async_retry_base'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\tenacity\asyncio\retry.py,retry_any,Retries if any of the retries condition is valid.,1,['async_retry_base'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\tenacity\asyncio\retry.py,retry_all,Retries if all the retries condition are valid.,1,['async_retry_base'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\tenacity\asyncio\__init__.py,AsyncRetrying,,5,['BaseRetrying'],0,"['__init__', '_add_action_func', '__iter__', '__aiter__', 'wraps']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\amp\autocast_mode.py,autocast,"Instances of :class:`autocast` serve as context managers or decorators that
allow regions of your script to run in mixed precision.

In these regions, ops run in an op-specific dtype chosen by autocast
to improve performance while maintaining accuracy.
See the :ref:`Autocast Op Reference<autocast-op-reference>` for details.

When entering an autocast-enabled region, Tensors may be any type.
You should not call ``half()`` or ``bfloat16()`` on your model(s) or inputs when using autocasting.

:class:`autocast` should wrap only the forward pass(es) of your network, including the loss
computation(s).  Backward passes under autocast are not recommended.
Backward ops run in the same type that autocast used for corresponding forward ops.

Example for CUDA Devices::

    # Creates model and optimizer in default precision
    model = Net().cuda()
    optimizer = optim.SGD(model.parameters(), ...)

    for input, target in data:
        optimizer.zero_grad()

        # Enables autocasting for the forward pass (model + loss)
        with torch.autocast(device_type=""cuda""):
            output = model(input)
            loss = loss_fn(output, target)

        # Exits the context manager before backward()
        loss.backward()
        optimizer.step()

See the :ref:`Automatic Mixed Precision examples<amp-examples>` for usage (along with gradient scaling)
in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).

:class:`autocast` can also be used as a decorator, e.g., on the ``forward`` method of your model::

    class AutocastModel(nn.Module):
        ...
        @torch.autocast(device_type=""cuda"")
        def forward(self, input):
            ...

Floating-point Tensors produced in an autocast-enabled region may be ``float16``.
After returning to an autocast-disabled region, using them with floating-point
Tensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)
produced in the autocast region back to ``float32`` (or other dtype if desired).
If a Tensor from the autocast region is already ``float32``, the cast is a no-op,
and incurs no additional overhead.
CUDA Example::

    # Creates some tensors in default dtype (here assumed to be float32)
    a_float32 = torch.rand((8, 8), device=""cuda"")
    b_float32 = torch.rand((8, 8), device=""cuda"")
    c_float32 = torch.rand((8, 8), device=""cuda"")
    d_float32 = torch.rand((8, 8), device=""cuda"")

    with torch.autocast(device_type=""cuda""):
        # torch.mm is on autocast's list of ops that should run in float16.
        # Inputs are float32, but the op runs in float16 and produces float16 output.
        # No manual casts are required.
        e_float16 = torch.mm(a_float32, b_float32)
        # Also handles mixed input types
        f_float16 = torch.mm(d_float32, e_float16)

    # After exiting autocast, calls f_float16.float() to use with d_float32
    g_float32 = torch.mm(d_float32, f_float16.float())

CPU Training Example::

    # Creates model and optimizer in default precision
    model = Net()
    optimizer = optim.SGD(model.parameters(), ...)

    for epoch in epochs:
        for input, target in data:
            optimizer.zero_grad()

            # Runs the forward pass with autocasting.
            with torch.autocast(device_type=""cpu"", dtype=torch.bfloat16):
                output = model(input)
                loss = loss_fn(output, target)

            loss.backward()
            optimizer.step()


CPU Inference Example::

    # Creates model in default precision
    model = Net().eval()

    with torch.autocast(device_type=""cpu"", dtype=torch.bfloat16):
        for input in data:
            # Runs the forward pass with autocasting.
            output = model(input)

CPU Inference Example with Jit Trace::

    class TestModel(nn.Module):
        def __init__(self, input_size, num_classes):
            super().__init__()
            self.fc1 = nn.Linear(input_size, num_classes)
        def forward(self, x):
            return self.fc1(x)

    input_size = 2
    num_classes = 2
    model = TestModel(input_size, num_classes).eval()

    # For now, we suggest to disable the Jit Autocast Pass,
    # As the issue: https://github.com/pytorch/pytorch/issues/75956
    torch._C._jit_set_autocast_mode(False)

    with torch.cpu.amp.autocast(cache_enabled=False):
        model = torch.jit.trace(model, torch.randn(1, input_size))
    model = torch.jit.freeze(model)
    # Models Run
    for _ in range(3):
        model(torch.randn(1, input_size))

Type mismatch errors *in* an autocast-enabled region are a bug; if this is what you observe,
please file an issue.

``autocast(enabled=False)`` subregions can be nested in autocast-enabled regions.
Locally disabling autocast can be useful, for example, if you want to force a subregion
to run in a particular ``dtype``.  Disabling autocast gives you explicit control over
the execution type.  In the subregion, inputs from the surrounding region
should be cast to ``dtype`` before use::

    # Creates some tensors in default dtype (here assumed to be float32)
    a_float32 = torch.rand((8, 8), device=""cuda"")
    b_float32 = torch.rand((8, 8), device=""cuda"")
    c_float32 = torch.rand((8, 8), device=""cuda"")
    d_float32 = torch.rand((8, 8), device=""cuda"")

    with torch.autocast(device_type=""cuda""):
        e_float16 = torch.mm(a_float32, b_float32)
        with torch.autocast(device_type=""cuda"", enabled=False):
            # Calls e_float16.float() to ensure float32 execution
            # (necessary because e_float16 was created in an autocasted region)
            f_float32 = torch.mm(c_float32, e_float16.float())

        # No manual casts are required when re-entering the autocast-enabled region.
        # torch.mm again runs in float16 and produces float16 output, regardless of input types.
        g_float16 = torch.mm(d_float32, f_float32)

The autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator
must be invoked in that thread.  This affects :class:`torch.nn.DataParallel` and
:class:`torch.nn.parallel.DistributedDataParallel` when used with more than one GPU per process
(see :ref:`Working with Multiple GPUs<amp-multigpu>`).

Args:
    device_type(str, required):  Device type to use. Possible values are: 'cuda', 'cpu', 'xpu' and 'hpu'.
                                 The type is the same as the `type` attribute of a :class:`torch.device`.
                                 Thus, you may obtain the device type of a tensor using `Tensor.device.type`.
    enabled(bool, optional):  Whether autocasting should be enabled in the region.
        Default: ``True``
    dtype(torch_dtype, optional):  Data type for ops run in autocast. It uses the default value
        (``torch.float16`` for CUDA and ``torch.bfloat16`` for CPU), given by
        :func:`~torch.get_autocast_dtype`, if :attr:`dtype` is ``None``.
        Default: ``None``
    cache_enabled(bool, optional):  Whether the weight cache inside autocast should be enabled.
        Default: ``True``",4,[],0,"['__init__', '__enter__', '__exit__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\anomaly_mode.py,detect_anomaly,"Context-manager that enable anomaly detection for the autograd engine.

This does two things:

- Running the forward pass with detection enabled will allow the backward
  pass to print the traceback of the forward operation that created the failing
  backward function.
- If ``check_nan`` is ``True``, any backward computation that generate ""nan""
  value will raise an error. Default ``True``.

.. warning::
    This mode should be enabled only for debugging as the different tests
    will slow down your program execution.

Example:
    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ANOMALY)
    >>> import torch
    >>> from torch import autograd
    >>> class MyFunc(autograd.Function):
    ...     @staticmethod
    ...     def forward(ctx, inp):
    ...         return inp.clone()
    ...     @staticmethod
    ...     def backward(ctx, gO):
    ...         # Error during the backward pass
    ...         raise RuntimeError(""Some error in backward"")
    ...         return gO.clone()
    >>> def run_fn(a):
    ...     out = MyFunc.apply(a)
    ...     return out.sum()
    >>> inp = torch.rand(10, 10, requires_grad=True)
    >>> out = run_fn(inp)
    >>> out.backward()
        Traceback (most recent call last):
          File ""<stdin>"", line 1, in <module>
          File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward
            torch.autograd.backward(self, gradient, retain_graph, create_graph)
          File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward
            allow_unreachable=True)  # allow_unreachable flag
          File ""/your/pytorch/install/torch/autograd/function.py"", line 76, in apply
            return self._forward_cls.backward(self, *args)
          File ""<stdin>"", line 8, in backward
        RuntimeError: Some error in backward
    >>> with autograd.detect_anomaly():
    ...     inp = torch.rand(10, 10, requires_grad=True)
    ...     out = run_fn(inp)
    ...     out.backward()
        Traceback of forward call that caused the error:
          File ""tmp.py"", line 53, in <module>
            out = run_fn(inp)
          File ""tmp.py"", line 44, in run_fn
            out = MyFunc.apply(a)
        Traceback (most recent call last):
          File ""<stdin>"", line 4, in <module>
          File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward
            torch.autograd.backward(self, gradient, retain_graph, create_graph)
          File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward
            allow_unreachable=True)  # allow_unreachable flag
          File ""/your/pytorch/install/torch/autograd/function.py"", line 76, in apply
            return self._forward_cls.backward(self, *args)
          File ""<stdin>"", line 8, in backward
        RuntimeError: Some error in backward",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\anomaly_mode.py,set_detect_anomaly,"Context-manager that sets the anomaly detection for the autograd engine on or off.

``set_detect_anomaly`` will enable or disable the autograd anomaly detection
based on its argument :attr:`mode`.
It can be used as a context-manager or as a function.

See ``detect_anomaly`` above for details of the anomaly detection behaviour.

Args:
    mode (bool): Flag whether to enable anomaly detection (``True``),
                 or disable (``False``).
    check_nan (bool): Flag whether to raise an error when the backward
                      generate ""nan""",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\forward_ad.py,UnpackedDualTensor,"Namedtuple returned by :func:`unpack_dual` containing the primal and tangent components of the dual tensor.

See :func:`unpack_dual` for more details.",0,['_UnpackedDualTensor'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\forward_ad.py,dual_level,"Context-manager for forward AD, where all forward AD computation must occur within the ``dual_level`` context.

.. Note::

    The ``dual_level`` context appropriately enters and exit the dual level to
    controls the current forward AD level, which is used by default by the other
    functions in this API.

    We currently don't plan to support nested ``dual_level`` contexts, however, so
    only a single forward AD level is supported. To compute higher-order
    forward grads, one can use :func:`torch.func.jvp`.

Example::

    >>> # xdoctest: +SKIP(""Undefined variables"")
    >>> x = torch.tensor([1])
    >>> x_t = torch.tensor([1])
    >>> with dual_level():
    ...     inp = make_dual(x, x_t)
    ...     # Do computations with inp
    ...     out = your_fn(inp)
    ...     _, grad = unpack_dual(out)
    >>> grad is None
    False
    >>> # After exiting the level, the grad is deleted
    >>> _, grad_after = unpack_dual(out)
    >>> grad is None
    True

Please see the `forward-mode AD tutorial <https://pytorch.org/tutorials/intermediate/forward_ad_usage.html>`__
for detailed steps on how to use this API.",2,['_DecoratorContextManager'],0,"['__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\forward_ad.py,_set_fwd_grad_enabled,,3,['_DecoratorContextManager'],0,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\gradcheck.py,GradcheckError,Error raised by :func:`gradcheck` and :func:`gradgradcheck`.,0,['RuntimeError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\grad_mode.py,no_grad,"Context-manager that disables gradient calculation.

Disabling gradient calculation is useful for inference, when you are sure
that you will not call :meth:`Tensor.backward()`. It will reduce memory
consumption for computations that would otherwise have `requires_grad=True`.

In this mode, the result of every computation will have
`requires_grad=False`, even when the inputs have `requires_grad=True`.
There is an exception! All factory functions, or functions that create
a new Tensor and take a requires_grad kwarg, will NOT be affected by
this mode.

This context manager is thread local; it will not affect computation
in other threads.

Also functions as a decorator.

.. note::
    No-grad is one of several mechanisms that can enable or
    disable gradients locally see :ref:`locally-disable-grad-doc` for
    more information on how they compare.

.. note::
    This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.
    If you want to disable forward AD for a computation, you can unpack
    your dual tensors.

Example::
    >>> # xdoctest: +SKIP
    >>> x = torch.tensor([1.], requires_grad=True)
    >>> with torch.no_grad():
    ...     y = x * 2
    >>> y.requires_grad
    False
    >>> @torch.no_grad()
    ... def doubler(x):
    ...     return x * 2
    >>> z = doubler(x)
    >>> z.requires_grad
    False
    >>> @torch.no_grad()
    ... def tripler(x):
    ...     return x * 3
    >>> z = tripler(x)
    >>> z.requires_grad
    False
    >>> # factory function exception
    >>> with torch.no_grad():
    ...     a = torch.nn.Parameter(torch.rand(10))
    >>> a.requires_grad
    True",3,['_NoParamDecoratorContextManager'],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\grad_mode.py,enable_grad,"Context-manager that enables gradient calculation.

Enables gradient calculation, if it has been disabled via :class:`~no_grad`
or :class:`~set_grad_enabled`.

This context manager is thread local; it will not affect computation
in other threads.

Also functions as a decorator.

.. note::
    enable_grad is one of several mechanisms that can enable or
    disable gradients locally see :ref:`locally-disable-grad-doc` for
    more information on how they compare.

.. note::
    This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.

Example::
    >>> # xdoctest: +SKIP
    >>> x = torch.tensor([1.], requires_grad=True)
    >>> with torch.no_grad():
    ...     with torch.enable_grad():
    ...         y = x * 2
    >>> y.requires_grad
    True
    >>> y.backward()
    >>> x.grad
    tensor([2.])
    >>> @torch.enable_grad()
    ... def doubler(x):
    ...     return x * 2
    >>> with torch.no_grad():
    ...     z = doubler(x)
    >>> z.requires_grad
    True
    >>> @torch.enable_grad()
    ... def tripler(x):
    ...     return x * 3
    >>> with torch.no_grad():
    ...     z = tripler(x)
    >>> z.requires_grad
    True",2,['_NoParamDecoratorContextManager'],0,"['__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\grad_mode.py,set_grad_enabled,"Context-manager that sets gradient calculation on or off.

``set_grad_enabled`` will enable or disable grads based on its argument :attr:`mode`.
It can be used as a context-manager or as a function.

This context manager is thread local; it will not affect computation
in other threads.

Args:
    mode (bool): Flag whether to enable grad (``True``), or disable
                 (``False``). This can be used to conditionally enable
                 gradients.

.. note::
    set_grad_enabled is one of several mechanisms that can enable or
    disable gradients locally see :ref:`locally-disable-grad-doc` for
    more information on how they compare.

.. note::
    This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.

Example::
    >>> # xdoctest: +SKIP
    >>> x = torch.tensor([1.], requires_grad=True)
    >>> is_train = False
    >>> with torch.set_grad_enabled(is_train):
    ...     y = x * 2
    >>> y.requires_grad
    False
    >>> _ = torch.set_grad_enabled(True)
    >>> y = x * 2
    >>> y.requires_grad
    True
    >>> _ = torch.set_grad_enabled(False)
    >>> y = x * 2
    >>> y.requires_grad
    False",5,['_DecoratorContextManager'],0,"['__init__', '__call__', '__enter__', '__exit__', 'clone']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\grad_mode.py,inference_mode,"Context-manager that enables or disables inference mode.

InferenceMode is a context manager analogous to :class:`~no_grad`
to be used when you are certain your operations will have no interactions
with autograd (e.g., model training). Code run under this mode gets better
performance by disabling view tracking and version counter bumps. Note that
unlike some other mechanisms that locally enable or disable grad,
entering inference_mode also disables to :ref:`forward-mode AD <forward-mode-ad>`.

This context manager is thread local; it will not affect computation
in other threads.

Also functions as a decorator.

.. note::
    Inference mode is one of several mechanisms that can enable or
    disable gradients locally see :ref:`locally-disable-grad-doc` for
    more information on how they compare.

Args:
    mode (bool or function): Either a boolean flag whether to enable or
        disable inference mode or a Python function to decorate with
        inference mode enabled

Example::
    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)
    >>> import torch
    >>> x = torch.ones(1, 2, 3, requires_grad=True)
    >>> with torch.inference_mode():
    ...     y = x * x
    >>> y.requires_grad
    False
    >>> # xdoctest: +SKIP(""want string isnt quite right"")
    >>> y._version
    Traceback (most recent call last):
    File ""<stdin>"", line 1, in <module>
    RuntimeError: Inference tensors do not track version counter.
    >>> @torch.inference_mode()
    ... def func(x):
    ...     return x * x
    >>> out = func(x)
    >>> out.requires_grad
    False
    >>> @torch.inference_mode()
    ... def doubler(x):
    ...     return x * 2
    >>> out = doubler(x)
    >>> out.requires_grad
    False",5,['_DecoratorContextManager'],0,"['__init__', '__new__', '__enter__', '__exit__', 'clone']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\grad_mode.py,set_multithreading_enabled,"Context-manager that sets multithreaded backwards on or off.

``set_multithreading_enabled`` will enable or disable multithreaded backwards based on its argument :attr:`mode`.
It can be used as a context-manager or as a function.

This context manager is thread local; it will not affect computation
in other threads.

Args:
    mode (bool): Flag whether to enable multithreaded backwards (``True``), or disable
                 (``False``).

.. note::
    This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.",4,['_DecoratorContextManager'],0,"['__init__', '__enter__', '__exit__', 'clone']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\grad_mode.py,_force_original_view_tracking,"Context-manager that sets whether or not to always enable view-replay in autograd.

``set_view_replay_enabled`` will enable or disable view-replay based on its argument :attr:`mode`.
It can be used as a context-manager or as a function.

This context manager is thread local; it will not affect computation
in other threads.

When a tensor view is mutated, the autograd engine needs to decide whether or not
to regenerate the ""updated view"" by either replaying the chain of views from the updated base,
or with a single call to as_strided.

If set_view_replay_enabled is set to True, then autograd will always use view replay.
Otherwise, it will fall back to its existing logic.

Args:
    mode (bool): Flag whether to enable view-replay (``True``), or disable
                 (``False``).",4,['_DecoratorContextManager'],0,"['__init__', '__enter__', '__exit__', 'clone']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\autograd\grad_mode.py,_unsafe_preserve_version_counter,"DO NOT USE THIS UNLESS YOU KNOW EXACTLY WHAT YOU'RE DOING.

This context manager can lead to arbitrary silent-correctness issues in any other part of your code
(even the ones not touched directly by the context manager)!

Ordinarily, autograd will track mutations to tensors by incrementing it's `._version` attribute.
This is generally important for correctness, as for example, mutating a tensor that autograd has saved
for the backwards pass can result in incorrect gradients, and autograd uses the version counter to detect
and error out in this situation.

However, there are rare instances where it might be useful to hide mutations from autograd. For example:
if a tensor is very large, and you'd like to free its memory by storing it elsewhere, and re-populate
the tensor right before it is needed by autograd.

Args:
    tensor (torch.Tensor): the tensor in question, that you would like to preserve the version counter of.

.. note::
    This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.",3,['_DecoratorContextManager'],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\backends\__init__.py,ContextProp,,3,[],0,"['__init__', '__get__', '__set__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\backends\__init__.py,PropModule,,2,['types.ModuleType'],0,"['__init__', '__getattr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cpu\__init__.py,Stream,N.B. This class only exists to facilitate device-agnostic code,2,[],0,"['__init__', 'wait_stream']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cpu\__init__.py,Event,,4,[],0,"['query', 'record', 'synchronize', 'wait']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cpu\__init__.py,StreamContext,"Context-manager that selects a given stream.

N.B. This class only exists to facilitate device-agnostic code",3,['AbstractContextManager'],1,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cuda\gds.py,_GdsFile,"Wrapper around cuFile.

cuFile is a file-like interface to the GPUDirect Storage (GDS) API.

Args:
    filename (str): Name of the file to open.
    flags (int): Flags to pass to ``os.open`` when opening the file. ``os.O_DIRECT`` will
        be added automatically.

.. _CUDA GPUDirect Storage Documentation:
    https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api",6,[],0,"['__init__', '__del__', 'register_handle', 'deregister_handle', 'load_storage', 'save_storage']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cuda\graphs.py,CUDAGraph,"Wrapper around a CUDA graph.

.. warning::
    This API is in beta and may change in future releases.",8,['torch._C._CUDAGraph'],0,"['__new__', 'capture_begin', 'capture_end', 'replay', 'reset', 'pool', 'enable_debug_mode', 'debug_dump']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cuda\graphs.py,graph,"Context-manager that captures CUDA work into a :class:`torch.cuda.CUDAGraph` object for later replay.

See :ref:`CUDA Graphs <cuda-graph-semantics>` for a general introduction,
detailed use, and constraints.

Arguments:
    cuda_graph (torch.cuda.CUDAGraph): Graph object used for capture.
    pool (optional): Opaque token (returned by a call to :func:`~torch.cuda.graph_pool_handle()` or
        :meth:`other_Graph_instance.pool()<torch.cuda.CUDAGraph.pool>`) hinting this graph's capture
        may share memory from the specified pool. See :ref:`Graph memory management<graph-memory-management>`.
    stream (torch.cuda.Stream, optional): If supplied, will be set as the current stream in the context.
        If not supplied, ``graph`` sets its own internal side stream as the current stream in the context.
    capture_error_mode (str, optional): specifies the cudaStreamCaptureMode for the graph capture stream.
        Can be ""global"", ""thread_local"" or ""relaxed"". During cuda graph capture, some actions, such as cudaMalloc,
        may be unsafe. ""global"" will error on actions in other threads, ""thread_local"" will only error for
        actions in the current thread, and ""relaxed"" will not error on actions. Do NOT change this setting
        unless you're familiar with `cudaStreamCaptureMode <https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85>`_

.. note::
    For effective memory sharing, if you pass a ``pool`` used by a previous capture and the previous capture
    used an explicit ``stream`` argument, you should pass the same ``stream`` argument to this capture.

.. warning::
    This API is in beta and may change in future releases.

.. _cudaStreamCaptureMode:
    https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85",3,[],1,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cuda\jiterator.py,_CodeParser,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cuda\jiterator.py,_JittedFunction,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\cuda\_memory_viz.py,Bytes,,3,[],0,"['__init__', '__add__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\argparse_util.py,env,"Get argument values from ``PET_{dest}`` before defaulting to the given ``default`` value.

For flags (e.g. ``--standalone``)
use ``check_env`` instead.

.. note:: when multiple option strings are specified, ``dest`` is
          the longest option string (e.g. for ``""-f"", ""--foo""``
          the env var to set is ``PET_FOO`` not ``PET_F``)

Example:
::

 parser.add_argument(""-f"", ""--foo"", action=env, default=""bar"")

 ./program                                      -> args.foo=""bar""
 ./program -f baz                               -> args.foo=""baz""
 ./program --foo baz                            -> args.foo=""baz""
 PET_FOO=""env_bar"" ./program -f baz    -> args.foo=""baz""
 PET_FOO=""env_bar"" ./program --foo baz -> args.foo=""baz""
 PET_FOO=""env_bar"" ./program           -> args.foo=""env_bar""

 parser.add_argument(""-f"", ""--foo"", action=env, required=True)

 ./program                                      -> fails
 ./program -f baz                               -> args.foo=""baz""
 PET_FOO=""env_bar"" ./program           -> args.foo=""env_bar""
 PET_FOO=""env_bar"" ./program -f baz    -> args.foo=""baz""",2,['Action'],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\argparse_util.py,check_env,"Check whether the env var ``PET_{dest}`` exists before defaulting to the given ``default`` value.

Equivalent to
``store_true`` argparse built-in action except that the argument can
be omitted from the commandline if the env var is present and has a
non-zero value.

.. note:: it is redundant to pass ``default=True`` for arguments
          that use this action because a flag should be ``True``
          when present and ``False`` otherwise.

Example:
::

 parser.add_argument(""--verbose"", action=check_env)

 ./program                                  -> args.verbose=False
 ./program --verbose                        -> args.verbose=True
 PET_VERBOSE=1 ./program           -> args.verbose=True
 PET_VERBOSE=0 ./program           -> args.verbose=False
 PET_VERBOSE=0 ./program --verbose -> args.verbose=True

Anti-pattern (don't do this):

::

 parser.add_argument(""--verbose"", action=check_env, default=True)

 ./program                                  -> args.verbose=True
 ./program --verbose                        -> args.verbose=True
 PET_VERBOSE=1 ./program           -> args.verbose=True
 PET_VERBOSE=0 ./program           -> args.verbose=False",2,['Action'],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\collective_utils.py,SyncPayload,,0,['Generic[T]'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_checkpointable.py,_Checkpointable,"Interface for checkpointable objects.
Implemented as a protocol, implicit subtyping is supported so subclasses do not need to inherit this explicitly.
This is to allow arbitrary objects/tensor subclasses to hook into DCP seamlessly through implementing the interface.",3,['Protocol'],0,"['__create_write_items__', '__create_chunk_list__', '__get_tensor_shard__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable_state.py,_State,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_state_dict_utils.py,CompanionMismatch,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_state_dict_utils.py,_TensorInfo,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributions\constraint_registry.py,ConstraintRegistry,Registry to link constraints to transforms.,3,[],0,"['__init__', 'register', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributions\utils.py,lazy_property,"Used as a decorator for lazy loading of class attributes. This uses a
non-data descriptor that calls the wrapped method to compute the property on
first call; thereafter replacing the wrapped method into an instance
attribute.",2,[],0,"['__init__', '__get__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributions\utils.py,_lazy_property_and_property,"We want lazy properties to look like multiple things.

* property when Sphinx autodoc looks
* lazy_property when Distribution validate_args looks",1,"['lazy_property', 'property']",0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\export\custom_obj.py,ScriptObjectMeta,Metadata which is stored on nodes representing ScriptObjects.,0,[],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\export\_safeguard.py,AutogradStateOpsFailSafeguard,"Detect grad state ops during exporting the graph and fail the process by
raising an error, to avoid unexpected behavior. Those grad mode ops could be:
`torch.no_grad`
`torch.enable_grad`
`torch.set_grad_enabled`

Export with predispatch mode is exempted.",1,['TorchFunctionMode'],0,['__torch_function__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\export\_unlift.py,_StatefulGraphModuleFactory,Metaclass that ensures a private constructor for _StatefulGraphModule,2,['type'],0,"['__call__', '_create']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\export\_unlift.py,_StatefulGraphModule,,1,['torch.fx.GraphModule'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\futures\__init__.py,_PyFutureMeta,,0,"['type(torch._C.Future)', 'type(Generic)']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\futures\__init__.py,Future,"Wrapper around a ``torch._C.Future`` which encapsulates an asynchronous
execution of a callable, e.g. :meth:`~torch.distributed.rpc.rpc_async`. It
also exposes a set of APIs to add callback functions and set results.

.. warning:: GPU support is a beta feature, subject to changes.",8,"['torch._C.Future', 'Generic[T]']",0,"['__init__', 'done', 'wait', 'value', 'then', 'add_done_callback', 'set_result', 'set_exception']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\operator_schemas.py,ArgsKwargsPair,Simple named tuple for wrapping args/kwargs pairs.,0,['NamedTuple'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\operator_schemas.py,_FakeGlobalNamespace,,1,[],0,['__getattr__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\subgraph_rewriter.py,Match,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\subgraph_rewriter.py,ReplacedPatterns,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedLinear,,1,['torch.jit.ScriptModule'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedLinearFP16,,1,['torch.jit.ScriptModule'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedRNNCellBase,,1,['torch.jit.ScriptModule'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedRNNCell,,1,['QuantizedRNNCellBase'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedLSTMCell,,1,['QuantizedRNNCellBase'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedGRUCell,,1,['QuantizedRNNCellBase'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedRNNBase,,1,['torch.jit.ScriptModule'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedLSTM,,1,['QuantizedRNNBase'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\quantized.py,QuantizedGRU,,1,['QuantizedRNNBase'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_check.py,AttributeTypeIsSupportedChecker,"Check the ``__init__`` method of a given ``nn.Module``.

It ensures that all instance-level attributes can be properly initialized.

Specifically, we do type inference based on attribute values...even
if the attribute in question has already been typed using
Python3-style annotations or ``torch.jit.annotate``. This means that
setting an instance-level attribute to ``[]`` (for ``List``),
``{}`` for ``Dict``), or ``None`` (for ``Optional``) isn't enough
information for us to properly initialize that attribute.

An object of this class can walk a given ``nn.Module``'s AST and
determine if it meets our requirements or not.

Known limitations
1. We can only check the AST nodes for certain constructs; we can't
``eval`` arbitrary expressions. This means that function calls,
class instantiations, and complex expressions that resolve to one of
the ""empty"" values specified above will NOT be flagged as
problematic.
2. We match on string literals, so if the user decides to use a
non-standard import (e.g. `from typing import List as foo`), we
won't catch it.

Example:
    .. code-block:: python

        class M(torch.nn.Module):
            def fn(self):
                return []

            def __init__(self) -> None:
                super().__init__()
                self.x: List[int] = []

            def forward(self, x: List[int]):
                self.x = x
                return 1

    The above code will pass the ``AttributeTypeIsSupportedChecker``
    check since we have a function call in ``__init__``. However,
    it will still fail later with the ``RuntimeError`` ""Tried to set
    nonexistent attribute: x. Did you forget to initialize it in
    __init__()?"".

Args:
    nn_module - The instance of ``torch.nn.Module`` whose
        ``__init__`` method we wish to check",5,['ast.NodeVisitor'],0,"['check', '_is_empty_container', 'visit_Assign', 'visit_AnnAssign', 'visit_Call']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_ir_utils.py,_InsertPoint,,3,[],0,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_recursive.py,SourceContext,,1,['torch._C._jit_tree_views.SourceRangeFactory'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_recursive.py,ConcreteTypeStore,,2,[],2,"['__init__', 'get_or_create_concrete_type']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,OrderedDictWrapper,,9,[],0,"['__init__', 'keys', 'values', '__len__', '__delitem__', 'items', '__setitem__', '__contains__', '__getitem__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,OrderedModuleDict,,5,['OrderedDictWrapper'],0,"['__init__', 'items', '__contains__', '__setitem__', '__getitem__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,ScriptMeta,,1,['type'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,_CachedForward,,1,[],0,['__get__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,ScriptWarning,,0,['Warning'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,ConstMap,,2,[],0,"['__init__', '__getattr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,_ScriptProfileColumn,,3,[],0,"['__init__', 'add_row', 'materialize']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,_ScriptProfileTable,,2,[],0,"['__init__', 'dump_string']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_script.py,_ScriptProfile,,5,[],0,"['__init__', 'enable', 'disable', 'dump_string', 'dump']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\_state.py,EnabledProxy,"Stores whether the JIT is enabled or not.

This is just a wrapper for a bool, so that we get reference semantics",3,[],0,"['__init__', 'parse_env', '__bool__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\__init__.py,strict_fusion,"Give errors if not all nodes have been fused in inference, or symbolically differentiated in training.

Example:
Forcing fusion of additions.

.. code-block:: python

    @torch.jit.script
    def foo(x):
        with torch.jit.strict_fusion():
            return x + x + x",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\monitor\__init__.py,TensorboardEventHandler,"TensorboardEventHandler is an event handler that will write known events to
the provided SummaryWriter.

This currently only supports ``torch.monitor.Stat`` events which are logged
as scalars.

Example:
    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MONITOR)
    >>> # xdoctest: +REQUIRES(module:tensorboard)
    >>> from torch.utils.tensorboard import SummaryWriter
    >>> from torch.monitor import TensorboardEventHandler, register_event_handler
    >>> writer = SummaryWriter(""log_dir"")
    >>> register_event_handler(TensorboardEventHandler(writer))",2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\mps\event.py,Event,"Wrapper around an MPS event.

MPS events are synchronization markers that can be used to monitor the
device's progress, to accurately measure timing, and to synchronize MPS streams.

Args:
    enable_timing (bool, optional): indicates if the event should measure time
        (default: ``False``)",7,[],0,"['__init__', '__del__', 'record', 'wait', 'query', 'synchronize', 'elapsed_time']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\mtia\__init__.py,DeferredMtiaCallError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\mtia\__init__.py,device,"Context-manager that changes the selected device.

Args:
    device (torch.device or int): device index to select. It's a no-op if
        this argument is a negative integer or ``None``.",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\mtia\__init__.py,StreamContext,"Context-manager that selects a given stream.

All MTIA kernels queued within its context will be enqueued on a selected
stream.

Args:
    Stream (Stream): selected stream. This manager is a no-op if it's
        ``None``.
.. note:: Streams are per-device.",3,[],1,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\multiprocessing\pool.py,Pool,"Pool implementation which uses our version of SimpleQueue.

This lets us pass tensors in shared memory across processes instead of
serializing the underlying data.",2,['multiprocessing.pool.Pool'],0,"['_setup_queues', '_repopulate_pool']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\multiprocessing\queue.py,ConnectionWrapper,Proxy class for _multiprocessing.Connection which uses ForkingPickler for object serialization.,4,[],0,"['__init__', 'send', 'recv', '__getattr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\multiprocessing\queue.py,Queue,,1,['multiprocessing.queues.Queue'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\multiprocessing\queue.py,SimpleQueue,,1,['multiprocessing.queues.SimpleQueue'],0,['_make_methods'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\errors.py,OnnxExporterWarning,Warnings in the ONNX exporter.,0,['UserWarning'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\errors.py,OnnxExporterError,Errors raised by the ONNX exporter. This is the base class for all exporter errors.,0,['RuntimeError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\errors.py,UnsupportedOperatorError,Raised when an operator is unsupported by the exporter.,1,['OnnxExporterError'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\errors.py,SymbolicValueError,Errors around TorchScript values and nodes.,1,['OnnxExporterError'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_experimental.py,ExportOptions,Arguments used by :func:`torch.onnx.export`.,0,[],12,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_onnx_supported_ops.py,_TorchSchema,,6,[],0,"['__init__', '__str__', '__hash__', '__eq__', 'is_aten', 'is_backward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\package\file_structure_representation.py,Directory,"A file structure representation. Organized as Directory nodes that have lists of
their Directory children. Directories for a package are created by calling
:meth:`PackageImporter.file_structure`.",6,[],0,"['__init__', '_get_dir', '_add_file', 'has_file', '__str__', '_stringify_tree']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\package\_directory_reader.py,_HasStorage,,2,[],0,"['__init__', 'storage']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\package\_directory_reader.py,DirectoryReader,"Class to allow PackageImporter to operate on unzipped packages. Methods
copy the behavior of the internal PyTorchFileReader class (which is used for
accessing packages in all other cases).

N.B.: ScriptObjects are not depickleable or accessible via this DirectoryReader
class due to ScriptObjects requiring an actual PyTorchFileReader instance.",6,[],0,"['__init__', 'get_record', 'get_storage_from_record', 'has_record', 'get_all_records', 'serialization_id']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\package\_mangling.py,PackageMangler,"Used on import, to ensure that all modules imported have a shared mangle parent.",4,[],0,"['__init__', 'mangle', 'demangle', 'parent_name']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\package\_mock.py,MockedObject,,3,[],1,"['__new__', '__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\package\_package_pickler.py,PackagePickler,"Package-aware pickler.

This behaves the same as a normal pickler, except it uses an `Importer`
to find objects and modules to save.",2,['_Pickler'],0,"['__init__', 'save_global']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\package\_package_unpickler.py,PackageUnpickler,"Package-aware unpickler.

This behaves the same as a normal unpickler, except it uses `importer` to
find any global names that it encounters while unpickling.",2,['pickle._Unpickler'],0,"['__init__', 'find_class']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\bundled_inputs.py,InflatableArg,"Helper type for bundled inputs.

'value' is the compressed/deflated input that is stored in the model. Value
must be of the same type as the argument to the function that it is a deflated
input for.

'fmt' is a formatable code string that is executed to inflate the compressed data into
the appropriate input. It can use 'value' as an input to the format str. It must result
in a value of the same type as 'value'.

'fmt_fn' is a formatable function code string that is executed to inflate the compressed
data into the appropriate input. It must result in a value of the same type as 'value'.
The function name should be the formatable part of the string.

Note: Only top level InflatableArgs can be inflated. i.e. you cannot place
an inflatable arg inside of some other structure. You should instead create
an inflatable arg such that the fmt code string returns the full structure
of your input.",0,['NamedTuple'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\file_baton.py,FileBaton,"A primitive, file-based synchronization utility.",4,[],0,"['__init__', 'try_acquire', 'wait', 'release']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\flop_counter.py,FlopCounterMode,"``FlopCounterMode`` is a context manager that counts the number of flops within its context.

It does this using a ``TorchDispatchMode``.

It also supports hierarchical output by passing a module (or list of
modules) to FlopCounterMode on construction. If you do not need hierarchical
output, you do not need to use it with a module.

Example usage

.. code-block:: python

    mod = ...
    with FlopCounterMode(mod) as flop_counter:
        mod.sum().backward()",8,['TorchDispatchMode'],0,"['__init__', 'get_total_flops', 'get_flop_counts', 'get_table', '__enter__', '__exit__', '__torch_dispatch__', '_count_flops']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\hooks.py,RemovableHandle,"A handle which provides the capability to remove a hook.

Args:
    hooks_dict (dict): A dictionary of hooks, indexed by hook ``id``.
    extra_dict (Union[dict, List[dict]]): An additional dictionary or list of
        dictionaries whose keys will be deleted when the same keys are
        removed from ``hooks_dict``.",6,[],2,"['__init__', 'remove', '__getstate__', '__setstate__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\hooks.py,BackwardHook,"A wrapper class to implement nn.Module backward hooks.

It handles:
  - Ignoring non-Tensor inputs and replacing them by None before calling the user hook
  - Generating the proper Node to capture a set of Tensor's gradients
  - Linking the gradients captures for the outputs with the gradients captured for the input
  - Calling the user hook once both output and input gradients are available",7,[],0,"['__init__', '_pack_with_none', '_unpack_none', '_set_user_hook', '_apply_on_tensors', 'setup_input_hook', 'setup_output_hook']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_content_store.py,ContentStoreWriter,,4,[],0,"['__init__', 'write_storage', 'compute_tensor_metadata', 'write_tensor']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_content_store.py,ContentStoreReader,,4,[],0,"['__init__', 'read_storage', 'read_tensor_metadata', 'read_tensor']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_contextlib.py,_DecoratorContextManager,Allow a context manager to be used as a decorator.,4,[],0,"['__call__', '__enter__', '__exit__', 'clone']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_contextlib.py,_NoParamDecoratorContextManager,Allow a context manager to be used as a decorator without parentheses.,1,['_DecoratorContextManager'],0,['__new__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_cpp_extension_versioner.py,ExtensionVersioner,,3,[],0,"['__init__', 'get_version', 'bump_version_if_changed']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_cxx_pytree.py,_DummyLeaf,,1,[],0,['__repr__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_cxx_pytree.py,LeafSpecMeta,,1,['type(TreeSpec)'],0,['__instancecheck__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_cxx_pytree.py,LeafSpec,,1,['TreeSpec'],0,['__new__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_device.py,DeviceContext,,4,['TorchFunctionMode'],0,"['__init__', '__enter__', '__exit__', '__torch_function__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,KeyEntry,,4,['Protocol'],0,"['__hash__', '__eq__', '__str__', 'get']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,NodeDef,,0,['NamedTuple'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,_SerializeNodeDef,,0,['NamedTuple'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,SequenceKey,,2,['Generic[T]'],1,"['__str__', 'get']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,MappingKey,,2,"['Generic[K, T]']",1,"['__str__', 'get']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,GetAttrKey,,2,[],1,"['__str__', 'get']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,TreeSpec,,6,[],6,"['__post_init__', '__repr__', 'is_leaf', '_flatten_up_to_helper', 'flatten_up_to', 'unflatten']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,LeafSpec,,3,['TreeSpec'],0,"['__init__', '__post_init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,_TreeSpecSchema,"_TreeSpecSchema is the schema used to serialize the TreeSpec
It contains the following fields:
- type: A string name of the type. null for the case of a LeafSpec.
- context: Any format which is json dumpable
- children_spec: A list of children serialized specs.",0,[],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,_ProtocolFn,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\_pytree.py,_DummyLeaf,,1,[],0,['__repr__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\xpu\__init__.py,_DeviceGuard,,3,[],0,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\xpu\__init__.py,device,"Context-manager that changes the selected device.

Args:
    device (torch.device or int or str): device index to select. It's a no-op if
        this argument is a negative integer or ``None``.",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\xpu\__init__.py,device_of,"Context-manager that changes the current device to that of given object.

You can use both tensors and storages as arguments. If a given object is
not allocated on a XPU, this is a no-op.

Args:
    obj (Tensor or Storage): object allocated on the selected device.",1,['device'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\xpu\__init__.py,StreamContext,"Context-manager that selects a given stream.

All XPU kernels queued within its context will be enqueued on a selected
stream.

Args:
    Stream (Stream): selected stream. This manager is a no-op if it's
        ``None``.
.. note:: Streams are per-device.",3,[],1,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_awaits\__init__.py,_PyAwaitMeta,,0,"['type(torch._C._Await)', 'type(Generic)']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_awaits\__init__.py,_Await,"Wrapper around a ``torch._C.Await`` which encapsulates delayed execution
of a callable. All manipulations happen with functions ``torch.jit._awaitable``,
``torch.jit._awaitable_wait``, ``torch.jit._awaitable_nowait``.

Torch scriptable manipulations:
``torch.jit._awaitable(func, *args)``
Creates ``Await[W]`` object, where W is return type of func.

Returns:
``torch.jit._awaitable_wait(Await[W])``
Returns the result of the function, specified at ``_awaitable``,  with specified arguments.

Returns:
    The result of type ``W`` of the function call. The result is owned by ``Await[W]``
    and returned on all following ``_awaitable_wait`` calls.


``torch.jit._awaitable_nowait(W)``
Returns:
    Trivial ``Await[W]`` with specified result.


Only in eager mode:
``fn() -> Callable[Tuple[Any], W]``
Returns:
    Specified at ``_awaitable`` python function ``func``.

``args() -> Tuple[Any]``
Returns:
    Specified at ``_awaitable`` python args.

``is_nowait() -> _bool``
Returns:
    ``True`` if this object was created via ``_awaitable_nowait`` call (trivial `Await[W]`).

In eager mode ``Await[W]`` can be used as ``W`` i.e. attributes of W can be called on ``Await[W]``,
``_awaitable_wait()`` call will be transparently added.",0,"['torch._C._Await', 'Generic[W]']",0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_custom_op\impl.py,CustomOp,"This API is deprecated, please use torch.library.custom_op instead",18,[],0,"['__init__', '_register_autograd_kernel_indirection', '_register_impl', '_get_impl', '_has_impl', '_destroy', '__repr__', '__call__', 'impl', '_check_doesnt_have_library_impl', 'impl_factory', 'impl_abstract', '_check_can_register_backward', '_check_doesnt_have_library_autograd_impl', '_check_doesnt_have_library_meta_impl', '_register_autograd_kernel', 'impl_save_for_backward', 'impl_backward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_custom_op\impl.py,FuncAndLocation,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dispatch\python.py,Lit,,2,[],0,"['__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\bytecode_analysis.py,ReadsWrites,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\bytecode_analysis.py,FixedPointBox,,0,[],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\bytecode_analysis.py,StackSize,,3,[],3,"['zero', 'offset_of', 'exn_tab_jump']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\bytecode_transformation.py,InstructionExnTabEntry,,2,[],5,"['__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\bytecode_transformation.py,Instruction,A mutable version of dis.Instruction,3,[],10,"['__hash__', '__eq__', 'short_inst_repr']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\bytecode_transformation.py,_NotProvided,,1,[],0,['__repr__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\bytecode_transformation.py,ExceptionTableEntry,,0,[],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\cache_size.py,CacheSizeRelevantForFrame,"We track the number of cache entries that have same id_match objects as the
given frame.

TODO(janimesh) - Consider adding a map from tuple_of_match_ids to count -
https://github.com/pytorch/pytorch/pull/107496#discussion_r1304564682 - this
could be useful for debugging as well.",3,[],2,"['will_compilation_exceed', 'will_compilation_exceed_accumulated_limit', 'will_compilation_exceed_specific_limit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\callback.py,CompilationCallbackHandler,,8,[],0,"['__init__', 'register_start_callback', 'register_end_callback', 'remove_start_callback', 'remove_end_callback', 'run_start_callbacks', 'run_end_callbacks', 'clear']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\code_context.py,CodeContextDict,,5,[],0,"['__init__', 'has_context', 'get_context', 'pop_context', 'clear']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\decorators.py,_DimRange,"This represents an dimension of a tensor and the corresponding
min and max values it can take.  Don't create this
class directly; instead, use :func:`mark_dynamic`.",0,[],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\hooks.py,Hooks,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\testing.py,CompileCounter,,3,[],0,"['__init__', '__call__', 'clear']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\testing.py,CompileCounterWithBackend,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\testing.py,EagerAndRecordGraphs,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\trace_rules.py,FunctionIdSet,"Track a set of `id()`s of objects which are either allowed or not
allowed to go into the generated FX graph.  Use to test for torch.*,
numpy.*, builtins.*, etc.

Support user modification to permit customization of what can be
added to the graph and what will cause a graph break.",6,[],2,"['__init__', '__call__', 'get_name', 'add', 'remove', '__contains__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\trace_rules.py,SkipResult,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\trace_rules.py,FunctionInfo,,0,[],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\types.py,GuardFail,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\types.py,GuardFn,,1,['Protocol'],8,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\types.py,GuardedCode,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\types.py,DynamoCallbackFn,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\types.py,DynamoGuardHook,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\types.py,ProfilerStartHook,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\types.py,ProfilerEndHook,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\types.py,BytecodeHook,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\_trace_wrapped_higher_order_op.py,TraceWrapped,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\non_strict_utils.py,_NonStrictTorchFunctionHandler,"1. Handles data-dependent errors raised by torch function calls in non-strict.

Any data-dependent error is due to some condition on unbacked symints
that cannot be resolved. A mechanical way of fixing the error is to use
a torch._check() call to assert either that condition or its negation.
The handler suggests these options as code and points to the location
of the torch function call that raised the error as part of the error
message shown to the user, who can then simply select and copy-paste
a suggested fix at that location.

NOTE: Not all data-dependent errors are raised by torch function calls.
In particular, conditions on unbacked symints can appear outside such
calls, and as such are not handled here.

2. Handles line-of-code logging for each torch function call in non-strict.

Usage: TORCHEXPORT_EXTENDED_DEBUG_CURRENT_LOC=1 TORCH_LOGS=""+export"" ...",1,['torch.overrides.TorchFunctionMode'],0,['__torch_function__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\wrappers.py,ExportTracepoint,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\__init__.py,ExportDynamoConfig,Manage Export-specific configurations of Dynamo.,0,[],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_functorch\compilers.py,DebugInterpreter,,2,['fx.Interpreter'],0,"['run', 'run_node']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_functorch\fx_minifier.py,LoadTensorMeta,,0,[],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_functorch\fx_minifier.py,ConcreteProp,,3,['torch.fx.Interpreter'],0,"['__init__', 'run_node', 'propagate']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_functorch\fx_minifier.py,ReproState,,1,[],2,['__post_init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\associative_scan.py,AssociativeScanOp,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\auto_functionalize.py,ViewInfo,,1,[],5,['regenerate_view'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\auto_functionalize.py,AutoFunctionalized,"auto_functionalized(_mutable_op, **kwargs)

This HOP runs a ""functional"" version of _mutable_op.

Concretely, it looks at all the arguments that are mutable through
_mutable_op's operator schema, clones those kwargs, runs
`out = _mutable_op(**kwargs)` with the cloned values, and then returns the
operator output concatenated with the cloned values that were mutated.

We have some restrictions on `_mutable_op`.
See `can_auto_functionalize` for the restrictions. We can likely lift
many of these if users request it.

The reason why _mutable_op is prefixed with an
underscore is to prevent collisions with kwarg names in **kwargs.",2,['HigherOrderOperator'],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\auto_functionalize.py,AutoFunctionalizedV2,"auto_functionalized_v2(_mutable_op, **kwargs)

This HOP runs a ""functional"" version of _mutable_op.
Unlike AutoFunctionalized, this version is improved to better handle
view tensors. This version is only used in non export mode.",2,['HigherOrderOperator'],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\executorch_call_delegate.py,ExecutorchCallDelegate,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\hints_wrap.py,HintsWrapper,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\out_dtype.py,OutDtypeOperator,"The out_dtype operator takes an existing ATen functional operator, an
`out_dtype` argument, and arguments to the original operator, and executes
the original operator and returns a Tensor with the `out_dtype` precision.
This operator does not mandate a compute precision so it allows the
representation to not be opinionated about the exact implementation.

The general implementation for all operators will be the following:
    1. Promote inputs dtypes based on default PyTorch dtype promotion rules,
        using the dtypes of all input Tensors/Scalars and the `out_dtype`
        arugument.
    2. Execute the operator
    3. Cast the output to `out_dtype`",2,['HigherOrderOperator'],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\run_const_graph.py,RunConstGraph,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\strict_mode.py,StrictMode,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\torchbind.py,CallTorchBind,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\utils.py,UnsupportedAliasMutationException,,0,['RuntimeError'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_higher_order_ops\while_loop.py,WhileLoopOp,,2,['HigherOrderOperator'],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\constant_folding.py,ConstantFolder,,10,['torch.fx.Interpreter'],0,"['__init__', '_support_dynamic_shape', '_deduce_value', 'is_impure', 'node_to_last_non_output_use', 'run_node', 'insertable_tensor_check', 'add_node_replacement', 'run', 'insert_placerholder_values']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\cpp_builder.py,BuildOptionsBase,"This is the Base class for store cxx build options, as a template.
Acturally, to build a cxx shared library. We just need to select a compiler
and maintains the suitable args.",16,[],0,"['__init__', '_process_compile_only_options', '_remove_duplicate_options', '_finalize_options', 'get_compiler', 'get_definations', 'get_include_dirs', 'get_cflags', 'get_ldflags', 'get_libraries_dirs', 'get_libraries', 'get_passthough_args', 'get_aot_mode', 'get_use_absolute_path', 'get_compile_only', 'save_flags_to_file']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\cpp_builder.py,CppOptions,"This class is inherited from BuildOptionsBase, and as cxx build options.
This option need contains basic cxx build option, which contains:
1. OS related args.
2. Toolchains related args.
3. Cxx standard related args.
Note:
1. This Options is good for assist modules build, such as x86_isa_help.",1,['BuildOptionsBase'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\cpp_builder.py,CppTorchOptions,"This class is inherited from CppTorchOptions, which automatic contains
base cxx build options. And then it will maintains torch related build
args.
1. Torch include_directories, libraries, libraries_directories.
2. Python include_directories, libraries, libraries_directories.
3. OpenMP related.
4. Torch MACROs.
5. MISC",1,['CppOptions'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\cpp_builder.py,CppTorchCudaOptions,"This class is inherited from CppTorchOptions, which automatic contains
base cxx build options and torch common build options. And then it will
maintains cuda device related build args.",1,['CppTorchOptions'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\cpp_builder.py,CppBuilder,"CppBuilder is a cpp jit builder, and it supports both Windows, Linux and MacOS.
Args:
    name:
        1. Build target name, the final target file will append extension type automatically.
        2. Due to the CppBuilder is supports mutliple OS, it will maintains ext for OS difference.
    sources:
        Source code file list to be built.
    BuildOption:
        Build options to the builder.
    output_dir:
        1. The output_dir the taget file will output to.
        2. The default value is empty string, and then the use current dir as output dir.
        3. Final target file: output_dir/name.ext",6,[],0,"['__get_python_module_ext', '__get_object_ext', '__init__', 'get_command_line', 'get_target_file_path', 'build']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_utils.py,FakeTensorUpdater,"The main idea here is that it's difficult to maintain accurate fake
tensors (our primary form of metadata) for each node in our graph as we
transform it.

The most reliable way to obtain this information is by rerunning
faketensor propagation. However, in general, faketensor propagation is
fairly expensive. So, instead we'd like to only rerun faketensor
propagation on nodes that have changed.

In order to detect which nodes have changed, we first hash its node,
target, and argument lists (which are immutable in FX).

Then, whenever we call incremental_update, we check which FX nodes have a
new hash, and recompute the faketensor metadata for that node. Then, we
continue to recursively compute the faketensors for all users until the
fake tensors stop changing.",3,[],0,"['__init__', 'hash_node', 'incremental_update']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\sizevars.py,SizeVarAllocator,,38,[],0,"['__init__', 'simplify', 'make_simplify_with_ranges_cache', 'make_simplify_loops_cache', '_simplify_with_ranges', '_simplify_loops_impl', 'is_expr_static_and_true', 'statically_known_equals', 'statically_known_list_equals', 'statically_known_leq', 'statically_known_geq', 'statically_known_lt', 'statically_known_gt', 'statically_known_multiple_of', 'statically_known_power_of_2', 'guard_equals', 'guard_leq', 'guard_lt', 'guarded_order', 'evaluate_expr', 'evaluate_min', 'evaluate_max', 'evaluate_static_shape', 'evaluate_static_shapes', 'remove_precomputed_replacements', 'symbolic_hint', 'size_hint', 'size_hints', '_lru_cache', 'make_stride_vars_cache', '_stride_vars', 'offset_var', 'stride_hints', 'stride_order', 'lookup_precomputed_size', 'free_symbols', 'combine_modular_indexing_pairs', 'expand_floor_div']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\sizevars.py,SimplifyIndexing,"A wrapper around .virtualize.ops that uses var range information to
simplify ModularIndexing/FloorDiv.",6,['V.WrapperHandler'],0,"['__init__', 'load', 'store', 'store_reduction', 'index_expr', 'check_bounds']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\test_case.py,TestCase,"A base TestCase for inductor tests. Enables FX graph caching and isolates
the cache directory for each test.",2,['DynamoTestCase'],0,"['setUp', 'tearDown']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\wrapper_benchmark.py,ProfileEvent,,0,[],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_lazy\closure.py,ClosureHandler,,3,[],0,"['__init__', 'run', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_lazy\closure.py,AsyncClosureHandler,"Handler for Asynchronous Step Closures
Args:
    max_queue_size: The maximum length of the closure queue after which
    the training loop will block until closures are evaluated.
    By default, a reasonable limit of a maximum of 100 on the queue.
    This value can be set using the `XLA_MAX_ASYNC_QUEUE` environment
    variable.",3,['ClosureHandler'],0,"['__init__', 'start_event_loop', 'run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_lazy\extract_compiled_graph.py,GraphInputMatcher,"The GraphInputMatcher class setup the graph inputs for future calls after lazy tracing.
Specifically, those graph inputs corresponding to method parameters should be replaced with the
arguments for the current call.

tensor_id_to_arg_idx maps the tensor id to the parameter index.
graph_input_tensor_ids, graph_input_ivalues list the tensor_id and ivalue for each of the
TS/XLA graph inputs.",1,[],3,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_lazy\extract_compiled_graph.py,ReturnValueHandler,"When ltc_sync_multi is called on multi tensors, the compiled graph
will contain output only for unique tensors - if a tensor appears multiple
times in the input to _ltc_sync_multi, only the first occurance matters.

However from python level, we still expect multi tensors returned with duplciation
even if the TS graph dedup the output. e.g. for method:

  def forward(self, a):
    return a, a

the TS graph captured by LTC will return a single tensor, but Python method expects 2.

This class dedup the lazy tensors first to get the index that will be used
to duplicate the eager tensors later.",2,[],0,"['__init__', 'duplicate_eager_tensors']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_library\autograd.py,InfoProtocol,,0,['Protocol'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_library\autograd.py,Info,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_library\utils.py,Kernel,"Models a (function, source location)",1,[],2,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_library\utils.py,RegistrationHandle,Does something when someone calls .destroy() on it,2,[],0,"['__init__', 'destroy']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_logging\_internal.py,LogRegistry,,10,[],8,"['is_artifact', 'is_log', 'register_log', 'register_artifact_name', 'register_artifact_log', 'register_child_log', 'get_log_qnames', 'get_artifact_log_qnames', 'get_child_log_qnames', 'is_off_by_default']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_logging\_internal.py,LogState,,5,[],2,"['enable_artifact', 'is_artifact_enabled', 'enable_log', 'get_log_level_pairs', 'clear']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_logging\_internal.py,TorchLogsFormatter,,2,['logging.Formatter'],0,"['__init__', 'format']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_logging\_internal.py,LazyTraceHandler,"Like FileHandler, but the file is allocated lazily only upon the first log message",3,['logging.StreamHandler'],0,"['__init__', 'close', 'emit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_logging\_internal.py,LazyString,,2,[],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_numpy\linalg.py,LinAlgError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_numpy\_funcs.py,IndexExpression,"Written by Konrad Hinsen <hinsen@cnrs-orleans.fr>
last revision: 1999-7-23

Cosmetic changes by T. Oliphant 2001",2,[],0,"['__init__', '__getitem__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_numpy\_util.py,AxisError,,0,"['ValueError', 'IndexError']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_numpy\_util.py,UFuncTypeError,,0,"['TypeError', 'RuntimeError']",0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_prims\context.py,TorchRefsMode,"Switches the interpretation of torch.* functions and Tensor methods to
use PrimTorch refs in torch._refs.  (Direct calls to _refs are unaffected.)

>>> # xdoctest: +SKIP
>>> with TorchRefsMode():
...     torch.add(x, y)  # calls torch._refs.add(x, y)

By default, this context manager will fall back on the torch.* if the
ref does not exist; set strict=True to error if this occurs.
If the ref exists we still would like to fall back on the torch.* sometimes,
this behavior can be customized by passing a function to should_fallback_fn.",2,['torch.overrides.TorchFunctionMode'],0,"['__init__', '__torch_function__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_prims_common\wrappers.py,elementwise_type_promotion_wrapper,"Adds elementwise type promotion to a Python reference implementation.

Takes two kwargs, type_promoting_args and type_promotion_kind.

type_promoting_args must be a string Sequence specifiying the argument names of all
arguments that participate in type promotion (and should be type promoted). If the
arg specifies a Sequence-type then every element of the Sequence will participate in
type promotion.

type_promotion_kind must be one of the kinds specified by ELEMENTWISE_TYPE_PROMOTION_KIND.
See its documentation for details.

The return_dtype will be coerced to the wrapped function's dtype arg if it is available and
not None.

Other type promotion behavior, like validating the Python type of scalar arguments, must
be handled separately.",2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_refs\fft.py,_ShapeAndDims,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_refs\fft.py,_CanonicalizeC2rReturn,,0,['NamedTuple'],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_subclasses\fake_utils.py,CrossRefFakeMode,,2,['TorchDispatchMode'],0,"['__init__', '__torch_dispatch__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_subclasses\schema_check_mode.py,SchemaCheckMode,,4,['TorchDispatchMode'],0,"['__init__', 'reset_cache', 'display_ops', '__torch_dispatch__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\ns\_numeric_suite.py,Logger,Base class for stats logging,2,['nn.Module'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\ns\_numeric_suite.py,ShadowLogger,"Class used in Shadow module to record the outputs of the original and
shadow modules.",2,['Logger'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\ns\_numeric_suite.py,OutputLogger,Class used to log the outputs of the module,2,['Logger'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\ns\_numeric_suite.py,Shadow,"Shadow module attaches the float module to its matching quantized module
as the shadow. Then it uses Logger module to process the outputs of both
modules.

Args:
    q_module: module quantized from float_module that we want to shadow
    float_module: float module used to shadow q_module
    logger_cls: type of logger used to process the outputs of q_module and
        float_module. ShadowLogger or custom loggers can be used.",8,['nn.Module'],0,"['__init__', 'forward', 'add', 'add_scalar', 'mul', 'mul_scalar', 'cat', 'add_relu']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\qconfig.py,QConfig,"Describes how to quantize a layer or a part of the network by providing
settings (observer classes) for activations and weights respectively.


Note that QConfig needs to contain observer **classes** (like MinMaxObserver) or a callable that returns
instances on invocation, not the concrete observer instances themselves.
Quantization preparation function will instantiate observers multiple times for each of the layers.


Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`
method (that behaves like functools.partial)::

  my_qconfig = QConfig(
      activation=MinMaxObserver.with_args(dtype=torch.qint8),
      weight=default_observer.with_args(dtype=torch.qint8))",1,"[""namedtuple('QConfig', ['activation', 'weight'])""]",0,['__new__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\qconfig.py,QConfigDynamic,"Describes how to dynamically quantize a layer or a part of the network by providing
settings (observer classes) for weights.

It's like QConfig, but for dynamic quantization.

Note that QConfigDynamic needs to contain observer **classes** (like MinMaxObserver) or a callable that returns
instances on invocation, not the concrete observer instances themselves.
Quantization function will instantiate observers multiple times for each of the layers.

Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`
method (that behaves like functools.partial)::

  my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))",1,"[""namedtuple('QConfigDynamic', ['activation', 'weight'])""]",0,['__new__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\stubs.py,QuantStub,"Quantize stub module, before calibration, this is same as an observer,
it will be swapped as `nnq.Quantize` in `convert`.

Args:
    qconfig: quantization configuration for the tensor,
        if qconfig is not provided, we will get qconfig from parent modules",2,['nn.Module'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\stubs.py,DeQuantStub,"Dequantize stub module, before calibration, this is same as identity,
this will be swapped as `nnq.DeQuantize` in `convert`.

Args:
    qconfig: quantization configuration for the tensor,
        if qconfig is not provided, we will get qconfig from parent modules",2,['nn.Module'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\stubs.py,QuantWrapper,"A wrapper class that wraps the input module, adds QuantStub and
DeQuantStub and surround the call to module with call to quant and dequant
modules.

This is used by the `quantization` utility functions to add the quant and
dequant modules, before `convert` function `QuantStub` will just be observer,
it observes the input tensor, after `convert`, `QuantStub`
will be swapped to `nnq.Quantize` which does actual quantization. Similarly
for `DeQuantStub`.",2,['nn.Module'],3,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\utils.py,MatchAllNode,"A node pattern that matches all nodes, used in defining
fusion patterns in FX Graph Mode Quantization",0,[],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\_correct_bias.py,MeanShadowLogger,"Mean Logger for a Shadow module.

A logger for a Shadow module whose purpose is to record the rolling mean
of the data passed to the floating point and quantized models",3,['ns.Logger'],0,"['__init__', 'forward', 'clear']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\__init__.py,_DerivedObserverOrFakeQuantize,"This observer is used to describe an observer whose quantization parameters
are derived from other observers",3,['ObserverBase'],0,"['__init__', 'forward', 'calculate_qparams']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,_FusedModule,,0,['torch.nn.Sequential'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvReLU1d,"This is a sequential container which calls the Conv1d and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvReLU2d,"This is a sequential container which calls the Conv2d and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvReLU3d,"This is a sequential container which calls the Conv3d and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,LinearReLU,"This is a sequential container which calls the Linear and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvBn1d,"This is a sequential container which calls the Conv 1d and Batch Norm 1d modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvBn2d,"This is a sequential container which calls the Conv 2d and Batch Norm 2d modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvBnReLU1d,"This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvBnReLU2d,"This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvBn3d,"This is a sequential container which calls the Conv 3d and Batch Norm 3d modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvBnReLU3d,"This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,BNReLU2d,"This is a sequential container which calls the BatchNorm 2d and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,BNReLU3d,"This is a sequential container which calls the BatchNorm 3d and ReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,LinearBn1d,"This is a sequential container which calls the Linear and BatchNorm1d modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,LinearLeakyReLU,"This is a sequential container which calls the Linear and LeakyReLU modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,LinearTanh,"This is a sequential container which calls the Linear and Tanh modules.
During quantization this will be replaced with the corresponding fused module.",1,['_FusedModule'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvAdd2d,"This is a sequential container which calls the Conv2d modules with extra Add.
During quantization this will be replaced with the corresponding fused module.",2,['_FusedModule'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\intrinsic\modules\fused.py,ConvAddReLU2d,"This is a sequential container which calls the Conv2d, add, Relu.
During quantization this will be replaced with the corresponding fused module.",2,['_FusedModule'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\qat\dynamic\modules\linear.py,Linear,"A linear module attached with FakeQuantize modules for weight,
used for dynamic quantization aware training.

We adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
for documentation.

Similar to `torch.nn.Linear`, with FakeQuantize modules initialized to
default.",1,['torch.ao.nn.qat.Linear'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\nn\quantized\reference\modules\utils.py,ReferenceQuantizedModule,,5,['torch.nn.Module'],0,"['_init_weight_qparams', 'get_weight', 'get_quantized_weight', '_save_to_state_dict', '_load_from_state_dict']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\ns\fx\n_shadows_utils.py,OutputProp,"Output propagation (modeled from shape propagation).

Given a GraphModule and an example input, saves the output flowing
through each node on `node.traced_result`.

Code based on the example from
https://pytorch.org/docs/stable/fx.html#the-interpreter-pattern",2,[],0,"['__init__', 'propagate']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\scheduler\base_scheduler.py,BaseScheduler,,9,[],0,"['__init__', 'state_dict', 'load_state_dict', 'get_last_sl', 'get_sl', 'print_sl', '__repr__', 'step', '_make_sure_a_list']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\scheduler\lambda_scheduler.py,LambdaSL,"Sets the sparsity level of each parameter group to the final sl
times a given function. When last_epoch=-1, sets initial sl as zero.
Args:
    sparsifier (BaseSparsifier): Wrapped sparsifier.
    sl_lambda (function or list): A function which computes a multiplicative
        factor given an integer parameter epoch, or a list of such
        functions, one for each group in sparsifier.param_groups.
    last_epoch (int): The index of last epoch. Default: -1.
    verbose (bool): If ``True``, prints a message to stdout for
        each update. Default: ``False``.
Example:
    >>> # Assuming sparsifier has two groups.
    >>> lambda1 = lambda epoch: epoch // 30
    >>> lambda2 = lambda epoch: 0.95 ** epoch
    >>> # xdoctest: +SKIP
    >>> scheduler = LambdaSL(sparsifier, sl_lambda=[lambda1, lambda2])
    >>> for epoch in range(100):
    >>>     train(...)
    >>>     validate(...)
    >>>     scheduler.step()",2,['BaseScheduler'],0,"['__init__', 'get_sl']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\sparsifier\nearly_diagonal_sparsifier.py,NearlyDiagonalSparsifier,"Nearly Diagonal Sparsifier

This sparsifier creates a nearly diagonal mask to be applied to the weight matrix.
Nearly Diagonal Matrix is a matrix that contains non-zero elements near the diagonal and the rest are zero.
An example of a nearly diagonal matrix with degree (or nearliness) 3 and 5 are follows respectively.
1 1 0 0       1 1 1 0
1 1 1 0       1 1 1 1
0 1 1 1       1 1 1 1
0 0 1 1       0 1 1 1
Note that a nearly diagonal matrix with degree 1 is just a matrix with main diagonal populated

This sparsifier is controlled by one variable:
1. `nearliness` defines the number of non-zero diagonal lines that are closest to the main diagonal.
    Currently - supports only odd number

Note:
    This can be accelerated (vectorized) once the Spdiagonal feature (PR: #78439) is landed or the banded matrix
    feature is landed: https://stackoverflow.com/questions/52463972/generating-banded-matrices-using-numpy

Args:
    nearliness: The degree of nearliness (default = 1)",2,['base_sparsifier.BaseSparsifier'],0,"['__init__', 'update_mask']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\sparsifier\utils.py,FakeSparsity,"Parametrization for the weights. Should be attached to the 'weight' or
any other parameter that requires a mask applied to it.

Note::

    Once the mask is passed, the variable should not change the id. The
    contents of the mask can change, but the mask reference itself should
    not.",3,['nn.Module'],0,"['__init__', 'forward', 'state_dict']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\sparsifier\weight_norm_sparsifier.py,WeightNormSparsifier,"Weight-Norm Sparsifier

This sparsifier computes the norm of every sparse block and ""zeroes-out"" the
ones with the lowest norm. The level of sparsity defines how many of the
blocks is removed.

This sparsifier is controlled by three variables:
1. `sparsity_level` defines the number of *sparse blocks* that are zeroed-out
2. `sparse_block_shape` defines the shape of the sparse blocks. Note that
    the sparse blocks originate at the zero-index of the tensor.
3. `zeros_per_block` is the number of zeros that we are expecting in each
    sparse block. By default we assume that all elements within a block are
    zeroed-out. However, setting this variable sets the target number of
    zeros per block. The zeros within each block are chosen as the *smallest
    absolute values*.

Args:

    sparsity_level: The target level of sparsity
    sparse_block_shape: The shape of a sparse block (see note below)
    zeros_per_block: Number of zeros in a sparse block
    norm: Norm to use. Could be either `int` or a callable.
        If `int`, only L1 and L2 are implemented.

Note::
    The `sparse_block_shape` is tuple representing (block_ROWS, block_COLS),
    irrespective of what the rows / cols mean in the data tensor. That means,
    if you were to sparsify a weight tensor in the nn.Linear, which has a
    weight shape `(Cout, Cin)`, the `block_ROWS` would refer to the output
    channels, while the `block_COLS` would refer to the input channels.

Note::
    All arguments to the WeightNormSparsifier constructor are ""default""
    arguments and could be overriden by the configuration provided in the
    `prepare` step.",5,['BaseSparsifier'],0,"['__init__', '_scatter_fold_block_mask', '_make_tensor_mask', '_make_block_mask', 'update_mask']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\data_sparsifier\data_norm_sparsifier.py,DataNormSparsifier,"L1-Norm Sparsifier
This sparsifier computes the *L1-norm* of every sparse block and ""zeroes-out"" the
ones with the lowest norm. The level of sparsity defines how many of the
blocks is removed.
This sparsifier is controlled by three variables:
1. `sparsity_level` defines the number of *sparse blocks* that are zeroed-out
2. `sparse_block_shape` defines the shape of the sparse blocks. Note that
    the sparse blocks originate at the zero-index of the tensor.
3. `zeros_per_block` is the number of zeros that we are expecting in each
    sparse block. By default we assume that all elements within a block are
    zeroed-out. However, setting this variable sets the target number of
    zeros per block. The zeros within each block are chosen as the *smallest
    absolute values*.
Args:
    sparsity_level: The target level of sparsity
    sparse_block_shape: The shape of a sparse block
    zeros_per_block: Number of zeros in a sparse block
Note::
    All arguments to the DataNormSparsifier constructor are ""default""
    arguments and could be overriden by the configuration provided in the
    `add_data` step.",5,['BaseDataSparsifier'],0,"['__init__', '__get_scatter_folded_mask', '__get_block_level_mask', '__get_data_level_mask', 'update_mask']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\pruner\base_structured_sparsifier.py,BaseStructuredSparsifier,"Base class for structured pruning.

Abstract methods that need to be implemented:
    - update_mask: Function to compute a new mask for all keys in the
        `groups` attribute.

Args:
    - defaults [dict]: default configurations will be attached to the
        configuration. Only the keys that don't exist in the `config` will
        be updated.",4,['BaseSparsifier'],0,"['__init__', 'make_config_from_model', '_prepare', 'prune']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\pruner\FPGM_pruner.py,FPGMPruner,"Filter Pruning via Geometric Median (FPGM) Structured Pruner
This sparsifier prune fliter (row) in a tensor according to distances among filters according to
`Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration <https://arxiv.org/abs/1811.00250>`_.

This sparsifier is controlled by three variables:
1. `sparsity_level` defines the number of filters (rows) that are zeroed-out.
2. `dist` defines the distance measurement type. Default: 3 (L2 distance).
Available options are: [1, 2, (custom callable distance function)].

Note::
    Inputs should be a 4D convolutional tensor of shape (N, C, H, W).
        - N: output channels size
        - C: input channels size
        - H: height of kernel
        - W: width of kernel",3,['BaseStructuredSparsifier'],0,"['__init__', '_compute_distance', 'update_mask']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\pruner\lstm_saliency_pruner.py,LSTMSaliencyPruner,"Prune packed LSTM weights based on saliency.
For each layer {k} inside a LSTM, we have two packed weight matrices
- weight_ih_l{k}
- weight_hh_l{k}

These tensors pack the weights for the 4 linear layers together for efficiency.

[W_ii | W_if | W_ig | W_io]

Pruning this tensor directly will lead to weights being misassigned when unpacked.
To ensure that each packed linear layer is pruned the same amount:
    1. We split the packed weight into the 4 constituent linear parts
    2. Update the mask for each individual piece using saliency individually

This applies to both weight_ih_l{k} and weight_hh_l{k}.",1,['BaseStructuredSparsifier'],0,['update_mask'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\pruner\parametrization.py,FakeStructuredSparsity,"Parametrization for Structured Pruning. Like FakeSparsity, this should be attached to
the  'weight' or any other parameter that requires a mask.

Instead of an element-wise bool mask, this parameterization uses a row-wise bool mask.",3,['nn.Module'],0,"['__init__', 'forward', 'state_dict']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\pruner\parametrization.py,BiasHook,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\pruner\saliency_pruner.py,SaliencyPruner,"Prune rows based on the saliency (L1 norm) of each row.

This pruner works on N-Dimensional weight tensors.
For each row, we will calculate the saliency, whic is the sum the L1 norm of all weights in that row.
We expect that the resulting saliency vector has the same shape as our mask.
We then pick elements to remove until we reach the target sparsity_level.",1,['BaseStructuredSparsifier'],0,['update_mask'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\data_sparsifier\lightning\callbacks\data_sparsity.py,PostTrainingDataSparsity,"Lightning callback that enables post-training sparsity.

This callback aims to sparsify the model inside lightning module after training.
**Note that the model is copied and then sparsified, so the existing model is not modified**

The sparsified model can be used for comparison and can be accessed using
    <callback_obj>.sparsified

Args:
    data_sparsifier_class (some implemented class of BaseDataSparsifier)
        The data sparsifier object of this class is created when the
        training starts.
        Note: Objects should not be passed in here as they are created
        once the training completes.

    data_sparsifier_args (Dict)
        Dictionary of args to be passed to the data sparsifier.
        Note: data_list arg should be ignored

Hooks implemented:
    on_fit_end()
        1. copies the model and attaches it to the sparsifier
        2. sparsier step() is called
        3. squashes the mask()",2,['pl.callbacks.Callback'],0,"['__init__', 'on_fit_end']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\pruning\_experimental\data_sparsifier\lightning\callbacks\data_sparsity.py,TrainingAwareDataSparsity,"Lightning callback that enables in-training sparsity.

This callback aims to sparsify the model inside lightning module during training.
**Note that the model is copied and then sparsified, so the existing model is not modified**

The sparsified model can be used for comparison and can be accessed using
    <callback_obj>.sparsified

Args:
    data_sparsifier_class (some implemented class of BaseDataSparsifier)
        The data sparsifier object of this class is created when the
        training starts.
        Note: Objects should not be passed in here as they are created
        when the training starts.

    data_sparsifier_args (Dict)
        Dictionary of args to be passed to the data sparsifier.
        Note: data_list arg should be ignored

    data_scheduler_class (some implemented class of BaseDataScheduler)
        The data scheduler of this class is created when the training starts
        Note: Objects should not be passed in here as they are created
        when the training starts.

    data_scheduler_args(Dict)
        Dictionary of args to be passed to the data scheduler.
        **Note: data_sparsifier arg should be ignored as the recipe
        creates and pass sparsifier object into the class**

Hooks implemented:
    on_train_start()
        Data sparsifier and scheduler objects are created.
        Pytorch model attached to the sparsifier

    on_train_epoch_start()
        Loads the state_dict of the data sparsifier

    on_train_epoch_end()
        1. Copies the model and attaches it to the sparsifier
        2. sparsifier step() and scheduler step()
        3. Dump state_dict of the current sparsifier

    on_train_end()
        squash mask",6,['pl.callbacks.Callback'],0,"['__init__', 'on_train_start', 'on_train_epoch_start', '__create_config_based_on_state', 'on_train_epoch_end', 'on_train_end']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\graph_module.py,FusedGraphModule,,2,['GraphModule'],0,"['__init__', '__deepcopy__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\graph_module.py,ObservedGraphModule,,2,['GraphModule'],0,"['__init__', '__deepcopy__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\graph_module.py,ObservedStandaloneGraphModule,,2,['ObservedGraphModule'],0,"['__init__', '__deepcopy__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\graph_module.py,QuantizedGraphModule,"This class is created to make sure PackedParams
(e.g. LinearPackedParams, Conv2dPackedParams) to appear in state_dict
so that we can serialize and deserialize quantized graph module with
torch.save(m.state_dict()) and m.load_state_dict(state_dict)",3,['GraphModule'],0,"['__init__', '_load_from_state_dict', '__deepcopy__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,QuantizeHandler,Base handler class for the quantizer patterns,4,['ABC'],0,"['__init__', 'is_general_tensor_value_op', 'is_custom_module', 'is_standalone_module']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,BinaryOpQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,CatQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,ConvReluQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,LinearReLUQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,BatchNormQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,EmbeddingQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,RNNDynamicQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,DefaultNodeQuantizeHandler,"Common quantized op, first input and first output will be quantized",0,['QuantizeHandler'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,FixedQParamsOpQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,CopyNodeQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,GeneralTensorShapeOpQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,CustomModuleQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\quantize_handler.py,StandaloneModuleQuantizeHandler,,0,['QuantizeHandler'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\tracer.py,ScopeContextManager,,1,['torch.fx.proxy.ScopeContextManager'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\tracer.py,QuantizationTracer,,2,['Tracer'],0,"['__init__', 'is_leaf_module']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\utils.py,ObservedGraphModuleAttrs,,0,[],10,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\pt2e\duplicate_dq_pass.py,DuplicateDQPass,,1,['PassBase'],0,['call'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\pt2e\export_utils.py,_WrapperModule,"Class to wrap a callable in an :class:`torch.nn.Module`. Use this if you
are trying to export a callable.",2,['torch.nn.Module'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\pt2e\port_metadata_pass.py,PortNodeMetaForQDQ,"Port metadata for nodes added by quantization flow.
For static quant these are:
- quantizer_per_tensor.default, dequantize_per_tensor.default
- quantizer_per_channel.default, dequantize_per_channel.default
For dynamic quant these are:
- choose_qparams.tensor
- quantizer_per_tensor.tensor, dequantize_per_tensor.tensor
- quantizer_per_channel.default, dequantize_per_channel.default

Rules of porting metadata:
- Metadata to be ported:
  - nn_module_stack
  - stack_trace
  - quantization_tag
- Metadata to NOT be ported:
  - Everything else
- Rules:
  - Statically quantized patterns:
    - Dequantize nodes on the inputs to be quantized inherit metadata of the consumer node.
    - Quantize nodes on the outputs inherit metadata of the producer node.
    - Example 1:
      - Original: [Conv -> AvgPool -> Linear]
      - Quantized [Q-> DQ -> Conv -> Q -> DQ -> AvgPool -> Q -> DQ -> Linear -> Q -> DQ]
      - Inner brackets specify which nodes Q/DQ inherit metdata from
      - [Q-> [DQ -> Conv -> Q] -> [DQ -> AvgPool -> Q] -> [DQ -> Linear -> Q] -> DQ]
      - Note first Q and last DQ do not inherit metadata from any nodes
    - Example 2:
      - Original: [Conv -> AvgPool -> Linear]
      - AvgPool is not quantized
      - Quantized [Q-> DQ -> Conv -> Q -> DQ -> AvgPool -> Q -> DQ -> Linear -> Q -> DQ]
      - Inner brackets specify which nodes Q/DQ inherit metdata from
      - [Q-> [DQ -> Conv -> Q] -> DQ -> [AvgPool] -> Q -> [DQ -> Linear -> Q] -> DQ]
      - Note DQ and Q nodes around AvgPool do not inherit metadata from AvgPool because
        AvgPool was not supposed to be quantized. Metadata porting relies on quantization_annotation
        on the nodes (in this case AvgPool node) to conclude if the node or patter was
        supposed to be quantized. And subsequntly decide if the preceding Q, if any, should
        inherit metadata from AvgPool.
  - Dynamically quantized patterns:
    - Input that are dynamically quantized have choose_qparams, quantize and dequantize nodes
    - For example, below linear is dynamically quantized while rest statically:
      - Original: [Conv -> AvgPool -> Linear]
      - Quantized [Q-> DQ -> Conv -> Q -> DQ -> AvgPool -> Q -> DQ -> choose_params -> Q -> DQ -> Linear]
      - Quantized [Q-> [DQ -> Conv -> Q] -> [DQ -> AvgPool -> Q] -> DQ -> [choose_params -> Q -> DQ -> Linear]]
      - Note first Q does not inherit metadata from any nodes
NB:
- The best place for porting metadata is during observer conversion to q/dq. This is because it precisely
  knows which quantization spec is converted to q/dq and thus from where the metadata should be ported.
  However, since FX and PT2E quant workflow are on a common code-base, this hurts readability quite a bit.
  Doing it via a separate pass, helps readability of the code. Once we are able to refactor PT2E quant
  code, this pass should like to be integrated in the refactored variant of ""convert"" step.",1,['PassBase'],0,['call'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\quantizer\composable_quantizer.py,ComposableQuantizer,"ComposableQuantizer allows users to combine more than one quantizer into a single quantizer.
This allows users to quantize a model with multiple quantizers. E.g., embedding quantization
maybe supported by one quantizer while linear layers and other ops might be supported by another
quantizer.

ComposableQuantizer is initialized with a list of `Quantizer` instances.
The order of the composition matters since that is the order in which the quantizers will be
applies.
Example:
```
embedding_quantizer = EmbeddingQuantizer()
linear_quantizer = MyLinearQuantizer()
xnnpack_quantizer = XNNPackQuantizer() # to handle ops not quantized by previous two quantizers
composed_quantizer = ComposableQuantizer([embedding_quantizer, linear_quantizer, xnnpack_quantizer])
prepared_m = prepare_pt2e(model, composed_quantizer)
```",5,['Quantizer'],0,"['__init__', '_record_and_validate_annotations', 'annotate', 'transform_for_annotation', 'validate']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\quantizer\xnnpack_quantizer_utils.py,QuantizationConfig,,0,[],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\quantizer\xnnpack_quantizer_utils.py,OperatorConfig,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\fx\_model_report\model_report.py,ModelReport,"The ModelReport class aims to provide users an easy way to diagnose issues that they run into
with their models. The class works with all traceable GraphModules to help diagnose issues,
though the requirements on the type of model more-so depends on the specific report the user
is trying to generate. With respect to the reports, the ModelReport class is initialized with
a set of Detector classes, each of which generate reports on quantization configuration
issues a use might have.

Currently supports generating reports on:
- Suggestions for per-channel vs. per-tensor quantization (nn.Module)
- Suggestions for dynamic vs static quantization for linear layers (Graph Modules)
- Suggestions for input-weight equalization for linear and conv layers (Graph Modules)
- Suggestions for outlier detection for all layers (Graph Modules)

The ModelReport class has the primary functionality of inserting observers (primarily the ModelReportObserver)
where needed for each detector to gather the information it needs, and then after callibration, the ModelReport
class compiles the report generated by each Detector class into a single report to return to the user. It also
has the capability to remove all the observers it inserted as well.

* :attr:`_model` The model we wish to generate the report for. Must be a traceable GraphModule

* :attr:`_desired_report_detectors` The set of Detectors representing desired reports from the ModelReport class
    Make sure that these are all unique types of detectors [do not have more than 1 of the same class]

* :attr:`_desired_detector_names` The set of detector names of the _desired_report_detectors.
    This set is generated by calling the get_detector_name() of each detector

* :attr:`_detector_name_to_observer_fqns` The mapping from each detector to fqns of observers of interest
    The purpose of this is to keep track of what observers were inserted for each detector, so that they
    can be removed at the end if desired

* :attr:`_prepared_flag` A boolean flag that keeps track of whether we have prepared the model or not
    This is to ensure we only insert observers once with the ModelReport instance

* :attr:`_removed_observers` A boolean to track if we have removed observers already
    The purpose is to ensure we don't attempt to remove observers twice with the same ModelReport
    instance. This also allows the functionality where we can generate the report multiple times
    as long as we haven't removed the observers yet.

Note:
    This class was initially designed to work with the Fx Graph Mode workflow in mind. However,
    full functionality is available as long as there is a traceable GraphModule that is being used.
    One method to get a traceable GraphModule without going through the Fx workflow is to use
    the QuantizationTracer class.

General Flow for Fx workflow:
1.) Initialize ModelReport object with reports of interest by passing in initialized detector objects and model
2.) Prepare your model with prepare_fx
3.) Call model_report.prepare_detailed_calibration to add relevant observers
4.) Callibrate your model with data
5.) Call model_report.generate_report on your model to generate report and optionally remove added observers
Optional
    6.) Call model_report.generate_visualizer to get a ModelReportVisualizer instance
    7.) To help in parsing report information and debugging, view report info as a:
        - Table
        - Histogram
        - Line plot
8.) Call model_report.generate_qconfigs to generate the qconfigs based on the report suggestions

Example (with QuantizationTracer):
    >>> # xdoctest: +SKIP
    >>> # get the necessary qconfig
    >>> config = PrepareCustomConfig()
    >>> skipped_module_names, skipped_module_classes = get_skipped_module_name_and_classes(config, False)

    >>> # initialize our model and get GraphModule
    >>> model = SomeModel()
    >>> tracer = QuantizationTracer(skipped_module_names, skipped_module_classes)
    >>> graph_module = GraphModule(model, tracer.trace(model))

    >>> # get our set of detectors and ModelReport instance
    >>> detector_set = set([DynamicStaticDetector(tolerance=0.5), InputWeightEqualizationDetector(ratio_threshold=0.7)])
    >>> tracer_reporter = ModelReport(graph_module, tracer_detector_set)

    >>> # now we insert the observers and callibrate the model
    >>> tracer_model_with_observers = tracer_reporter.prepare_detailed_calibration()
    >>> for i in range(num_callibration_batches):
    >>>     example_input = get_callibration_input()
    >>>     tracer_model_with_observers(example_input)

    >>> # finally we generate the reports and optionally remove the observers we inserted
    >>> reports = tracer_reporter.generate_model_report(remove_inserted_observers=True)

    >>> # Optional: we can generate the qconfig mapping based on the suggestions
    >>> qconfigs = model_report.generate_qconfig_mapping()

    >>> # Optional: we can generate the equalization mapping based on the suggestions
    >>> qconfigs = model_report.generate_equalization_mapping()

    >>> # Optional: we get a ModelReportVisualizer instance to do any visualizations desired
    >>> model_report_visualizer = tracer_reporter.generate_visualizer()",18,[],0,"['__init__', 'get_desired_reports_names', 'get_observers_of_interest', 'prepare_detailed_calibration', '_insert_observer_around_module', '_get_node_from_fqn', 'generate_model_report', '_is_same_info_for_same_key', '_reformat_reports_for_visualizer', 'generate_visualizer', '_generate_qconfig_mapping_helper', '_update_detector_quantizaiton_qconfig_info', '_update_detector_equalization_qconfig_info', '_generate_module_fqn_to_detector_info_mapping', 'generate_qconfig_mapping', '_quantization_config_generator', '_equalization_config_generator', 'generate_equalization_mapping']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\ao\quantization\pt2e\representation\rewrite.py,_RewriteInfo,"Data needed for rewrite, this includes example inputs, pattern and replacement functions
and post transformation functions for the exported pattern and replacement GraphModule",0,[],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\backends\cudnn\rnn.py,Unserializable,,4,[],0,"['__init__', 'get', '__getstate__', '__setstate__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\backends\mkl\__init__.py,verbose,"On-demand oneMKL verbosing functionality.

To make it easier to debug performance issues, oneMKL can dump verbose
messages containing execution information like duration while executing
the kernel. The verbosing functionality can be invoked via an environment
variable named `MKL_VERBOSE`. However, this methodology dumps messages in
all steps. Those are a large amount of verbose messages. Moreover, for
investigating the performance issues, generally taking verbose messages
for one single iteration is enough. This on-demand verbosing functionality
makes it possible to control scope for verbose message dumping. In the
following example, verbose messages will be dumped out for the second
inference only.

.. highlight:: python
.. code-block:: python

    import torch
    model(data)
    with torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):
        model(data)

Args:
    level: Verbose level
        - ``VERBOSE_OFF``: Disable verbosing
        - ``VERBOSE_ON``:  Enable verbosing",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\autograd\__init__.py,context,"Context object to wrap forward and backward passes when using
distributed autograd. The ``context_id`` generated in the ``with``
statement  is required to uniquely identify a distributed backward pass
on all workers. Each worker stores metadata associated with this
``context_id``, which is required to correctly execute a distributed
autograd pass.

Example::
    >>> # xdoctest: +SKIP
    >>> import torch.distributed.autograd as dist_autograd
    >>> with dist_autograd.context() as context_id:
    >>>     t1 = torch.rand((3, 3), requires_grad=True)
    >>>     t2 = torch.rand((3, 3), requires_grad=True)
    >>>     loss = rpc.rpc_sync(""worker1"", torch.add, args=(t1, t2)).sum()
    >>>     dist_autograd.backward(context_id, [loss])",2,[],0,"['__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\default_planner.py,DefaultSavePlanner,,8,['SavePlanner'],1,"['__init__', 'set_up_planner', 'create_local_plan', 'create_global_plan', 'finish_plan', 'resolve_data', 'lookup_object', 'transform_object']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\default_planner.py,DefaultLoadPlanner,"DefaultLoadPlanner that adds multiple features on top of LoadPlanner.

In particular it adds the following:

flatten_state_dict: Handle state_dict with nested dicts
flatten_sharded_tensors: For FSDP in 2D parallel mode
allow_partial_load: If False, will raise a runtime error if a key is present in state_dict, but not in the checkpoint.",10,['LoadPlanner'],2,"['__init__', 'set_up_planner', 'create_local_plan', 'create_global_plan', 'finish_plan', 'load_bytes', 'resolve_tensor', 'commit_tensor', 'lookup_tensor', 'transform_tensor']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\default_planner.py,_EmptyStateDictLoadPlanner,"Extension of DefaultLoadPlanner, which rebuilds state_dict from the saved metadata.
Useful for loading in state_dict without first initializing a model, such as
when converting a DCP checkpoint into a Torch save file.

. N.B. `state_dict` must be an empty dictionary when used with this LoadPlanner

.. warning::
    Because the entire state dict is initialized, It's recommended to only utilize
    this LoadPlanner on a single rank or process to avoid OOM.",3,['DefaultLoadPlanner'],0,"['__init__', '_should_include_key', 'set_up_planner']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\optimizer.py,_ReaderWithOffset,,3,['DefaultLoadPlanner'],3,"['__init__', 'create_local_plan', 'lookup_tensor']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\stateful.py,Stateful,Stateful protocol for objects that can be checkpointed and restored.,2,['Protocol'],0,"['state_dict', 'load_state_dict']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\state_dict.py,StateDictOptions,"This dataclass specifies how get_state_dict/set_state_dict will work.

- ``full_state_dict``: if this is set to True, all the tensors in the
  returned state_dict will be gathered. No ShardedTensor and DTensor
  will be in the returned state_dict.

- ``cpu_offload``: offload all the tensors to cpu. To prevent CPU OOM, if
  ``full_state_dict`` is also true, then only the rank0 will get the
  state_dict and all other ranks will get empty state_dict.

- ``ignore_frozen_params``: if the value is True, the returned state_dict
  won't contain any frozen parameters -- the ``requires_grad`` is False.
  The default value is False.

- ``keep_submodule_prefixes`` (deprecated): when ``submodules`` is not None, this option
  indicates whether to keep the submodule prefixes from the state_dict keys.
  or example, if the submodule is ``module.pretrain`` and the full FQN of
  the parameter is ``pretrain.layer1.weight`` of the param. When this option
  is True, the parameter's key in the returned state_dict will be
  ``pretrain.layer1.weight``. If the options is False, the key will be
  ``layer1.weight``.
  Note that if ``keep_submodule_prefixes`` is False, there may be conflicted
  FQNs, hence there should be only one submodule in ``submodules``.

- ``strict``: the ``strict`` option when ``set_state_dict`` calls
  model.load_state_dict().

- ``broadcast_from_rank0``: when the option is True, rank0 should receive a
   full state_dict and will broadcast the tensors in the state_dict/
   optim_state_dict one by one to other ranks. Other ranks will receive
   the tensors and shard according to the local shards in the model and
   optimizer. ``full_state_dict`` must be set to True when using this option.
   This option currently only supports DTensor, not the legacy ShardedTensor.",0,[],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\state_dict.py,_StateDictInfo,,0,['StateDictOptions'],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\state_dict.py,_EXTRA_STATE,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\utils.py,_DistWrapper,"This is a wrapper around PG that provides a series of features around object collectives.

It works without distributed initialized, where most collectives turns into nops.

All variants that take functions are exception robust, meaning that if one or more
ranks raise errors, all ranks will observe those.",11,[],0,"['__init__', 'get_rank', 'get_world_size', 'broadcast_object', 'gather_object', 'all_gather_object', 'scatter_object', 'reduce_scatter', 'all_reduce', 'all_gather', 'broadcast']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\utils.py,_ReaderView,,7,['io.IOBase'],0,"['__init__', 'seek', 'tell', 'readable', 'seekable', 'readinto', 'read']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\checkpoint\_checkpointer.py,_Checkpointer,"This base class specefies a high level API for saving and loading
distributed `state_dict` 's. It provides an abstraction over the low-level APIs
provided by :py:mod:`torch.distributed.checkpoint.storage`, essentially calling
:py:meth: `torch.distributed.state_dict_saver.save` and
:py:meth: `torch.distributed.state_dict_loader.load` with the provided storage
readers and writers.

.. warning::
    This feature is experimental and subject to removal/change.",4,[],0,"['__init__', 'save', 'async_save', 'load']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\fsdp\_limiter_utils.py,_FreeEventQueue,"This tracks all pending frees corresponding to inflight all-gathers. The
queueing pattern is iterative enqueues with a single dequeue per iteration
once the limit ``_max_num_inflight_all_gathers`` is reached.",4,[],0,"['__init__', 'enqueue', 'dequeue_if_needed', '_dequeue']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\fsdp\_optim_utils.py,FSDPParamInfo,,0,[],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\fsdp\_optim_utils.py,_ConsolidatedOptimState,"This holds the consolidated optimizer state on the target rank. Positive-
dimension tensor state is communicated across ranks, while zero-dimension
tensor state and non-tensor state is taken directly from the target rank.

PyTorch version 1.12 moved to using zero-dimension tensors for scalar
values, but user implemented optimizers may still use float (i.e. a
non-tensor). Thus, we support both and handle them identically.

Attributes:
    tensor_state (Dict[str, torch.Tensor]): Mapping from positive-dimension
        tensor state name to the unsharded flat tensor representing the
        state.
    zero_dim_tensor_state (Dict[str, torch.Tensor]): Mapping from zero-
        dimension tensor state name to its value.
    non_tensor_state (Dict[str, Any]): Mapping from non-tensor state
        name to its value.",0,[],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\fsdp\_optim_utils.py,_PosDimTensorInfo,"Meatadata for positive-dimension tensors used internally for
:meth:`scatter_full_optim_state_dict`.

Attributes:
    shape (torch.Size): Sharded tensor shape (which is equal to the
        unsharded tensor shape if the tensor is optimizer state for a
        non-FSDP parameter and is hence not sharded).
    dtype (torch.dtype): Data type of the tensor.",0,['NamedTuple'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\fsdp\_optim_utils.py,_OptimStateKey,"This represents an optimizer state key that may be used commonly across
ranks. It is based on the unflattened parameter names rather than parameter
IDs to make it independent of each rank's own optimizer construction.",0,['NamedTuple'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\fsdp\_optim_utils.py,StateInfo,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\launcher\api.py,LaunchConfig,"Creates a rendezvous config.

Args:
    min_nodes: Minimum amount of nodes that the user function will
                    be launched on. Elastic agent ensures that the user
                    function start only when the min_nodes amount enters
                    the rendezvous.
    max_nodes: Maximum amount of nodes that the user function
                    will be launched on.
    nproc_per_node: On each node the elastic agent will launch
                        this amount of workers that will execute user
                        defined function.
    rdzv_backend: rdzv_backend to use in the rendezvous (zeus-adapter, etcd).
    rdzv_endpoint: The endpoint of the rdzv sync. storage.
    rdzv_configs: Key, value pair that specifies rendezvous specific configuration.
    rdzv_timeout: Legacy argument that specifies timeout for the rendezvous. It is going
        to be removed in future versions, see the note below. The default timeout is 900 seconds.
    run_id: The unique run id of the job (if not passed a unique one will be
            deduced from run environment - flow workflow id in flow - or auto generated).
    role: User defined role of the worker (defaults to ""trainer"").
    max_restarts: The maximum amount of restarts that elastic agent will conduct
                on workers before failure.
    monitor_interval: The interval in seconds that is used by the elastic_agent
                    as a period of monitoring workers.
    start_method: The method is used by the elastic agent to start the
                workers (spawn, fork, forkserver).
    metrics_cfg: configuration to initialize metrics.
    local_addr: address of the local node if any. If not set, a lookup on the local
            machine's FQDN will be performed.
    local_ranks_filter: ranks for which to show logs in console. If not set, show from all.
..note:
    `rdzv_timeout` is a legacy argument that will be removed in future.
    Set the timeout via `rdzv_configs['timeout']`",1,[],16,['__post_init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\launcher\api.py,elastic_launch,"Launches an torchelastic agent on the container that invoked the entrypoint.

    1. Pass the ``entrypoint`` arguments as non ``kwargs`` (e.g. no named parameters)/
       ``entrypoint`` can be a function or a command.
    2. The return value is a map of each worker's output mapped
       by their respective global rank.

Usage

::

def worker_fn(foo):
    # ...

def main():
    # entrypoint is a function.
    outputs = elastic_launch(LaunchConfig, worker_fn)(foo)
    # return rank 0's output
    return outputs[0]

    # entrypoint is a command and ``script.py`` is the python module.
    outputs = elastic_launch(LaunchConfig, ""script.py"")(args)
    outputs = elastic_launch(LaunchConfig, ""python"")(""script.py"")",2,[],0,"['__init__', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\optim\functional_adadelta.py,_FunctionalAdadelta,,2,[],0,"['__init__', 'step']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\optim\functional_adagrad.py,_FunctionalAdagrad,,2,[],0,"['__init__', 'step']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\optim\functional_adam.py,_FunctionalAdam,,3,[],0,"['__init__', 'step_param', 'step']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\optim\functional_adamax.py,_FunctionalAdamax,,2,[],0,"['__init__', 'step']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\optim\functional_adamw.py,_FunctionalAdamW,,3,[],0,"['__init__', 'step_param', 'step']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\optim\functional_rmsprop.py,_FunctionalRMSprop,,2,[],0,"['__init__', 'step']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\optim\functional_rprop.py,_FunctionalRprop,,2,[],0,"['__init__', 'step']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\optim\functional_sgd.py,_FunctionalSGD,,3,[],0,"['__init__', 'step_param', 'step']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\pipelining\_backward.py,Holder,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\pipelining\_utils.py,PipeliningShapeError,Shape mismatch between configured and runtime values.,0,['RuntimeError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\pipelining\_utils.py,PipeInfo,Captures information for a pipeline (`Pipe` object).,0,[],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\rpc\api.py,AllGatherStates,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\rpc\options.py,TensorPipeRpcBackendOptions,"The backend options for
:class:`~torch.distributed.rpc.TensorPipeAgent`, derived from
:class:`~torch.distributed.rpc.RpcBackendOptions`.

Args:
    num_worker_threads (int, optional): The number of threads in the
        thread-pool used by
        :class:`~torch.distributed.rpc.TensorPipeAgent` to execute
        requests (default: 16).
    rpc_timeout (float, optional): The default timeout, in seconds,
        for RPC requests (default: 60 seconds). If the RPC has not
        completed in this timeframe, an exception indicating so will
        be raised. Callers can override this timeout for individual
        RPCs in :meth:`~torch.distributed.rpc.rpc_sync` and
        :meth:`~torch.distributed.rpc.rpc_async` if necessary.
    init_method (str, optional): The URL to initialize the distributed
        store used for rendezvous. It takes any value accepted for the
        same argument of :meth:`~torch.distributed.init_process_group`
        (default: ``env://``).
    device_maps (Dict[str, Dict], optional): Device placement mappings from
        this worker to the callee. Key is the callee worker name and value
        the dictionary (``Dict`` of ``int``, ``str``, or ``torch.device``)
        that maps this worker's devices to the callee worker's devices.
        (default: ``None``)
    devices (List[int, str, or ``torch.device``], optional): all local
        CUDA devices used by RPC agent. By Default, it will be initialized
        to all local devices from its own ``device_maps`` and corresponding
        devices from its peers' ``device_maps``. When processing CUDA RPC
        requests, the agent will properly synchronize CUDA streams for
        all devices in this ``List``.",3,['_TensorPipeRpcBackendOptionsBase'],0,"['__init__', 'set_device_map', 'set_devices']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\rpc\rref_proxy.py,RRefProxy,,2,[],0,"['__init__', '__getattr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\rpc\server_process_global_profiler.py,_server_process_global_profile,"It has the same API as ``torch.autograd.profiler.profile`` class,
except that it enables profiling on all threads running RPC server request callbacks.

Context manager that manages autograd profiler state and holds a summary of results.
Under the hood it just records events of functions being executed in C++ and
exposes those events to Python. You can wrap any code into it and it will
only report runtime of PyTorch functions.
Note: profiler is thread local and is automatically propagated into the async tasks

Args:
    enabled (bool, optional): Setting this to False makes this context manager a no-op.
        Default: ``True``.

    use_cuda (bool, optional): Enables timing of CUDA events as well using the cudaEvent API.
        Adds approximately 4us of overhead to each tensor operation.
        Default: ``False``

    record_shapes (bool, optional): If shapes recording is set, information
        about input dimensions will be collected. This allows one to see which
        dimensions have been used under the hood and further group by them
        using prof.key_averages(group_by_input_shape=True). Please note that
        shape recording might skew your profiling data. It is recommended to
        use separate runs with and without shape recording to validate the timing.
        Most likely the skew will be negligible for bottom most events (in a case
        of nested function calls). But for higher level functions the total
        self cpu time might be artificially increased because of the shape
        collection.

    profile_memory (bool, optional): Whether to report memory usage, default: ``False``

.. warning:
    Enabling memory profiling incurs additional profiler overhead

.. warning:
    Due to some CUDA multiprocessing limitations (multiprocessing-cuda-note_),
    one cannot use the profiler with ``use_cuda = True`` to benchmark
    DataLoaders with ``num_workers > 0``. If you wish to benchmark data loading,
    please use ``use_cuda = False`` or ``num_workers = 0``.

Example:
    >>> # xdoctest: +SKIP
    >>> # On worker 0:
    >>> import torch
    >>> import torch.distributed.rpc as rpc
    >>> rpc.init_rpc(""worker0"", rank=0, world_size=2)
    >>> x, y = torch.tensor(1), torch.tensor(2)
    >>> outer_profile_rref = rpc.remote(dst_worker_name, rpc._server_process_global_profile)
    >>> outer_profile_rref.rpc_sync().__enter__()
    >>> rpc.rpc_sync(dst_worker_name, torch.add, (x, y))
    >>> inner_profile_rref = rpc.remote(dst_worker_name, rpc._server_process_global_profile)
    >>> inner_profile_rref.rpc_sync().__enter__()
    >>> rpc.rpc_sync(dst_worker_name, torch.sub, (x, y))
    >>> inner_profile_rref.rpc_sync().__exit__(None, None, None)
    >>> outer_profile_rref.rpc_sync().__exit__(None, None, None)
    >>> print(inner_profile_rref.rpc_sync().key_averages())
    ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
    Name       Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls
    ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
    sub        85.06%           76.275us         100.00%          89.667us         89.667us         1
    empty      14.94%           13.392us         14.94%           13.392us         13.392us         1
    ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
    Self CPU time total: 89.667us
    >>> print(outer_profile_rref.rpc_sync().key_averages())
    ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
    Name       Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls
    ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
    sub        35.65%           76.275us         41.91%           89.667us         89.667us         1
    empty      12.67%           27.101us         12.67%           27.101us         13.551us         2
    add        51.68%           110.550us        58.09%           124.259us        124.259us        1
    ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
    Self CPU time total: 213.926us
    >>> rpc.shutdown()

    >>> # On worker 1:
    >>> import torch.distributed.rpc as rpc
    >>> rpc.init_rpc(""worker1"", rank=1, world_size=2)
    >>> # wait for worker 0 to finish work, and then shutdown.
    >>> rpc.shutdown()",3,['profile'],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\contract.py,RegistryItem,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\replicate.py,_ReplicateState,,8,['_State'],0,"['__init__', '_collect_params', 'lazy_init', 'init', 'register_comm_hook', 'record_init_args', 'forward_pre_hook', 'forward_post_hook']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\replicate.py,DDP,,3,[],0,"['__new__', 'set_requires_gradient_sync', 'register_comm_hook']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_symmetric_memory\__init__.py,Work,,2,['_Work'],0,"['__init__', 'wait']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\algorithms\ddp_comm_hooks\mixed_precision_hooks.py,_AllreduceUpcastHookState,"State to manage DDP mixed precision in backward / gradient communication.

This contains a weakref to the DDP module for access to reducer and process
group, and a stream to run parameter and gradient upcasts.",0,[],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\algorithms\model_averaging\hierarchical_model_averager.py,HierarchicalModelAverager,"Runs hierarchical model averaging (`hierarchical SGD <https://arxiv.org/pdf/2010.12998.pdf>`_).

Process groups of different sizes are organized in a hierarchy, and they average parameters
by using different periods concurrently after the warm-up stage.
This is an extension of :class:`~torch.distributed.algorithms.model_averaging.averagers.PeriodicModelAverager`
that supports `post-local SGD <https://arxiv.org/abs/1808.07217>`_, which essentially only supports
a two-level hierarchy: the intra-machine level and the global level, where the intra-machine
level is usually embedded in :meth:`~torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook`.
Similarly, the process groups within this class do not have such an intra-machine process
subgroup, which should be embedded by the post-local SGD communication hook instead.

Args:
    period_group_size_dict: An ordered dict mapping keys of model averaging period to
                            process group size, used for initializing process groups of
                            different sizes in a hierarchy to average parameters concurrently.
                            Particularly, at each iteration, there will be at most a single
                            process group that runs averaging -- the period of such group should
                            have the largest period which the current step can be divided by.
                            For example, if the dict has three keys: 2, 4, and 8,
                            then this means totally three process groups will be created to
                            average parameters every 2, 4, and 8 iterations, respectively.
                            At the 4th iteration, only the second process group will run
                            averaging, because the first process group should be a
                            subset of the second process group, and no need to execute the first
                            process group redundantly.
                            On the other hand, the third process group can only be triggered
                            every 8 iterations, so it will not be triggered at the 4th iteration.
    warmup_steps (int): The number of warm-up steps. During this stage, model averaging is skipped.
    process_group (ProcessGroup, optional): The overall process group containing all the processes that runs model averaging.
                                            If ``None``, the default process group, which is created
                                            by :func:`torch.distributed.init_process_group`, will be used.
                                            (default: ``None``)

Example::
    >>> # xdoctest: +SKIP('undefined rank')
    >>> from collections import OrderedDict
    >>> import torch
    >>> import torch.distributed as dist
    >>> from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (
    >>>     PostLocalSGDState,
    >>>     post_localSGD_hook,
    >>> )
    >>> import torch.distributed.algorithms.model_averaging.hierarchical_model_averager as hierarchicalSGD
    >>> import torch.nn as nn
    >>>
    >>> dist.init_process_group(""nccl"", rank=rank, world_size=16)
    >>> torch.cuda.set_device(rank)
    >>> module = nn.Linear(1, 1, bias=False).to(rank)
    >>> model = nn.parallel.DistributedDataParallel(
    >>>    module, device_ids=[rank], output_device=rank
    >>> )
    >>> # Register a post-localSGD communication hook.
    >>> # Assume that each machine has 4 GPUs, then each intra-machine subgroup has a size of 4.
    >>> subgroup, _ = dist.new_subgroups()
    >>> state = PostLocalSGDState(process_group=None, subgroup=subgroup, start_localSGD_iter=100)
    >>> model.register_comm_hook(state, post_localSGD_hook)
    >>>
    >>> # Average parameters among each group of 8 processes every 4 iterations, and among all
    >>> # the 16 processes every 16 iterations.
    >>> averager = hierarchicalSGD.HierarchicalModelAverager(
    >>>     period_group_size_dict=OrderedDict([(4, 8), (16, 16)]), warmup_steps=100)
    >>> # Note that ``warmup_steps`` must be the same as ``start_localSGD_iter`` used in ``PostLocalSGDState``.
    >>> # In the first 100 steps, run global gradient averaging like normal DDP at every step.
    >>> # After 100 steps, run model averaging at two levels.
    >>> for step in range(0, 200):
    >>>    optimizer.zero_grad()
    >>>    loss = loss_fn(output, labels)
    >>>    loss.backward()
    >>>    optimizer.step()
    >>>    # Average parameters after ``optimizer.step()``.
    >>>    # Thus, the inter-node communication only occurs periodically after ``warmup_steps``.
    >>>    averager.average_parameters(model.parameters())

.. warning ::
    The last group size in the dict must be the size of the provided ``process_group``,
    which indicates model averaging at the highest level of the hierarchy.
    If ``process_group`` is not provided, then the last group size should be equal to the world size.

.. warning ::
    `HierarchicalModelAverager` is experimental and subject to change.",3,['averagers.ModelAverager'],0,"['__init__', '_find_process_group', 'average_parameters']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\multiprocessing\tail_log.py,TailLog,"Tail the given log files.

The log files do not have to exist when the ``start()`` method is called. The tail-er will gracefully wait until
the log files are created by the producer and will tail the contents of the
log files until the ``stop()`` method is called.

.. warning:: ``TailLog`` will wait indefinitely for the log file to be created!

Each log file's line will be suffixed with a header of the form: ``[{name}{idx}]:``,
where the ``name`` is user-provided and ``idx`` is the index of the log file
in the ``log_files`` mapping. ``log_line_prefixes`` can be used to override the
header for each log file.

Usage:

::

 log_files = {0: ""/tmp/0_stdout.log"", 1: ""/tmp/1_stdout.log""}
 tailer = TailLog(""trainer"", log_files, sys.stdout).start()
 # actually run the trainers to produce 0_stdout.log and 1_stdout.log
 run_trainers()
 tailer.stop()

 # once run_trainers() start writing the ##_stdout.log files
 # the tailer will print to sys.stdout:
 # >>> [trainer0]:log_line1
 # >>> [trainer1]:log_line1
 # >>> [trainer0]:log_line2
 # >>> [trainer0]:log_line3
 # >>> [trainer1]:log_line2

.. note:: Due to buffering log lines between files may not necessarily
          be printed out in order. You should configure your application's
          logger to suffix each log line with a proper timestamp.",4,[],0,"['__init__', 'start', 'stop', 'stopped']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\rendezvous\etcd_rendezvous.py,EtcdRendezvousRetryableFailure,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\rendezvous\etcd_rendezvous.py,EtcdRendezvousRetryImmediately,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\rendezvous\etcd_rendezvous.py,EtcdRendezvousHandler,"Implements a
:py:class:`torch.distributed.elastic.rendezvous.RendezvousHandler` interface
backed by
:py:class:`torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous`.
``EtcdRendezvousHandler`` uses a URL to configure the type of rendezvous to
use and to pass implementation specific configurations to the rendezvous
module. The basic etcd rendezvous configuration URL looks like the following
::

 etcd://<etcd_address>:<port>/<job_id>?min_workers=<min_workers>&max_workers=<max_workers>  # noqa: W605

 -- example --

 etcd://localhost:2379/1234?min_workers=1&max_workers=3

The URL above is interpreted as follows:

1. Use the rendezvous handler that is registered with the ``etcd``
   scheme
2. The ``etcd`` endpoint to use is ``localhost:2379``
3. ``job_id == 1234`` is used as the prefix in etcd (this allows one to
   share a common etcd server for multiple jobs so long as the
   ``job_ids`` are guaranteed to be unique). Note that the job id can be
   any string (e.g. does not need to be a number) as long as it is
   unique.
4. ``min_workers=1`` and ``max_workers=3`` specifies a range for
   membership size - Torch Distributed Elastic starts running the job as
   long as the cluster size is greater than or equal to ``min_workers``
   and admits up to ``max_workers`` into the cluster.

Below are a full list of the parameters that can be passed to etcd
rendezvous:

+--------------------------------------------+--------------------------+
| Parameter                                  | Description              |
+============================================+==========================+
| min_workers                                | minimum number of        |
|                                            | workers for the          |
|                                            | rendezvous to be valid   |
+--------------------------------------------+--------------------------+
| max_workers                                | maximum number of        |
|                                            | workers to admit         |
+--------------------------------------------+--------------------------+
| timeout                                    | total timeout within     |
|                                            | which next_rendezvous is |
|                                            | expected to succeed      |
|                                            | (default 600s)           |
+--------------------------------------------+--------------------------+
| last_call_timeout                          | additional wait amount   |
|                                            | (""last call"") after min  |
|                                            | number of workers has    |
|                                            | been reached (defaults   |
|                                            | to 30s)                  |
+--------------------------------------------+--------------------------+
| etcd_prefix                                | path prefix (from etcd   |
|                                            | root), inside which all  |
|                                            | etcd nodes will be       |
|                                            | created (defaults to     |
|                                            | ``/torchelastic/p2p``)   |
+--------------------------------------------+--------------------------+",9,['RendezvousHandler'],0,"['__init__', '__del__', 'get_backend', 'next_rendezvous', 'is_closed', 'set_closed', 'num_nodes_waiting', 'get_run_id', 'shutdown']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\rendezvous\etcd_rendezvous.py,EtcdRendezvous,A rendezvous implementation that uses `etcd <https://etcd.io/>`__ as the backend store.,24,[],0,"['__init__', '__del__', 'rendezvous_barrier', 'init_phase', 'join_phase', 'confirm_phase', 'handle_existing_rendezvous', 'try_create_rendezvous', 'join_rendezvous', 'wait_for_peers', 'confirm_membership', 'wait_for_final', 'announce_self_waiting', 'wait_for_rendezvous_to_free', 'handle_join_last_call', 'set_closed', 'get_rdzv_state', 'try_wait_for_state_change', 'get_path', 'create_path_if_not_exists', 'setup_lease_renewal', 'store_extra_data', 'load_extra_data', 'setup_kv_store']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\rendezvous\etcd_server.py,EtcdServer,".. note:: tested on etcd server v3.4.3.

Starts and stops a local standalone etcd server on a random free
port. Useful for single node, multi-worker launches or testing,
where a sidecar etcd server is more convenient than having to
separately setup an etcd server.

This class registers a termination handler to shutdown the etcd
subprocess on exit. This termination handler is NOT a substitute for
calling the ``stop()`` method.

The following fallback mechanism is used to find the etcd binary:

1. Uses env var TORCHELASTIC_ETCD_BINARY_PATH
2. Uses ``<this file root>/bin/etcd`` if one exists
3. Uses ``etcd`` from ``PATH``

Usage
::

 server = EtcdServer(""/usr/bin/etcd"", 2379, ""/tmp/default.etcd"")
 server.start()
 client = server.get_client()
 # use client
 server.stop()

Args:
    etcd_binary_path: path of etcd server binary (see above for fallback path)",10,[],0,"['__init__', '_get_etcd_server_process', 'get_port', 'get_host', 'get_endpoint', 'start', '_start', 'get_client', '_wait_for_ready', 'stop']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\rendezvous\etcd_store.py,EtcdStore,"Implement a c10 Store interface by piggybacking on the rendezvous etcd instance.

This is the store object returned by ``EtcdRendezvous``.",9,['Store'],0,"['__init__', 'set', 'get', 'add', 'wait', 'check', '_encode', '_decode', '_try_wait_get']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\timer\local_timer.py,LocalTimerClient,"Client side of ``LocalTimerServer``. This client is meant to be used
on the same host that the ``LocalTimerServer`` is running on and uses
pid to uniquely identify a worker. This is particularly useful in situations
where one spawns a subprocess (trainer) per GPU on a host with multiple
GPU devices.",3,['TimerClient'],0,"['__init__', 'acquire', 'release']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\timer\local_timer.py,MultiprocessingRequestQueue,A ``RequestQueue`` backed by python ``multiprocessing.Queue``,3,['RequestQueue'],0,"['__init__', 'size', 'get']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\timer\local_timer.py,LocalTimerServer,"Server that works with ``LocalTimerClient``. Clients are expected to be
subprocesses to the parent process that is running this server. Each host
in the job is expected to start its own timer server locally and each
server instance manages timers for local workers (running on processes
on the same host).",5,['TimerServer'],0,"['__init__', 'register_timers', 'clear_timers', 'get_expired_timers', '_reap_worker']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\agent\server\health_check_server.py,HealthCheckServer,"Interface for health check monitoring server, which can be extended
by starting tcp/http server on the specified port.

Args:

    alive_callback: Callable[[], int], callback to last progress time of agent

    port: int, port number to start tcp/http server

    timeout: int, timeout seconds to decide agent is alive/dead",3,[],3,"['__init__', 'start', 'stop']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\error_handler.py,ErrorHandler,"Write the provided exception object along with some other metadata about
the error in a structured way in JSON format to an error file specified by the
environment variable: ``TORCHELASTIC_ERROR_FILE``. If this environment
variable is not set, then simply logs the contents of what would have been
written to the error file.

This handler may be subclassed to customize the handling of the error.
Subclasses should override ``initialize()`` and ``record_exception()``.",7,[],0,"['_get_error_file_path', 'initialize', '_write_error_file', 'record_exception', 'override_error_code_in_rootcause_data', 'dump_error_file', '_rm']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py,ProcessFailure,"Represent the failed process result. When the worker process fails, it may record failure root cause into the file.

Tries to read the failure timestamp from the provided ``error_file``,
if the ``error_file`` does not exist, the timestamp is the current
timestamp (seconds since epoch).

The ``message`` field is a concise explanation of the failure. If
the error file exists then the message is obtained from the error file.
Otherwise one is generated based on the failure signature.

.. note:: It is assumed that the ``error_file`` is written by
          ``torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler``.
          Otherwise the behavior is undefined.",5,[],7,"['__post_init__', '_get_error_data', '_set_no_reply_file', 'signal_name', 'timestamp_isoformat']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py,ChildFailedError,"Special exception type that can be raised from a function annotated with the
``@record`` decorator to have the child process' (root exception) propagate
up the stack as-is (e.g. without being wrapped in the parent's traceback).

Useful in cases where the parent is a simple nanny process
and the child (worker) processes are actually doing meaningful compute.
In this case, errors typically occur on the child process as the parent
is not doing anything non-trivial, and child errors should be propagated
to the scheduler for accurate root cause diagnostics.

.. note:: The propagation relies on error files rather than exception handling to
          support both function and binary launches.

Example:
::

 # process tree on a host (container)
 0: scheduler-init-process:
            |- 1: torchelastic_agent:
                     |- 2: trainer_0 (ok)
                     |- 3: trainer_1 (fail) -> error.json
                     |- ...
                     |- n+2: trainer_n (ok)
            |- n+3: other processes
            |- ...

In the example above, trainer 1's failure (written into error.json) is
the root cause and should be reported to the scheduler's init process.
The torchelastic agent raises a ``ChildFailedError(""trainer"", {1: ""trainer_1/error.json""})``
upon detecting trainer 1's failure which would propagate the contents
of trainer 1's error file to the scheduler's init process.",4,['Exception'],0,"['__init__', 'get_first_failure', 'format_msg', '_format_failure']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\multiprocessing\subprocess_handler\subprocess_handler.py,SubprocessHandler,"Convenience wrapper around python's ``subprocess.Popen``. Keeps track of
meta-objects associated to the process (e.g. stdout and stderr redirect fds).",3,[],0,"['__init__', '_popen', 'close']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\utils\data\cycling_iterator.py,CyclingIterator,"An iterator decorator that cycles through the
underlying iterator ""n"" times. Useful to ""unroll""
the dataset across multiple training epochs.

The generator function is called as ``generator_fn(epoch)``
to obtain the underlying iterator, where ``epoch`` is a
number less than or equal to ``n`` representing the ``k``th cycle

For example if ``generator_fn`` always returns ``[1,2,3]``
then ``CyclingIterator(n=2, generator_fn)`` will iterate through
``[1,2,3,1,2,3]``",3,[],0,"['__init__', '__iter__', '__next__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\elastic\utils\data\elastic_distributed_sampler.py,ElasticDistributedSampler,"Sampler that restricts data loading to a subset of
the dataset for elastic training.

It is especially useful in conjunction with
:class:`torch.nn.parallel.DistributedDataParallel`. In such case, each
process can pass a DistributedSampler instance as a DataLoader sampler,
and load a subset of the original dataset that is exclusive to it.

.. note::
    Dataset is assumed to be of constant size.

Args:
    dataset: Dataset used for sampling.
    num_replicas (optional): Number of processes participating in
        distributed training.
    rank (optional): Rank of the current process within num_replicas.
    start_index (optional):  Which index of the dataset to start sampling from",3,['DistributedSampler'],0,"['__init__', '__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\tensor\debug\_comm_mode.py,_CommModeModuleTracker,"Inherits ModuleTracker and expands on its functionality to track the
parameters and sharding information of a model at a module-level",9,['ModTracker'],0,"['__init__', '_fw_set_module_hook', '_fw_pre_hook', '_fw_post_hook', '_bw_hook', '__enter__', '__exit__', 'print_paramater_info', 'print_sharding_info']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\tensor\debug\_comm_mode.py,CommDebugMode,":class:`CommDebugMode` is a context manager that counts the number of
functional collectives within its context. It does this using a
``TorchDispatchMode``.

.. note: Not all collectives are supported yet.

Example usage

.. code-block:: python

    mod = ...
    comm_mode = CommDebugMode()
    with comm_mode:
        mod.sum().backward()
    print(comm_mode.get_comm_counts())",13,['TorchDispatchMode'],0,"['__init__', 'generate_json_dump', 'generate_comm_debug_tracing_table', '_get_operations_list', 'get_total_counts', 'get_comm_counts', 'get_parameter_info', 'get_sharding_info', '__enter__', '__exit__', 'log_comm_debug_tracing_table_to_file', '_set_noise_parameters', '__torch_dispatch__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\tensor\experimental\_tp_transform.py,_TensorParallelTransformPass,"This pass is responsible for transforming a single-device graph into a tensor parallel
graph. It will mark the placement strategy of each node in the graph,
partition the graph into distributed graph, then shard the parameters/buffers accordingly.",2,['PassBase'],0,"['__init__', 'call']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\tensor\parallel\fsdp.py,DTensorExtensions,"DTensorExtension is the TensorFlattener extension needed for 2D FSDP + TP.

This is the implementation for FSDPExtensions defined in
https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/_fsdp_extensions.py",7,['FSDPExtensions'],0,"['__init__', 'pre_flatten_transform', 'post_unflatten_transform', 'chunk_tensor', 'chunk_dtensor', 'pre_load_state_dict_transform', 'all_gather_dtensor']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\tensor\_ops\_embedding_ops.py,MaskBuffer,,3,[],2,"['materialize_mask', 'release_mask', 'apply_mask']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\tensor\_ops\_embedding_ops.py,_MaskPartial,"A partial mask placement devised for rowwise sharded embedding op, where we need
to mask and adjust the indices to the local embedding shard, embedding masking
is a special type of the Partial placement

NOTE: the lifecycle of this MaskPartial placement follows the corresponding DTensor
lifecycle, i.e. the indices_mask would only be alive during the lifetime of the DTensor.",7,['Partial'],3,"['_partition_value', '_reduce_value', '_reduce_shard_value', '__eq__', '__hash__', '__repr__', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\fsdp\fully_shard.py,FSDPModule,,13,[],0,"['__new__', 'reshard', 'unshard', 'set_is_last_backward', 'set_requires_gradient_sync', 'set_requires_all_reduce', 'set_reshard_after_backward', 'set_modules_to_forward_prefetch', 'set_modules_to_backward_prefetch', 'set_post_optim_event', 'set_reduce_scatter_divide_factor', '_get_fsdp_state', '_apply']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\fsdp\fully_shard.py,UnshardHandle,"A handle to wait on the unshard op.

Args:
    fsdp_param_group (FSDPParamGroup, optional): FSDP parameter group to
        unshard. This should be ``None`` iff the FSDP module does not
        manage any parameters, meaning the unshard is a no-op.",2,[],0,"['__init__', 'wait']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\fsdp\_fsdp_api.py,MixedPrecisionPolicy,"This configures FSDP's mixed precision. Unlike autocast, this applies mixed
precision at the module level, not op level, which means low-precision
activations are saved for backward and high-to-low-precision casts are
incurred only at module boundaries.

FSDP works well with module-level mixed precision since it keeps the
high-precision sharded parameters in memory anyway. In other words, FSDP
does not require any extra memory to keep a high-precision copy of the
parameters for the optimizer step.

Attributes:
    param_dtype (Optional[torch.dtype]): This specifies the dtype for
        the unsharded parameter and hence the dtype for forward/backward
        computation and the parameter all-gather. If this is ``None``, then
        the unsharded parameter uses the original dtype. The optimizer step
        uses the sharded parameter in the original dtype. (Default:
        ``None``)
    reduce_dtype (Optional[torch.dtype]): This specifies the dtype for
        gradient reduction (i.e. reduce-scatter or all-reduce). If this is
        ``None`` but ``param_dtype`` is not ``None``, then the reduction
        uses the compute dtype. This can be used to run gradient reduction
        in full precision while using low precision for compute. If also
        gradient reduction is disabled via :meth:`set_requires_gradient_sync`,
        then FSDP will accumulate gradients using ``reduce_dtype``.
        (Default: ``None``)
    output_dtype (Optional[torch.dtype]): This specifies the dtype for
        casting floating-point forward outputs. This can be used to
        help implement cases where different modules have different mixed
        precision policies. (Default: ``None``)
    cast_forward_inputs (bool): This specifies whether FSDP should cast the
        forward's floating-point input tensors to ``param_dtype`` or not.",1,[],4,['__post_init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\fsdp\_fsdp_api.py,OffloadPolicy,This base class represents the policy of no offloading.,0,[],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\fsdp\_fsdp_api.py,CPUOffloadPolicy,"This offload policy offloads parameters, gradients, and optimizer states to
CPU. Sharded parameters are copied host-to-device before all-gather. The
all-gathered parameters are freed according to ``reshard_after_forward``.
Sharded gradients are copied device-to-host in backward, and the optimizer
step runs on CPU with CPU optimizer states.

Attributes:
    pin_memory (bool): Whether to pin sharded parameter and gradient
        memory. Pinning memory allows H2D/D2H copying without blocking the
        CPU and in turn, overlap with compute, but pinned memory cannot be
        used by other processes. Set this to ``False`` if you have
        insufficient CPU memory. (Default: ``True``)",0,['OffloadPolicy'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_composable\fsdp\_fsdp_collectives.py,AllGatherResult,,0,['NamedTuple'],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\distributed\_shard\sharded_optim\api.py,ShardedOptimizer,,6,['optim.Optimizer'],0,"['__init__', 'zero_grad', 'step', 'state_dict', 'load_state_dict', 'add_param_group']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\accelerator_partitioner.py,DAGNode,"DAGNode class maintains useful information for a partition (submodule),
and its input submodules and output submodules.",2,[],0,"['__init__', '__str__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\accelerator_partitioner.py,DAG,DAG class contains all the DAG nodes,2,[],0,"['__init__', 'create_node']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\accelerator_partitioner.py,PartitionResult,NameTuple used for returning DAG and a new fx module,0,['NamedTuple'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\accelerator_partitioner.py,Partitioner,"A fx module may not fit into one device.
Partitioner class helps partition one fx module into submodules (partitions),
so that the submodules can be executed crossing different accelerators.
The main function of this class is self.partition_graph.
It partitions the fx module based on the scheme specified in partition_config
A DAG structure is returned
along with a new fx module with submodule nodes.",13,[],0,"['__init__', 'partition_graph', 'find_single_partition', 'size_based_partition', 'saturate_host', 'do_partition', 'dump_dag', 'create_partition', 'create_single_node_partition', 'sparse_nn_partition', 'cost_aware_partition', 'kl_based_partition', 'aot_based_partition']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\const_fold.py,FoldedGraphModule,"FoldedGraphModule is a GraphModule which also contains another
`const_subgraph_module` representing a subgraph which has all const attr
inputs and which can be run once before running the main standard
`graph`. The `const_output_names` are the ordered list names of attrs which
represent what each respective output from the const_subgraph should be set
on which attrs.",3,['torch.fx.GraphModule'],0,"['__init__', '__call__', 'run_folding']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\graph_gradual_typechecker.py,GraphTypeChecker,,3,[],0,"['__init__', 'type_check', 'type_check_node']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\graph_gradual_typechecker.py,Refine,"Symbolic shape inference.
Generates constraints over type variables.
Currently all constraints are equality constraints.",7,[],0,"['__init__', 'refine', 'symbolic_relations', 'replace_dyn_with_fresh_var', 'convert_to_sympy_symbols', 'refine_node', 'infer_symbolic_relations']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\normalize.py,NormalizeArgs,"Normalize arguments to Python targets. This means that
`args/kwargs` will be matched up to the module/functional's
signature and rewritten to exclusively kwargs in positional order
if `normalize_to_only_use_kwargs` is true. Also populates default
values. Does not support positional-only parameters or varargs
parameters (*args, **kwargs).

If the nodes have 'type' metadata, it will use it to disambiguate
overloads. Otherwise, it will throw an error.

Example usage:
    m = torchvision.models.resnet18()
    traced = torch.fx.symbolic_trace(m)
    traced = NormalizeArgs(traced).transform()",4,['Transformer'],0,"['__init__', 'run_node', 'call_function', 'call_module']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\normalize.py,NormalizeOperators,"Normalize callsites that are different ways of ""spelling"" the same
invocation into a single, canonical call. Currently supports:

1. Normalize operators (e.g. operator.add) to the `torch` ops they
   ultimately invoke (e.g. torch.add) when it is possible to statically
   reason that

Example usage:

    m = torchvision.models.resnet18()

    traced = torch.fx.symbolic_trace(m)

    traced = NormalizeOperators(traced).transform()",1,['AnnotateTypesWithSchema'],1,['call_function'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\optimization.py,MklSubgraph,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\optimization.py,UnionFind,,4,[],0,"['__init__', 'make_set', 'find', 'join']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\refinement_types.py,Equality,,4,[],0,"['__init__', '__str__', '__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\schema_type_annotation.py,AnnotateTypesWithSchema,"Use Python function signatures to annotate types for `Nodes` within an FX graph.
This pulls out Python function signatures for:

    1. Standard `torch.nn` Module calls
    2. `torch.nn.functional` calls
    3. Attribute fetches via `get_attr`

Example usage:

    m = torchvision.models.resnet18()

    traced = torch.fx.symbolic_trace(m)

    traced = AnnotateTypesWithSchema(traced).transform()",5,['Transformer'],0,"['__init__', 'call_function', 'call_module', 'get_attr', '_extract_python_return_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\validator.py,ValidationException,,2,['TorchDynamoException'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\validator.py,BisectValidationException,,2,['TorchDynamoException'],0,"['__init__', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\_backward_state.py,BackwardState,"BackwardState is used to pass Python hooks from the forwards pass
into the backwards pass in Dynamo+Compiled Autograd.

It is created by TorchDynamo and has special handling there.
Dynamo will pass an empty BackwardState to the forwards, then populate
members on it (via setattr) only after the forwards graph is finished.
Later on, in CompileAutograd we will inline and add the needed guards
on the BackwardState.

BackwardState is identified and has special handling in AOTAutograd.
During AOTAutograd:
    1) BackwardState is an input to the forwards graph
    2) It must only be used in the backwards
    3) It will be empty in the forwards
    4) In the forwards we add a wrapper to save it
    5) In the backwards it becomes an input
    6) There can only be one per graph

BackwardState requires CompiledAutograd.",0,[],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\fake_tensor_prop.py,FakeTensorProp,"Execute an FX graph Node-by-Node and record a fake tensor representing
the metadata for the node.  Unlike ShapeProp, (1) this propagation
is cheap--it does the propagation with meta tensors which do not actually
store data, and (2) the fake tensors have much more fine grained information,
e.g., they have accurate alias information that can be consulted by looking
at the storages.

Args:
     module (GraphModule): The module to be executed
     mode (Optional[FakeTensorMode]): The dispatch mode used to execute computation indicated by each FX Node.",4,['torch.fx.Interpreter'],0,"['__init__', 'run_node', 'propagate', 'propagate_dont_convert_inputs']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\graph_manipulation.py,size_bytes,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\net_min_base.py,FxNetMinimizerBadModuleError,Raised if failed to split out a minimize module,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\net_min_base.py,FxNetMinimizerRunFuncError,Raised if error occurs during run_a or run_b functions,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\net_min_base.py,FxNetMinimizerResultMismatchError,Raised if comparing function thinks the results are mismatching.,0,['Exception'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\net_min_base.py,_MinimizerSettingBase,"Args:
`accumulate_error`: Instead of using a's input for both converted module to verify
, use the previous outputs of each converted module as input to accumulate the
errors.

`traverse_method`: ""sequential"" or ""binary"" or ""accumulate""
Determine the way of traverse the nodes in FX module.

`find_all`: Minimizer will go through the entire model and return all problematic nodes.

`return_intermediate`: If true, when using `run_nodes()` function to run the
model, intermediate results of all the ops will be returned as output.",1,[],4,['__str__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\net_min_base.py,_MinimizerBase,"This class is used to automatically find problematic nodes in a model. It takes a FX
graphmodule and generate some submodules while traverse the graph. Then two functions
`run_a` and `run_b` will be used to run the same submodule and a function `compare_fn`
will be used to compare the results.

Currently we provides two ways to traverse the graph and generate submodules.
    1. Sequential traversal: this will traverse the graph node by node and generate
       one submodule with one sigle node.
    2. Binary searching: this will do a binary search style traversal on the graph.

For internal Users, a guide can be found here https://fb.quip.com/HDtuAgiKGfkP.",22,[],0,"['__init__', 'run_a', 'run_b', '_store_outputs', '_get_submod_inputs', '_tag_nodes', '_build_submodule', '_run_and_compare', '_binary_search_impl', '_binary_traverse', '_sequential_traverse', '_block_traverse_impl', '_block_traverse', '_defined_traverse', '_accumulate_traverse', '_skip_traverse_impl', '_skip_traverse', '_collect_nodes', 'run_nodes', 'print_report', 'print_reports', 'minimize']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\shape_prop.py,TensorMetadata,,0,['NamedTuple'],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\shape_prop.py,ShapeProp,"Execute an FX graph Node-by-Node and
record the shape and type of the result
into the corresponding node.

Example:
     In this example, we record the shape
     and data type of a module given
     an example input ``torch.randn(50, D_in)``.
     We print the name, shape and dtype of each node.

    class TwoLayerNet(torch.nn.Module):
        def __init__(self, D_in, H, D_out):
            super().__init__()
            self.linear1 = torch.nn.Linear(D_in, H)
            self.linear2 = torch.nn.Linear(H, D_out)
        def forward(self, x):
            h_relu = self.linear1(x).clamp(min=0)
            y_pred = self.linear2(h_relu)
            return y_pred
    N, D_in, H, D_out = 64, 1000, 100, 10
    x = torch.randn(N, D_in)
    y = torch.randn(N, D_out)
    model = TwoLayerNet(D_in, H, D_out)
    gm = torch.fx.symbolic_trace(model)
    sample_input = torch.randn(50, D_in)
    ShapeProp(gm).propagate(sample_input)

    for node in gm.graph.nodes:
        print(node.name, node.meta['tensor_meta'].dtype,
            node.meta['tensor_meta'].shape)

    The output of this code is:

    x torch.float32 torch.Size([50, 1000])
    linear1 torch.float32 torch.Size([50, 100])
    clamp_1 torch.float32 torch.Size([50, 100])
    linear2 torch.float32 torch.Size([50, 10])
    output torch.float32 torch.Size([50, 10])

Args:
     module (GraphModule): The module to be executed
     fake_mode (FakeTensorMode): A fake mode for copying the gm",3,['torch.fx.Interpreter'],0,"['__init__', 'run_node', 'propagate']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\split_module.py,Partition,,2,[],0,"['__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\split_utils.py,Component,A component serves as a container for a subgraph we want to create afterwards.,0,[],9,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\tools_common.py,FxNetAccFusionsFinder,"Finds groups of connected ACC nodes that pass non-tensor data between each other.
Such groups are called fusion groups.",3,[],0,"['__init__', 'recursive_add_node', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,Constraint,,0,[],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,Conj,,3,['Constraint'],0,"['__init__', '__eq__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,Disj,,3,['Constraint'],0,"['__init__', '__eq__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,Prod,,3,['Constraint'],0,"['__init__', '__eq__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,T,True,3,['Constraint'],0,"['__init__', '__eq__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,F,False,3,['Constraint'],0,"['__init__', '__eq__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,BinaryConstraint,Represents all binary operations,3,['Constraint'],0,"['__init__', '__eq__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,BinConstraintT,Binary constraints about tensors,2,['BinaryConstraint'],0,"['__init__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,BinConstraintD,Binary constraints about dimensions,2,['BinaryConstraint'],0,"['__init__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,TGreatestUpperBound,Greatest Upper bound for tensors with dynamic type,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,DGreatestUpperBound,Greatest Upper bound for dimensions,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,CanReshape,can_reshape constraint,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,IndexSelect,,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,Transpose,,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,GetItem,,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,GetItemTensor,,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,CalcConv,,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,CalcMaxPool,,3,['Constraint'],0,"['__init__', '__repr__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,ApplyBroadcasting,,3,['Constraint'],0,"['__init__', '__eq__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,CalcProduct,"Given correct dimensions, calculate the product for flatten accounting for Dyn",3,['Constraint'],0,"['__init__', '__eq__', '__repr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,TVar,Tensor variable with no tensor constructor,3,[],0,"['__init__', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,DVar,Dimension variable,3,[],0,"['__init__', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint.py,BVar,Boolean variable,3,[],0,"['__init__', '__repr__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\migrate_gradual_types\constraint_generator.py,ConstraintGenerator,,3,[],0,"['__init__', 'generate_constraints', 'generate_constraints_node']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\unification\match.py,Dispatcher,,5,[],0,"['__init__', 'add', '__call__', 'resolve', 'register']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\unification\match.py,VarDispatcher,"A dispatcher that calls functions with variable names
>>> # xdoctest: +SKIP
>>> d = VarDispatcher('d')
>>> x = var('x')
>>> @d.register('inc', x)
... def f(x):
...     return x + 1
>>> @d.register('double', x)
... def f(x):
...     return x * 2
>>> d('inc', 10)
11
>>> d('double', 10)
20",1,['Dispatcher'],0,['__call__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\unification\multipledispatch\conflict.py,AmbiguityWarning,,0,['Warning'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\unification\multipledispatch\variadic.py,VariadicSignatureType,,3,['type'],0,"['__subclasscheck__', '__eq__', '__hash__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\unification\multipledispatch\variadic.py,VariadicSignatureMeta,"A metaclass that overrides ``__getitem__`` on the class. This is used to
generate a new type for Variadic signatures. See the Variadic class for
examples of how this behaves.",1,['type'],0,['__getitem__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\experimental\unification\multipledispatch\variadic.py,Variadic,"A class whose getitem method can be used to generate a new type
representing a specific variadic signature.
Examples
--------
>>> # xdoctest: +SKIP
>>> Variadic[int]  # any number of int arguments
<class 'multipledispatch.variadic.Variadic[int]'>
>>> Variadic[(int, str)]  # any number of one of int or str arguments
<class 'multipledispatch.variadic.Variadic[(int, str)]'>
>>> issubclass(int, Variadic[int])
True
>>> issubclass(int, Variadic[(int, str)])
True
>>> issubclass(str, Variadic[(int, str)])
True
>>> issubclass(float, Variadic[(int, str)])
False",0,[],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\backends\cudagraphs.py,CudaGraphsSupport,,1,['OperatorSupport'],0,['is_node_supported'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\infra\partitioner.py,Partition,,5,[],0,"['__init__', '__repr__', 'add_node', 'remove_node', 'size']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\infra\partitioner.py,_DependencyViewer,,3,[],0,"['__init__', 'downstreams_of', 'upstreams_of']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\infra\partitioner.py,CapabilityBasedPartitioner,,6,[],0,"['__init__', '__is_node_supported', 'propose_partitions', 'fuse_partitions', 'remove_bookend_non_compute_ops', 'partition_and_fuse']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\infra\pass_manager.py,PassManager,"Construct a PassManager.

Collects passes and constraints. This defines the pass schedule, manages
pass constraints and pass execution.

Args:
    passes (Optional[List[Callable]]): List of passes. A pass is a
        callable which modifies an object and returns a PassResult
    constraint (Optional[List[Callable]]): List of constraints. A
        constraint is a callable which takes two passes (A, B) and returns
        True if A depends on B and False otherwise. See implementation of
        `this_before_that_pass_constraint` for example.
    steps (int): Max number of times we run the passes (default = 1).
    run_checks_after_each_pass (bool): Whether to run checks and linting
        after each pass
    suppress_check_failures (bool): Whether to raise errors when running
        checks",8,[],4,"['__init__', 'add_pass', 'add_constraint', 'validate_constraints', 'solve_constraints', 'add_checks', 'check', '__call__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\tests\test_pass_manager.py,TestPassManager,,4,['unittest.TestCase'],0,"['test_pass_manager_builder', 'test_this_before_that_pass_constraint', 'test_these_before_those_pass_constraint', 'test_two_pass_managers']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\utils\common.py,HolderModule,"HolderModule is used to copy all the attributes from original module to submodules
that uses the attributes",1,['Module'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\utils\matcher_utils.py,InternalMatch,,1,[],5,['__copy__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\utils\matcher_utils.py,SubgraphMatcher,,8,[],0,"['__init__', '_match_attributes', '_nodes_are_equal', '_is_contained', '_remove_overlapping_matches', '_match_literals', '_match_nodes', 'match']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\utils\matcher_with_name_node_map_utils.py,SubgraphMatcherWithNameNodeMap,"Extends SubgraphMatcher to support querying the matched subgraph nodes through node name,
this requires pattern to have specific format (returning and additional dictionary at the output,
that has node name as key, and the node in the pattern graph as value, see Example for more details)

Difference with SubgraphMatcher is that it takes a `pattern_gm` GraphModule as input during
initialization since we need to modify the graph (which requires `recompile` the GraphModule)

Example::
    def pattern(x, weight):
        conv = F.conv2d(x, weight)
        relu = F.relu(conv)
        return relu, {""conv"": conv, ""relu"": relu}

    def target_graph(x, weight):
        conv = F.conv2d(x, weight)
        relu = F.relu(conv)
        relu *= 2
        return relu

    pattern_gm = capture_pre_autograd_graph(pattern, example_inputs)
    target_gm = capture_pre_autograd_graph(target_graph, example_inputs)
    matcher = SubgraphMatcherWithNameNodeMap(pattern_gm)
    matches = matcher.match(target_gm)
    for match in matches:
        match.name_node_map[""conv""].meta[""annotation""] = ...",2,['SubgraphMatcher'],0,"['__init__', 'match']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\utils\source_matcher_utils.py,SourcePartition,,0,[],5,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\fx\passes\dialect\common\cse_pass.py,CSEPass,,2,['PassBase'],0,"['__init__', 'call']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\jit\mobile\__init__.py,LiteScriptModule,,5,[],0,"['__init__', '__call__', 'find_method', 'forward', 'run_method']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\nn\modules\adaptive.py,AdaptiveLogSoftmaxWithLoss,"Efficient softmax approximation.

As described in
`Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin,
Moustapha Cissé, David Grangier, and Hervé Jégou
<https://arxiv.org/abs/1609.04309>`__.

Adaptive softmax is an approximate strategy for training models with large
output spaces. It is most effective when the label distribution is highly
imbalanced, for example in natural language modelling, where the word
frequency distribution approximately follows the `Zipf's law`_.

Adaptive softmax partitions the labels into several clusters, according to
their frequency. These clusters may contain different number of targets
each.
Additionally, clusters containing less frequent labels assign lower
dimensional embeddings to those labels, which speeds up the computation.
For each minibatch, only clusters for which at least one target is
present are evaluated.

The idea is that the clusters which are accessed frequently
(like the first one, containing most frequent labels), should also be cheap
to compute -- that is, contain a small number of assigned labels.

We highly recommend taking a look at the original paper for more details.

* :attr:`cutoffs` should be an ordered Sequence of integers sorted
  in the increasing order.
  It controls number of clusters and the partitioning of targets into
  clusters. For example setting ``cutoffs = [10, 100, 1000]``
  means that first `10` targets will be assigned
  to the 'head' of the adaptive softmax, targets `11, 12, ..., 100` will be
  assigned to the first cluster, and targets `101, 102, ..., 1000` will be
  assigned to the second cluster, while targets
  `1001, 1002, ..., n_classes - 1` will be assigned
  to the last, third cluster.

* :attr:`div_value` is used to compute the size of each additional cluster,
  which is given as
  :math:`\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor`,
  where :math:`idx` is the cluster index (with clusters
  for less frequent words having larger indices,
  and indices starting from :math:`1`).

* :attr:`head_bias` if set to True, adds a bias term to the 'head' of the
  adaptive softmax. See paper for details. Set to False in the official
  implementation.

.. warning::
    Labels passed as inputs to this module should be sorted according to
    their frequency. This means that the most frequent label should be
    represented by the index `0`, and the least frequent
    label should be represented by the index `n_classes - 1`.

.. note::
    This module returns a ``NamedTuple`` with ``output``
    and ``loss`` fields. See further documentation for details.

.. note::
    To compute log-probabilities for all classes, the ``log_prob``
    method can be used.

Args:
    in_features (int): Number of features in the input tensor
    n_classes (int): Number of classes in the dataset
    cutoffs (Sequence): Cutoffs used to assign targets to their buckets
    div_value (float, optional): value used as an exponent to compute sizes
        of the clusters. Default: 4.0
    head_bias (bool, optional): If ``True``, adds a bias term to the 'head' of the
        adaptive softmax. Default: ``False``

Returns:
    ``NamedTuple`` with ``output`` and ``loss`` fields:
        * **output** is a Tensor of size ``N`` containing computed target
          log probabilities for each example
        * **loss** is a Scalar representing the computed negative
          log likelihood loss

Shape:
    - input: :math:`(N, \texttt{in\_features})` or :math:`(\texttt{in\_features})`
    - target: :math:`(N)` or :math:`()` where each value satisfies :math:`0 <= \texttt{target[i]} <= \texttt{n\_classes}`
    - output1: :math:`(N)` or :math:`()`
    - output2: ``Scalar``

.. _Zipf's law: https://en.wikipedia.org/wiki/Zipf%27s_law",6,['Module'],7,"['__init__', 'reset_parameters', 'forward', '_get_full_log_prob', 'log_prob', 'predict']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\nn\parallel\data_parallel.py,DataParallel,"Implements data parallelism at the module level.

This container parallelizes the application of the given :attr:`module` by
splitting the input across the specified devices by chunking in the batch
dimension (other objects will be copied once per device). In the forward
pass, the module is replicated on each device, and each replica handles a
portion of the input. During the backwards pass, gradients from each replica
are summed into the original module.

The batch size should be larger than the number of GPUs used.

.. warning::
    It is recommended to use :class:`~torch.nn.parallel.DistributedDataParallel`,
    instead of this class, to do multi-GPU training, even if there is only a single
    node. See: :ref:`cuda-nn-ddp-instead` and :ref:`ddp`.

Arbitrary positional and keyword inputs are allowed to be passed into
DataParallel but some types are specially handled. tensors will be
**scattered** on dim specified (default 0). tuple, list and dict types will
be shallow copied. The other types will be shared among different threads
and can be corrupted if written to in the model's forward pass.

The parallelized :attr:`module` must have its parameters and buffers on
``device_ids[0]`` before running this :class:`~torch.nn.DataParallel`
module.

.. warning::
    In each forward, :attr:`module` is **replicated** on each device, so any
    updates to the running module in ``forward`` will be lost. For example,
    if :attr:`module` has a counter attribute that is incremented in each
    ``forward``, it will always stay at the initial value because the update
    is done on the replicas which are destroyed after ``forward``. However,
    :class:`~torch.nn.DataParallel` guarantees that the replica on
    ``device[0]`` will have its parameters and buffers sharing storage with
    the base parallelized :attr:`module`. So **in-place** updates to the
    parameters or buffers on ``device[0]`` will be recorded. E.g.,
    :class:`~torch.nn.BatchNorm2d` and :func:`~torch.nn.utils.spectral_norm`
    rely on this behavior to update the buffers.

.. warning::
    Forward and backward hooks defined on :attr:`module` and its submodules
    will be invoked ``len(device_ids)`` times, each with inputs located on
    a particular device. Particularly, the hooks are only guaranteed to be
    executed in correct order with respect to operations on corresponding
    devices. For example, it is not guaranteed that hooks set via
    :meth:`~torch.nn.Module.register_forward_pre_hook` be executed before
    `all` ``len(device_ids)`` :meth:`~torch.nn.Module.forward` calls, but
    that each such hook be executed before the corresponding
    :meth:`~torch.nn.Module.forward` call of that device.

.. warning::
    When :attr:`module` returns a scalar (i.e., 0-dimensional tensor) in
    :func:`forward`, this wrapper will return a vector of length equal to
    number of devices used in data parallelism, containing the result from
    each device.

.. note::
    There is a subtlety in using the
    ``pack sequence -> recurrent network -> unpack sequence`` pattern in a
    :class:`~torch.nn.Module` wrapped in :class:`~torch.nn.DataParallel`.
    See :ref:`pack-rnn-unpack-with-data-parallelism` section in FAQ for
    details.


Args:
    module (Module): module to be parallelized
    device_ids (list of int or torch.device): CUDA devices (default: all devices)
    output_device (int or torch.device): device location of output (default: device_ids[0])

Attributes:
    module (Module): the module to be parallelized

Example::

    >>> # xdoctest: +SKIP
    >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
    >>> output = net(input_var)  # input_var can be on any device, including CPU",6,"['Module', 'Generic[T]']",0,"['__init__', 'forward', 'replicate', 'scatter', 'parallel_apply', 'gather']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\nn\parallel\__init__.py,DistributedDataParallelCPU,,0,['DistributedDataParallel'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\nn\utils\parametrize.py,ParametrizationList,"A sequential container that holds and manages the original parameters or buffers of a parametrized :class:`torch.nn.Module`.

It is the type of ``module.parametrizations[tensor_name]`` when ``module[tensor_name]``
has been parametrized with :func:`register_parametrization`.

If the first registered parametrization has a ``right_inverse`` that returns one tensor or
does not have a ``right_inverse`` (in which case we assume that ``right_inverse`` is the identity),
it will hold the tensor under the name ``original``.
If it has a ``right_inverse`` that returns more than one tensor, these will be registered as
``original0``, ``original1``, ...

.. warning::
    This class is used internally by :func:`register_parametrization`. It is documented
    here for completeness. It shall not be instantiated by the user.

Args:
    modules (sequence): sequence of modules representing the parametrizations
    original (Parameter or Tensor): parameter or buffer that is parametrized
    unsafe (bool): a boolean flag that denotes whether the parametrization
        may change the dtype and shape of the tensor. Default: `False`
        Warning: the parametrization is not checked for consistency upon registration.
        Enable this flag at your own risk.",3,['ModuleList'],2,"['__init__', 'right_inverse', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\nn\utils\stateless.py,_ReparametrizeModule,,3,[],0,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\nn\utils\_named_member_accessor.py,NamedMemberAccessor,"A class that provides a way to access the submodules and parameters/buffers of a module.

It provides caching mechanism to speed up submodule lookups.
This is useful for functional programming to manipulate the module state.",18,[],0,"['__init__', 'get_submodule', 'swap_submodule', 'get_tensor', 'set_tensor', 'del_tensor', 'swap_tensor', 'get_tensors', 'set_tensors', 'set_tensors_dict', 'del_tensors', 'swap_tensors', 'swap_tensors_dict', 'check_keys', 'named_parameters', 'named_buffers', 'named_tensors', 'named_modules']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,InputAdaptStep,"A protocol that defines a step in the input adapting process.

The input adapting process is a sequence of steps that are applied to the
PyTorch model inputs to transform them into the inputs format expected by the
exported ONNX model. Each step takes the PyTorch model inputs as arguments and
returns the transformed inputs.

This serves as a base formalized construct for the transformation done to model
input signature by any individual component in the exporter.",1,['Protocol'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,InputAdapter,A class that adapts the PyTorch model inputs to exported ONNX model inputs format.,3,[],0,"['__init__', 'append_step', 'apply']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,OutputAdaptStep,"A protocol that defines a step in the output adapting process.

The output adapting process is a sequence of steps that are applied to the
PyTorch model outputs to transform them into the outputs format produced by the
exported ONNX model. Each step takes the PyTorch model outputs as arguments and
returns the transformed outputs.

This serves as a base formalized construct for the transformation done to model
output signature by any individual component in the exporter.",1,['Protocol'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,OutputAdapter,A class that adapts the PyTorch model outputs to exported ONNX model outputs format.,3,[],0,"['__init__', 'append_step', 'apply']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,BindInputStep,Bind the input arguments to the model signature.,2,['InputAdaptStep'],0,"['__init__', 'apply']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,MergeKwargsIntoArgsInputStep,Merge the input kwargs into the input args.,1,['InputAdaptStep'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,LiftParametersAndBuffersIntoArgsInputStep,Append parameters and buffers to model's positional argument list.,2,['InputAdaptStep'],0,"['__init__', 'apply']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,ConvertComplexToRealRepresentationInputStep,"Convert complex dtype tensors to real representation tensors.

ONNX does not support complex dtype tensors. Thus, we convert complex dtype tensors
to real representation tensors (i.e., float dtype tensors with an extra dimension
representing the real and imaginary parts of the complex number).",1,['InputAdaptStep'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,RemoveNoneInputStep,"Remove `None` from arguments.

This adapt step assumes ``model_kwargs`` is empty. It also assumes ``model_args``
is flattened, i.e. it does not check `None` inside nested collections.",1,['InputAdaptStep'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,RemoveNonTensorInputStep,"Remove the non-tensor input arguments.

Dynamo does not support non-tensor input arguments (https://github.com/pytorch/pytorch/issues/99534).

Specifically, it does put the input into graph with an empty node, but consumed by no ones.
The concrete value is embedded into the graph as a constant arg of a target node. Meta
suggests in this case that one should rewrite the model code to make it tensor if the
input value is supposed to change at runtime. We might need to further investigate
the feasibility of that suggestion.

For example,

    def func(x, b=1.0):
        y = x + b
        z = y.relu()
        return (y, z)

    x = torch.randn(1, 1, 2, dtype=torch.float32)
    gm_fun, _ = dynamo.export(func, x, b=8.0, aten_graph=True, tracing_mode=""real"")

    # class GraphModule(torch.nn.Module):
    #     def forward(self, x, b):
    #         arg0: f32[1, 1, 2], arg1, = fx_pytree.tree_flatten_spec(([x, b], {}), self._in_spec)
    #         # File: path/to/pytorch/test_constant_input.py:5, code: y = x + b
    #         add_tensor: f32[1, 1, 2] = torch.ops.aten.add.Tensor(arg0, 8.0);  arg0 = None

    #         # File: path/to/pytorch/test_constant_input.py:6, code: z = y.relu()
    #         relu_default: f32[1, 1, 2] = torch.ops.aten.relu.default(add_tensor)
    #         return pytree.tree_unflatten([add_tensor, relu_default], self._out_spec)

Empty torch.fx.Node input leading to a mismatched number of input with PyTorch, as
it's ignored in ONNX graph. Thus, we delete the useless input here.",1,['InputAdaptStep'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,FlattenInputWithTreeSpecValidationInputStep,"Flatten nested collection types and return a flat list of elements.

ONNX can't represent collection types (e.g., dictionary, tuple of tuple of tensor,
etc).

This class stores the `SpecTree` output produced when `adapt` was called the first
time. It then validates the `SpecTree` output produced from later `adapt` calls.",1,['InputAdaptStep'],1,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,FlattenOutputStep,"Flatten nested collection types and return a flat list of elements.

ONNX can't represent collection types (e.g., dictionary, tuple of tuple of tensor,
etc).

NOTE: Ideally we would want to use ``FlattenOutputWithTreeSpecValidationOutputStep``, such
that `SpecTree` can be validate for new model outputs. However, this is not possible
currently because we never have access to real PyTorch model outputs during export.
Only traced outputs may be available, but they are not an accurate reflection of the
original PyTorch model outputs format as they are typically in their own unique format,
depending on the tracing strategy.",1,['OutputAdaptStep'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,ConvertComplexToRealRepresentationOutputStep,"Convert complex dtype tensors to real representation tensors.

ONNX does not support complex dtype tensors. Thus, we convert complex dtype tensors
to real representation tensors (i.e., float dtype tensors with an extra dimension
representing the real and imaginary parts of the complex number).",1,['OutputAdaptStep'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,FlattenOutputWithTreeSpecValidationOutputStep,"Same as ``FlattenOutputStep``, with additional `TreeSpec` validation.

This class stores the `SpecTree` output produced when `adapt` was called the first
time. It then validates the `SpecTree` output produced from later `adapt` calls.",1,['OutputAdaptStep'],1,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,PrependParamsBuffersConstantAotAutogradInputStep,"Prepend model parameters, buffers and constants to the user input.

:func:`torch.export.export` lifts model parameters, buffers and constants as model input, thus, they
must be added to the user input before the model is executed.

Args:
    model: The PyTorch model with embedded parameters and buffers.",1,['InputAdaptStep'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\io_adapter.py,PrependParamsAndBuffersAotAutogradOutputStep,"Prepend model's mutated buffers to the user output.

:func:`torch.export.export` lifts model's mutated buffers as outputs, thus, they
must be added to the user output after the model is executed.

Args:
    model: The PyTorch model with mutated buffers.",1,['OutputAdaptStep'],0,['apply'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\registration.py,OverrideDict,"A dictionary that merges built-in and custom symbolic functions.

It supports overriding and un-overriding built-in symbolic functions with custom
ones.",13,"['Collection[_K]', 'Generic[_K, _V]']",0,"['__init__', 'set_base', 'in_base', 'override', 'remove_override', 'overridden', '__getitem__', 'get', '__contains__', '__iter__', '__len__', '__repr__', '__bool__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\registration.py,_SymbolicFunctionGroup,"Different versions of symbolic functions registered to the same name.

O(number of registered versions of an op) search is performed to find the most
recent version of the op.

The registration is delayed until op is used to improve startup time.

Function overloads with different arguments are not allowed.
Custom op overrides are supported.",8,[],0,"['__init__', '__repr__', '__getitem__', 'get', 'add', 'add_custom', 'remove_custom', 'get_min_supported']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\registration.py,SymbolicRegistry,"Registry for symbolic functions.

The registry maintains a mapping from qualified names to symbolic functions.
It is used to register new symbolic functions and to dispatch calls to
the appropriate function.",6,[],0,"['__init__', 'register', 'unregister', 'get_function_group', 'is_registered_op', 'all_functions']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\_lazy_import.py,_LazyModule,Lazily import a module.,3,[],0,"['__init__', '__repr__', '__getattr__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_NodeMissingOnnxShapeInference,Node is missing ONNX shape inference.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_MissingCustomSymbolicFunction,"Missing symbolic function for custom PyTorch operator, cannot translate node to ONNX.",2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_MissingStandardSymbolicFunction,"Missing symbolic function for standard PyTorch operator, cannot translate node to ONNX.",2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_OperatorSupportedInNewerOpsetVersion,Operator is supported in newer opset version.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_FxGraphToOnnx,Transforms graph from FX IR to ONNX IR.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_FxNodeToOnnx,Transforms an FX node to an ONNX node.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_FxPass,FX graph transformation during ONNX export before converting from FX IR to ONNX IR.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_NoSymbolicFunctionForCallFunction,"Cannot find symbolic function to convert the ""call_function"" FX node to ONNX.",2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_UnsupportedFxNodeAnalysis,Result from FX graph analysis to reveal unsupported FX nodes.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_OpLevelDebugging,Report any op level validation failure in warnings.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_FindOpschemaMatchedSymbolicFunction,Find the OnnxFunction that matches the input/attribute dtypes by comparing them with their opschemas.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_FxNodeInsertTypePromotion,Determine if type promotion is required for the FX node. Insert cast nodes if needed.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_FindOperatorOverloadsInOnnxRegistry,Find the list of OnnxFunction of the PyTorch operator in onnx registry.,2,['infra.Rule'],0,"['format_message', 'format']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\_rules.py,_POERules,,0,['infra.RuleCollection'],13,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_analysis.py,ModelInfo,Information about the model.,0,[],8,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_building.py,OpRecorder,An onnxscript Evaluator that captures the graph into torchscript.,4,['evaluator.Evaluator'],0,"['__init__', '_call_op', 'eval', 'eval_function']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_core.py,TorchTensor,,4,['ir.Tensor'],0,"['__init__', 'numpy', '__array__', 'tobytes']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_errors.py,TorchExportError,Error during graph capturing using torch.export.,0,['torch.onnx.errors.OnnxExporterError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_errors.py,ConversionError,Error during ExportedProgram to ONNX conversion.,0,['torch.onnx.errors.OnnxExporterError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_errors.py,DispatchError,Error during ONNX Function dispatching.,0,['ConversionError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_errors.py,GraphConstructionError,Error during ONNX graph construction.,0,['ConversionError'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_reporting.py,ExportStatus,,0,[],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\exporter\_verification.py,VerificationInfo,,0,[],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\diagnostics.py,Diagnostic,,1,['infra.Diagnostic'],1,['log'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\diagnostics.py,DiagnosticContext,,1,['infra.DiagnosticContext[Diagnostic]'],2,['__enter__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\diagnostics.py,UnsupportedFxNodeDiagnostic,,1,['Diagnostic'],1,['__post_init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\dynamo_graph_extractor.py,_PyTreeExtensionContext,Context manager to register PyTree extension.,5,[],1,"['__init__', '__enter__', '__exit__', 'register_pytree_node', '_register_huggingface_model_output_extension']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\dynamo_graph_extractor.py,DynamoFlattenOutputStep,"Flatten nested collection and custom python types and return a flat list of elements.

Extended from :class:`io_adapter.FlattenOutputStep` to support flattening arbitrary
types via pytree extension. By default this supports many common user defined python
types such as :class:`ModelOutput` from HuggingFace transformers.

The pytree extension can be customized by passing in a ``_PyTreeExtensionContext``
object. See :meth:`_PyTreeExtensionContext.register_pytree_node`.",2,['io_adapter.FlattenOutputStep'],0,"['__init__', 'apply']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\dynamo_graph_extractor.py,DynamoExport,"Generates a FX GraphModule using torch.dynamo.export API
Args:
    aten_graph: If True, exports a graph with ATen operators.
                If False, exports a graph with Python operators.",3,['_exporter_legacy.FXGraphExtractor'],0,"['__init__', 'generate_fx', 'pre_export_passes']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\fx_symbolic_graph_extractor.py,ModuleExpansionTracer,"Tracer to create ONNX-exporting friendly FX graph.

This tracer traces models into operators. That is,
the traced graph mostly contains call_function nodes and
has no call_module nodes. The call_module nodes
are problematic to the use of make_fx(...) in ONNX
exporter.",2,['torch.fx._symbolic_trace.Tracer'],0,"['is_leaf_module', 'to_bool']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\fx_symbolic_graph_extractor.py,FXSymbolicTracer,"Generates a FX GraphModule using torch.fx.symbolic_trace API
Args:
    concrete_args: Inputs to be partially specialized
        It can be used to remove control flow or data structures.
        For example::
            def f(a, b):
                if b == True:
                    return a
                else:
                    return a*2
        FX can typically not trace through this due to the presence of control
        flow. However, we can use `concrete_args` to specialize on the value of
        `b` to trace through this::
            f = fx.symbolic_trace(f, concrete_args={'b': False})
            assert f(3, False)  == 6
        Note that although you can still pass in different values of `b`, they will be ignored.
        It can also be used to eliminate data-structure handling from
        our function. This will use pytrees to flatten your input. To avoid
        overspecializing, pass in `fx.PH` for values that shouldn't be
        specialized. For example::
            def f(x):
                out = 0
                for v in x.values():
                    out += v
                return out


            f = fx.symbolic_trace(f, concrete_args={""x"": {""a"": fx.PH, ""b"": fx.PH, ""c"": fx.PH}})
            assert f({""a"": 1, ""b"": 2, ""c"": 4}) == 7",4,['_exporter_legacy.FXGraphExtractor'],0,"['__init__', '_trace_into_fx_graph_via_fx_symbolic_trace', 'generate_fx', 'pre_export_passes']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\patcher.py,ONNXTorchPatcher,"Context manager to temporarily patch PyTorch during FX-to-ONNX export.

This class is a collection of ""patches"" required by FX-to-ONNX exporter.

This context overrides several torch functions to support symbolic
export of large scale models.

torch.load:
    This function is patched to record the files PyTorch stores model
    parameters and buffers. Downstream FX-to-ONNX exporter can create
    initializers from these files.
torch.fx._symbolic_trace._wrapped_methods_to_patch:
    This list is extended with (torch.Tensor, ""__getitem__"") so that
    weight[x, :, y] becomes exportable with torch.fx.symbolic_trace.
safetensors.torch.load_file:
    This function is patched to allow safetensors to be loaded within
    FakeTensorMode. Remove after https://github.com/huggingface/safetensors/pull/318

Search for ONNXTorchPatcher in test_fx_to_onnx_with_onnxruntime.py for
example usage.

TODO: Should this really be a global patcher? Can we make it a local patcher?
    A reason for splitting this into several patchers is to patch one part of the code
    as a collateral damage of patching another part of the code. For example, we
    for tracing model with torch._dynamo.export, we don't need to patch
    `torch.fx._symbolic_trace._wrapped_methods_to_patch`",3,[],0,"['__init__', '__enter__', '__exit__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_address.py,Address,"A physical or virtual address, or a range of addresses, in an 'addressable region' (memory or a binary file).",0,['object'],10,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_artifact.py,Artifact,"A single artifact. In some cases, this artifact might be nested within another artifact.",0,['object'],13,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_artifact_change.py,ArtifactChange,A change to a single artifact.,0,['object'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_artifact_content.py,ArtifactContent,Represents the contents of an artifact.,0,['object'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_artifact_location.py,ArtifactLocation,Specifies the location of an artifact.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_attachment.py,Attachment,An artifact relevant to a result.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_code_flow.py,CodeFlow,A set of threadFlows which together describe a pattern of code execution relevant to detecting a result.,0,['object'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_configuration_override.py,ConfigurationOverride,Information about how a specific rule or notification was reconfigured at runtime.,0,['object'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_conversion.py,Conversion,Describes how a converter transformed the output of a static analysis tool from the analysis tool's native output format into the SARIF format.,0,['object'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_edge.py,Edge,Represents a directed edge in a graph.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_edge_traversal.py,EdgeTraversal,Represents the traversal of a single edge during a graph traversal.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_exception.py,Exception,Describes a runtime exception encountered during the execution of an analysis tool.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_external_properties.py,ExternalProperties,The top-level element of an external property file.,0,['object'],21,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_external_property_file_reference.py,ExternalPropertyFileReference,Contains information that enables a SARIF consumer to locate the external property file that contains the value of an externalized property associated with the run.,0,['object'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_external_property_file_references.py,ExternalPropertyFileReferences,References to external property files that should be inlined with the content of a root log file.,0,['object'],17,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_fix.py,Fix,"A proposed fix for the problem represented by a result object. A fix specifies a set of artifacts to modify. For each artifact, it specifies a set of bytes to remove, and provides a set of new bytes to replace them.",0,['object'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_graph.py,Graph,"A network of nodes and directed edges that describes some aspect of the structure of the code (for example, a call graph).",0,['object'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_graph_traversal.py,GraphTraversal,Represents a path through a graph.,0,['object'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_invocation.py,Invocation,The runtime environment of the analysis tool run.,0,['object'],26,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_location.py,Location,A location within a programming artifact.,0,['object'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_location_relationship.py,LocationRelationship,Information about the relation of one location to another.,0,['object'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_logical_location.py,LogicalLocation,A logical location of a construct that produced a result.,0,['object'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_message.py,Message,Encapsulates a message intended to be read by the end user.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_multiformat_message_string.py,MultiformatMessageString,A message string or message format string rendered in multiple formats.,0,['object'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_node.py,Node,Represents a node in a graph.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_notification.py,Notification,"Describes a condition relevant to the tool itself, as opposed to being relevant to a target being analyzed by the tool.",0,['object'],9,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_physical_location.py,PhysicalLocation,A physical location relevant to a result. Specifies a reference to a programming artifact together with a range of bytes or characters within that artifact.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_property_bag.py,PropertyBag,Key/value pairs that provide additional information about the object.,0,['object'],1,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_rectangle.py,Rectangle,An area within an image.,0,['object'],6,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_region.py,Region,A region within an artifact where a result was detected.,0,['object'],12,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_replacement.py,Replacement,The replacement of a single region of an artifact.,0,['object'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_reporting_configuration.py,ReportingConfiguration,Information about a rule or notification that can be configured at runtime.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_reporting_descriptor.py,ReportingDescriptor,"Metadata that describes a specific report produced by the tool, as part of the analysis it provides or its runtime reporting.",0,['object'],14,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_reporting_descriptor_reference.py,ReportingDescriptorReference,Information about how to locate a relevant reporting descriptor.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_reporting_descriptor_relationship.py,ReportingDescriptorRelationship,Information about the relation of one reporting descriptor to another.,0,['object'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_result.py,Result,A result produced by an analysis tool.,0,['object'],30,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_result_provenance.py,ResultProvenance,Contains information about how and when a result was detected.,0,['object'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_run.py,Run,"Describes a single run of an analysis tool, and contains the reported output of that run.",0,['object'],28,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_run_automation_details.py,RunAutomationDetails,Information that describes a run's identity and role within an engineering system process.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_sarif_log.py,SarifLog,Static Analysis Results Format (SARIF) Version 2.1.0 JSON Schema: a standard format for the output of static analysis tools.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_special_locations.py,SpecialLocations,Defines locations of special significance to SARIF consumers.,0,['object'],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_stack.py,Stack,A call stack that is relevant to a result.,0,['object'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_stack_frame.py,StackFrame,A function call within a stack trace.,0,['object'],5,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_suppression.py,Suppression,A suppression that is relevant to a result.,0,['object'],6,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_thread_flow.py,ThreadFlow,Describes a sequence of code locations that specify a path through a single thread of execution such as an operating system or fiber.,0,['object'],6,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_thread_flow_location.py,ThreadFlowLocation,A location visited by an analysis tool while simulating or monitoring the execution of a program.,0,['object'],14,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_tool.py,Tool,The analysis tool that was run.,0,['object'],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_tool_component.py,ToolComponent,"A component, such as a plug-in or the driver, of the analysis tool that was run.",0,['object'],28,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_tool_component_reference.py,ToolComponentReference,"Identifies a particular toolComponent object, either the driver or an extension.",0,['object'],4,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_translation_metadata.py,TranslationMetadata,Provides additional metadata related to translation.,0,['object'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_version_control_details.py,VersionControlDetails,Specifies the information necessary to retrieve a desired revision from a version control system.,0,['object'],7,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_web_request.py,WebRequest,Describes an HTTP request.,0,['object'],9,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\diagnostics\infra\sarif\_web_response.py,WebResponse,Describes the response to an HTTP request.,0,['object'],9,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\analysis\unsupported_nodes.py,UnsupportedFxNodesAnalysisResult,,0,['_pass.AnalysisResult'],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\analysis\unsupported_nodes.py,UnsupportedFxNodesAnalysis,An analysis that detects unsupported FX nodes in the graph.,2,['_pass.Analysis'],0,"['_lint', 'analyze']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\passes\decomp.py,Decompose,,2,['_pass.Transform'],0,"['__init__', '_run']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\passes\functionalization.py,Functionalize,"Functionalize a GraphModule.

This pass utilizes ``functionalization`` utility of ``torch._functorch`` to convert
a GraphModule into a functional form. The two main functionalities are (copied from
its documentations):

* ``functionalization`` removes (intermediate) mutations and aliasing from a
function, while preserving the function's semantics.

* ``functionalization`` also removes mutations (and views) that were performed
on function inputs. However to preserve semantics, functionalize will ""fix up"" the
mutations after the transform has finished running, by detecting if any tensor inputs
""should have"" been mutated, and copying the new data back to the inputs if necessary.
For example, consider::

    def fn(a, b):
        a.add_(b)
        return a

  For a call like `fn(x, y)`, the variable `x` outside is also mutated. Hence just
  functionalizing is not enough for preserving the original semantics. A ""special""
  input mutation step needs to be inserted at the end.::

    # After functionalization, without input mutation ""fix up"".
    # This is not semantically the same. The variable outside the function call that
    # was passed in as `a` is not mutated.
    def fn(a, b):
        new_a = a + b
        return new_a

    # Functionalization with input mutation ""fix up"" that preserves semantics.
    def fn(a, b):
        new_a = a + b

        # Copying the new data back to the inputs
        a.copy_(new_a)

        return new_a

For ONNX inference, it is recommended to run ``RemoveInputMutation`` after this pass.
``RemoveInputMutation`` removes the ""fix up"" nodes that were added by ``Functionalize``,
which are not needed for ONNX inference.",3,['_pass.Transform'],0,"['__init__', '_functionalize', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\passes\functionalization.py,RemoveInputMutation,"Remove `aten.copy_.default` nodes that mutate module inputs.

This pass is recommended to be used after ``Functionalization`` pass.
``Functionalization`` pass adds `aten.copy_.default` nodes to the graph
when it detects mutations to inputs. These nodes are not needed for ONNX export
for inference. They could be useful for training.",1,['_pass.Transform'],0,['_run'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\onnx\_internal\fx\passes\readability.py,RestoreParameterAndBufferNames,"Restore parameter and buffer names from original nn.module.

This pass is useful for readability of the exported ONNX graph. It restores the
parameter and buffer names from the original nn.module. For example, if the original
nn.module has a parameter named `root.linear.0.weight`, and the parameter is renamed to
`_param_constant9` by FX, this pass will rename it back.

This pass must be run after `Decompose` pass. Because this pass is expected to be called on
`fx.GraphModule` produced by `proxy_tensor.make_fx`, where all parameters and buffers
are registered at root level.",3,['_pass.Transform'],0,"['__init__', '_rename_param_and_buffer', '_run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\autocast_test_lists.py,AutocastTestLists,,2,[],0,"['_rnn_cell_args', '__init__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\autocast_test_lists.py,AutocastCPUTestLists,,1,[],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\autocast_test_lists.py,TestAutocast,,2,['TestCase'],0,"['args_maybe_kwargs', '_run_autocast_outofplace']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\common_dist_composable.py,UnitModule,,2,['nn.Module'],0,"['__init__', 'forward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\common_dist_composable.py,CompositeModel,,2,['nn.Module'],0,"['__init__', 'forward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\common_dist_composable.py,UnitParamModule,,2,['nn.Module'],0,"['__init__', 'forward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\common_dist_composable.py,CompositeParamModel,,2,['nn.Module'],0,"['__init__', 'forward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\common_dist_composable.py,FakeSequential,,2,['nn.Module'],0,"['__init__', 'forward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\common_dist_composable.py,NestedSequentialModel,,1,['nn.Module'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\common_dtype.py,_dispatch_dtypes,,1,['tuple'],0,['__add__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\common_jit.py,JitCommonTestCase,,8,['TestCase'],0,"['createFunctionFromGraph', 'assertExportImport', 'assertExportImportModule', 'runAndSaveRNG', 'getExportImportCopy', 'autoDiffErrorMessage', 'assertAutodiffNode', 'checkShapeAnalysis']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\quantization_torch_package_models.py,LinearReluFunctionalChild,,2,['nn.Module'],0,"['__init__', 'forward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\quantization_torch_package_models.py,LinearReluFunctional,,2,['nn.Module'],0,"['__init__', 'forward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\static_module.py,StaticModule,,5,[],0,"['__init__', '__call__', 'benchmark', 'runAsync', 'benchmark_individual_ops']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\data\network1.py,Net,,1,['nn.Module'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\data\network2.py,Net,,1,['nn.Module'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\distributed\common_state_dict.py,VerifyStateDictMixin,,4,[],0,"['_compare_tensor', '_verify_msd', '_verify_osd', '_verify_osd_by_load']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\distributed\distributed_utils.py,MockProcessGroup,,2,['dist.ProcessGroup'],0,"['__init__', 'getBackendName']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\distributed\fake_pg.py,FakeStore,"A fake store is a fake Key-Value store simply for initialization usage
the of fake process group, one can either use FakeStore or HashStore.",0,['dist.Store'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\distributed\rpc_utils.py,SpawnHelper,,2,['MultiProcessTestCase'],0,"['setUp', 'tearDown']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\opinfo\refs.py,PythonRefInfo,An OpInfo for a Python reference of an OpInfo base class operation.,1,['OpInfo'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\opinfo\refs.py,ReductionPythonRefInfo,An OpInfo for a Python reference of an elementwise unary operation.,1,['ReductionOpInfo'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\opinfo\refs.py,ElementwiseUnaryPythonRefInfo,An OpInfo for a Python reference of an elementwise unary operation.,1,['UnaryUfuncInfo'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\opinfo\refs.py,ElementwiseBinaryPythonRefInfo,An OpInfo for a Python reference of an elementwise binary operation.,1,['BinaryUfuncInfo'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\opinfo\utils.py,_dynamic_dispatch_dtypes,,0,['_dispatch_dtypes'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\optests\aot_autograd.py,assert_raises_regex,,3,[],0,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\distributed\_shard\test_common.py,SimpleMegatronLM,,6,['nn.Module'],0,"['__init__', 'forward', 'get_weights', 'get_biases', 'get_weight_grads', 'get_bias_grads']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\distributed\_shard\sharded_tensor\_test_st_common.py,MyShardedModel2,,1,['torch.nn.Module'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\distributed\_shard\sharded_tensor\_test_st_common.py,MyShardedModel1,,1,['torch.nn.Module'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\testing\_internal\opinfo\definitions\fft.py,SpectralFuncPythonRefInfo,An OpInfo for a Python reference of an elementwise unary operation.,1,['SpectralFuncInfo'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\distributed.py,DistributedSampler,"Sampler that restricts data loading to a subset of the dataset.

It is especially useful in conjunction with
:class:`torch.nn.parallel.DistributedDataParallel`. In such a case, each
process can pass a :class:`~torch.utils.data.DistributedSampler` instance as a
:class:`~torch.utils.data.DataLoader` sampler, and load a subset of the
original dataset that is exclusive to it.

.. note::
    Dataset is assumed to be of constant size and that any instance of it always
    returns the same elements in the same order.

Args:
    dataset: Dataset used for sampling.
    num_replicas (int, optional): Number of processes participating in
        distributed training. By default, :attr:`world_size` is retrieved from the
        current distributed group.
    rank (int, optional): Rank of the current process within :attr:`num_replicas`.
        By default, :attr:`rank` is retrieved from the current distributed
        group.
    shuffle (bool, optional): If ``True`` (default), sampler will shuffle the
        indices.
    seed (int, optional): random seed used to shuffle the sampler if
        :attr:`shuffle=True`. This number should be identical across all
        processes in the distributed group. Default: ``0``.
    drop_last (bool, optional): if ``True``, then the sampler will drop the
        tail of the data to make it evenly divisible across the number of
        replicas. If ``False``, the sampler will add extra indices to make
        the data evenly divisible across the replicas. Default: ``False``.

.. warning::
    In distributed mode, calling the :meth:`set_epoch` method at
    the beginning of each epoch **before** creating the :class:`DataLoader` iterator
    is necessary to make shuffling work properly across multiple epochs. Otherwise,
    the same ordering will be always used.

Example::

    >>> # xdoctest: +SKIP
    >>> sampler = DistributedSampler(dataset) if is_distributed else None
    >>> loader = DataLoader(dataset, shuffle=(sampler is None),
    ...                     sampler=sampler)
    >>> for epoch in range(start_epoch, n_epochs):
    ...     if is_distributed:
    ...         sampler.set_epoch(epoch)
    ...     train(loader)",4,['Sampler[_T_co]'],0,"['__init__', '__iter__', '__len__', 'set_epoch']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\tensorboard\_pytorch_graph.py,NodeBase,,2,[],0,"['__init__', '__repr__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\tensorboard\_pytorch_graph.py,NodePy,,1,['NodeBase'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\tensorboard\_pytorch_graph.py,NodePyIO,,1,['NodePy'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\tensorboard\_pytorch_graph.py,NodePyOP,,1,['NodePy'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\tensorboard\_pytorch_graph.py,GraphPy,"Helper class to convert torch.nn.Module to GraphDef proto and visualization with TensorBoard.

GraphDef generation operates in two passes:

In the first pass, all nodes are read and saved to two lists.
One list is for input/output nodes (nodes_io), which only have inbound
or outbound connections, but not both. Another list is for internal
operator nodes (nodes_op). The first pass also saves all scope name
appeared in the nodes in scope_name_appeared list for later processing.

In the second pass, scope names are fully applied to all nodes.
debugNameToScopedName is a mapping from a node's ID to its fully qualified
scope name. e.g. Net1/Linear[0]/1. Unfortunately torch.jit doesn't have
totally correct scope output, so this is nontrivial. The function
populate_namespace_from_OP_to_IO and find_common_root are used to
assign scope name to a node based on the connection between nodes
in a heuristic kind of way. Bookkeeping is done with shallowest_scope_name
and scope_name_appeared.",6,[],0,"['__init__', 'append', 'printall', 'find_common_root', 'populate_namespace_from_OP_to_IO', 'to_proto']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\viz\_cycles.py,Node,,0,['NamedTuple'],4,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\examples\compare.py,FauxTorch,"Emulate different versions of pytorch.

In normal circumstances this would be done with multiple processes
writing serialized measurements, but this simplifies that model to
make the example clearer.",6,[],0,"['__init__', 'extra_overhead', 'add', 'mul', 'cat', 'matmul']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\op_fuzzers\binary.py,BinaryOpFuzzer,,1,['Fuzzer'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\op_fuzzers\sparse_binary.py,BinaryOpSparseFuzzer,,1,['Fuzzer'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\op_fuzzers\sparse_unary.py,UnaryOpSparseFuzzer,,1,['Fuzzer'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\op_fuzzers\spectral.py,SpectralOpFuzzer,,1,['benchmark.Fuzzer'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\op_fuzzers\unary.py,UnaryOpFuzzer,,1,['Fuzzer'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\utils\_stubs.py,TimerClass,This is the portion of the `timeit.Timer` API used by benchmark utils.,2,['Protocol'],0,"['__init__', 'timeit']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\utils\_stubs.py,TimeitModuleType,Modules generated from `timeit_template.cpp`.,1,['Protocol'],0,['timeit'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\benchmark\utils\_stubs.py,CallgrindModuleType,"Replicates the valgrind endpoints in `torch._C`.

These bindings are used to collect Callgrind profiles on earlier versions
of PyTorch and will eventually be removed.",2,['Protocol'],2,"['_valgrind_supported_platform', '_valgrind_toggle']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\_decorator.py,functional_datapipe,,2,[],1,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\_decorator.py,guaranteed_datapipes_determinism,,3,[],1,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\_decorator.py,non_deterministic,,3,[],2,"['__init__', '__call__', 'deterministic_wrapper_fn']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\_decorator.py,runtime_validation_disabled,,3,[],1,"['__init__', '__enter__', '__exit__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\_utils\fetch.py,_BaseDatasetFetcher,,2,[],0,"['__init__', 'fetch']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\_utils\fetch.py,_IterableDatasetFetcher,,2,['_BaseDatasetFetcher'],0,"['__init__', 'fetch']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\_utils\fetch.py,_MapDatasetFetcher,,1,['_BaseDatasetFetcher'],0,['fetch'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\dataframe\datapipes.py,DataFramesAsTuplesPipe,,2,['IterDataPipe'],0,"['__init__', '__iter__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\dataframe\datapipes.py,PerRowDataFramesPipe,,2,['DFIterDataPipe'],0,"['__init__', '__iter__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\dataframe\datapipes.py,ConcatDataFramesPipe,,2,['DFIterDataPipe'],0,"['__init__', '__iter__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\dataframe\datapipes.py,ShuffleDataFramesPipe,,2,['DFIterDataPipe'],0,"['__init__', '__iter__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\dataframe\datapipes.py,FilterDataFramesPipe,,2,['DFIterDataPipe'],0,"['__init__', '__iter__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\dataframe\datapipes.py,ExampleAggregateAsDataFrames,,3,['DFIterDataPipe'],0,"['__init__', '_as_list', '__iter__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\dataframe\structures.py,DataChunkDF,"DataChunkDF iterating over individual items inside of DataFrame containers, to access DataFrames user `raw_iterator`.",2,['DataChunk'],0,"['__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\callable.py,MapperIterDataPipe,"Applies a function over each item from the source DataPipe (functional name: ``map``).

The function can be any regular Python function or partial object. Lambda
function is not recommended as it is not supported by pickle.

Args:
    datapipe: Source Iterable DataPipe
    fn: Function being applied over each item
    input_col: Index or indices of data which ``fn`` is applied, such as:

        - ``None`` as default to apply ``fn`` to the data directly.
        - Integer(s) is used for list/tuple.
        - Key(s) is used for dict.

    output_col: Index of data where result of ``fn`` is placed. ``output_col`` can be specified
        only when ``input_col`` is not ``None``

        - ``None`` as default to replace the index that ``input_col`` specified; For ``input_col`` with
          multiple indices, the left-most one is used, and other indices will be removed.
        - Integer is used for list/tuple. ``-1`` represents to append result at the end.
        - Key is used for dict. New key is acceptable.

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import IterableWrapper, Mapper
    >>> def add_one(x):
    ...     return x + 1
    >>> dp = IterableWrapper(range(10))
    >>> map_dp_1 = dp.map(add_one)  # Invocation via functional form is preferred
    >>> list(map_dp_1)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    >>> # We discourage the usage of `lambda` functions as they are not serializable with `pickle`
    >>> # Use `functools.partial` or explicitly define the function instead
    >>> map_dp_2 = Mapper(dp, lambda x: x + 1)
    >>> list(map_dp_2)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]",4,['IterDataPipe[_T_co]'],2,"['__init__', '_apply_fn', '__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\callable.py,CollatorIterDataPipe,"Collates samples from DataPipe to Tensor(s) by a custom collate function (functional name: ``collate``).

By default, it uses :func:`torch.utils.data.default_collate`.

.. note::
    While writing a custom collate function, you can import :func:`torch.utils.data.default_collate` for the
    default behavior and `functools.partial` to specify any additional arguments.

Args:
    datapipe: Iterable DataPipe being collated
    collate_fn: Customized collate function to collect and combine data or a batch of data.
        Default function collates to Tensor(s) based on data type.

Example:
    >>> # xdoctest: +SKIP
    >>> # Convert integer data to float Tensor
    >>> class MyIterDataPipe(torch.utils.data.IterDataPipe):
    ...     def __init__(self, start, end):
    ...         super(MyIterDataPipe).__init__()
    ...         assert end > start, ""this example code only works with end >= start""
    ...         self.start = start
    ...         self.end = end
    ...
    ...     def __iter__(self):
    ...         return iter(range(self.start, self.end))
    ...
    ...     def __len__(self):
    ...         return self.end - self.start
    ...
    >>> ds = MyIterDataPipe(start=3, end=7)
    >>> print(list(ds))
    [3, 4, 5, 6]
    >>> def collate_fn(batch):
    ...     return torch.tensor(batch, dtype=torch.float)
    ...
    >>> collated_ds = CollateIterDataPipe(ds, collate_fn=collate_fn)
    >>> print(list(collated_ds))
    [tensor(3.), tensor(4.), tensor(5.), tensor(6.)]",1,['MapperIterDataPipe'],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\combinatorics.py,SamplerIterDataPipe,"Generate sample elements using the provided ``Sampler`` (defaults to :class:`SequentialSampler`).

Args:
    datapipe: IterDataPipe to sample from
    sampler: Sampler class to generate sample elements from input DataPipe.
        Default is :class:`SequentialSampler` for IterDataPipe",3,['IterDataPipe[_T_co]'],2,"['__init__', '__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\combinatorics.py,ShufflerIterDataPipe,"Shuffle the input DataPipe with a buffer (functional name: ``shuffle``).

The buffer with ``buffer_size`` is filled with elements from the datapipe first. Then,
each item will be yielded from the buffer by reservoir sampling via iterator.

``buffer_size`` is required to be larger than ``0``. For ``buffer_size == 1``, the
datapipe is not shuffled. In order to fully shuffle all elements from datapipe,
``buffer_size`` is required to be greater than or equal to the size of datapipe.

When it is used with :class:`torch.utils.data.DataLoader`, the methods to
set up random seed are different based on :attr:`num_workers`.

For single-process mode (:attr:`num_workers == 0`), the random seed is set before
the :class:`~torch.utils.data.DataLoader` in the main process. For multi-process
mode (:attr:`num_worker > 0`), `worker_init_fn` is used to set up a random seed
for each worker process.

Args:
    datapipe: The IterDataPipe being shuffled
    buffer_size: The buffer size for shuffling (default to ``10000``)
    unbatch_level: Specifies if it is necessary to unbatch source data before
        applying the shuffle

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import IterableWrapper
    >>> dp = IterableWrapper(range(10))
    >>> shuffle_dp = dp.shuffle()
    >>> list(shuffle_dp)
    [0, 4, 1, 6, 3, 2, 9, 5, 7, 8]",9,['IterDataPipe[_T_co]'],6,"['__init__', 'set_shuffle', 'set_seed', '__iter__', '__len__', 'reset', '__getstate__', '__setstate__', '__del__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\filelister.py,FileListerIterDataPipe,"Given path(s) to the root directory, yields file pathname(s) (path + filename) of files within the root directory.

Multiple root directories can be provided (functional name: ``list_files``).

Args:
    root: Root directory or a sequence of root directories
    masks: Unix style filter string or string list for filtering file name(s)
    recursive: Whether to return pathname from nested directories or not
    abspath: Whether to return relative pathname or absolute pathname
    non_deterministic: Whether to return pathname in sorted order or not.
        If ``False``, the results yielded from each root directory will be sorted
    length: Nominal length of the datapipe

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import FileLister
    >>> dp = FileLister(root=""."", recursive=True)
    >>> list(dp)
    ['example.py', './data/data.tar']",3,['IterDataPipe[str]'],0,"['__init__', '__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\fileopener.py,FileOpenerIterDataPipe,"Given pathnames, opens files and yield pathname and file stream in a tuple (functional name: ``open_files``).

Args:
    datapipe: Iterable datapipe that provides pathnames
    mode: An optional string that specifies the mode in which
        the file is opened by ``open()``. It defaults to ``r``, other options are
        ``b`` for reading in binary mode and ``t`` for text mode.
    encoding: An optional string that specifies the encoding of the
        underlying file. It defaults to ``None`` to match the default encoding of ``open``.
    length: Nominal length of the datapipe

Note:
    The opened file handles will be closed by Python's GC periodically. Users can choose
    to close them explicitly.

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import FileLister, FileOpener, StreamReader
    >>> dp = FileLister(root=""."").filter(lambda fname: fname.endswith('.txt'))
    >>> dp = FileOpener(dp)
    >>> dp = StreamReader(dp)
    >>> list(dp)
    [('./abc.txt', 'abc')]",3,"['IterDataPipe[Tuple[str, IOBase]]']",0,"['__init__', '__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\grouping.py,BatcherIterDataPipe,"Creates mini-batches of data (functional name: ``batch``).

An outer dimension will be added as ``batch_size`` if ``drop_last`` is set to ``True``, or ``length % batch_size`` for the
last batch if ``drop_last`` is set to ``False``.

Args:
    datapipe: Iterable DataPipe being batched
    batch_size: The size of each batch
    drop_last: Option to drop the last batch if it's not full
    wrapper_class: wrapper to apply onto each batch (type ``List``) before yielding,
        defaults to ``DataChunk``

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import IterableWrapper
    >>> dp = IterableWrapper(range(10))
    >>> dp = dp.batch(batch_size=3, drop_last=True)
    >>> list(dp)
    [[0, 1, 2], [3, 4, 5], [6, 7, 8]]",3,['IterDataPipe[DataChunk]'],3,"['__init__', '__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\grouping.py,UnBatcherIterDataPipe,"Undos batching of data (functional name: ``unbatch``).

In other words, it flattens the data up to the specified level within a batched DataPipe.

Args:
    datapipe: Iterable DataPipe being un-batched
    unbatch_level: Defaults to ``1`` (only flattening the top level). If set to ``2``,
        it will flatten the top two levels, and ``-1`` will flatten the entire DataPipe.

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import IterableWrapper
    >>> source_dp = IterableWrapper([[[0, 1], [2]], [[3, 4], [5]], [[6]]])
    >>> dp1 = source_dp.unbatch()
    >>> list(dp1)
    [[0, 1], [2], [3, 4], [5], [6]]
    >>> dp2 = source_dp.unbatch(unbatch_level=2)
    >>> list(dp2)
    [0, 1, 2, 3, 4, 5, 6]",3,['IterDataPipe'],0,"['__init__', '__iter__', '_dive']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\grouping.py,GrouperIterDataPipe,"Groups data from IterDataPipe by keys from ``group_key_fn``, yielding a ``DataChunk`` with batch size up to ``group_size``.

(functional name: ``groupby``).

The samples are read sequentially from the source ``datapipe``, and a batch of samples belonging to the same group
will be yielded as soon as the size of the batch reaches ``group_size``. When the buffer is full,
the DataPipe will yield the largest batch with the same key, provided that its size is larger
than ``guaranteed_group_size``. If its size is smaller, it will be dropped if ``drop_remaining=True``.

After iterating through the entirety of source ``datapipe``, everything not dropped due to the buffer capacity
will be yielded from the buffer, even if the group sizes are smaller than ``guaranteed_group_size``.

Args:
    datapipe: Iterable datapipe to be grouped
    group_key_fn: Function used to generate group key from the data of the source datapipe
    keep_key: Option to yield the matching key along with the items in a tuple,
        resulting in `(key, [items])` otherwise returning [items]
    buffer_size: The size of buffer for ungrouped data
    group_size: The max size of each group, a batch is yielded as soon as it reaches this size
    guaranteed_group_size: The guaranteed minimum group size to be yielded in case the buffer is full
    drop_remaining: Specifies if the group smaller than ``guaranteed_group_size`` will be dropped from buffer
        when the buffer is full

Example:
    >>> import os
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import IterableWrapper
    >>> def group_fn(file):
    ...     return os.path.basename(file).split(""."")[0]
    >>> source_dp = IterableWrapper([""a.png"", ""b.png"", ""a.json"", ""b.json"", ""a.jpg"", ""c.json""])
    >>> dp0 = source_dp.groupby(group_key_fn=group_fn)
    >>> list(dp0)
    [['a.png', 'a.json', 'a.jpg'], ['b.png', 'b.json'], ['c.json']]
    >>> # A group is yielded as soon as its size equals to `group_size`
    >>> dp1 = source_dp.groupby(group_key_fn=group_fn, group_size=2)
    >>> list(dp1)
    [['a.png', 'a.json'], ['b.png', 'b.json'], ['a.jpg'], ['c.json']]
    >>> # Scenario where `buffer` is full, and group 'a' needs to be yielded since its size > `guaranteed_group_size`
    >>> dp2 = source_dp.groupby(group_key_fn=group_fn, buffer_size=3, group_size=3, guaranteed_group_size=2)
    >>> list(dp2)
    [['a.png', 'a.json'], ['b.png', 'b.json'], ['a.jpg'], ['c.json']]",7,['IterDataPipe[DataChunk]'],0,"['__init__', '_remove_biggest_key', '__iter__', 'reset', '__getstate__', '__setstate__', '__del__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\routeddecoder.py,RoutedDecoderIterDataPipe,"Decodes binary streams from input DataPipe, yields pathname and decoded data in a tuple.

(functional name: ``routed_decode``)

Args:
    datapipe: Iterable datapipe that provides pathname and binary stream in tuples
    handlers: Optional user defined decoder handlers. If ``None``, basic and image decoder
        handlers will be set as default. If multiple handles are provided, the priority
        order follows the order of handlers (the first handler has the top priority)
    key_fn: Function for decoder to extract key from pathname to dispatch handlers.
        Default is set to extract file extension from pathname

Note:
    When ``key_fn`` is specified returning anything other than extension, the default
    handler will not work and users need to specify custom handler. Custom handler
    could use regex to determine the eligibility to handle data.",4,"['IterDataPipe[Tuple[str, Any]]']",0,"['__init__', 'add_handler', '__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\selecting.py,FilterIterDataPipe,"Filters out elements from the source datapipe according to input ``filter_fn`` (functional name: ``filter``).

Args:
    datapipe: Iterable DataPipe being filtered
    filter_fn: Customized function mapping an element to a boolean.
    input_col: Index or indices of data which ``filter_fn`` is applied, such as:

        - ``None`` as default to apply ``filter_fn`` to the data directly.
        - Integer(s) is used for list/tuple.
        - Key(s) is used for dict.

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import IterableWrapper
    >>> def is_even(n):
    ...     return n % 2 == 0
    >>> dp = IterableWrapper(range(5))
    >>> filter_dp = dp.filter(filter_fn=is_even)
    >>> list(filter_dp)
    [0, 2, 4]",4,['IterDataPipe[_T_co]'],2,"['__init__', '_apply_filter_fn', '__iter__', '_returnIfTrue']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\streamreader.py,StreamReaderIterDataPipe,"Given IO streams and their label names, yield bytes with label name as tuple.

(functional name: ``read_from_stream``).

Args:
    datapipe: Iterable DataPipe provides label/URL and byte stream
    chunk: Number of bytes to be read from stream per iteration.
        If ``None``, all bytes will be read until the EOF.

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import IterableWrapper, StreamReader
    >>> from io import StringIO
    >>> dp = IterableWrapper([(""alphabet"", StringIO(""abcde""))])
    >>> list(StreamReader(dp, chunk=1))
    [('alphabet', 'a'), ('alphabet', 'b'), ('alphabet', 'c'), ('alphabet', 'd'), ('alphabet', 'e')]",2,"['IterDataPipe[Tuple[str, bytes]]']",0,"['__init__', '__iter__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\iter\utils.py,IterableWrapperIterDataPipe,"Wraps an iterable object to create an IterDataPipe.

Args:
    iterable: Iterable object to be wrapped into an IterDataPipe
    deepcopy: Option to deepcopy input iterable object for each
        iterator. The copy is made when the first element is read in ``iter()``.

.. note::
    If ``deepcopy`` is explicitly set to ``False``, users should ensure
    that the data pipeline doesn't contain any in-place operations over
    the iterable instance to prevent data inconsistency across iterations.

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.iter import IterableWrapper
    >>> dp = IterableWrapper(range(10))
    >>> list(dp)
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]",3,['IterDataPipe'],0,"['__init__', '__iter__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\map\callable.py,MapperMapDataPipe,"Apply the input function over each item from the source DataPipe (functional name: ``map``).

The function can be any regular Python function or partial object. Lambda
function is not recommended as it is not supported by pickle.

Args:
    datapipe: Source MapDataPipe
    fn: Function being applied to each item

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.map import SequenceWrapper, Mapper
    >>> def add_one(x):
    ...     return x + 1
    >>> dp = SequenceWrapper(range(10))
    >>> map_dp_1 = dp.map(add_one)
    >>> list(map_dp_1)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    >>> map_dp_2 = Mapper(dp, lambda x: x + 1)
    >>> list(map_dp_2)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]",3,['MapDataPipe[_T_co]'],2,"['__init__', '__len__', '__getitem__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\map\combinatorics.py,ShufflerIterDataPipe,"Shuffle the input MapDataPipe via its indices (functional name: ``shuffle``).

When it is used with :class:`~torch.utils.data.DataLoader`, the methods to
set up random seed are different based on :attr:`num_workers`.

For single-process mode (:attr:`num_workers == 0`), the random seed is set before
the :class:`~torch.utils.data.DataLoader` in the main process. For multi-process
mode (:attr:`num_worker > 0`), ``worker_init_fn`` is used to set up a random seed
for each worker process.

Args:
    datapipe: MapDataPipe being shuffled
    indices: a list of indices of the MapDataPipe. If not provided, we assume it uses 0-based indexing

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.map import SequenceWrapper
    >>> dp = SequenceWrapper(range(10))
    >>> shuffle_dp = dp.shuffle().set_seed(0)
    >>> list(shuffle_dp)
    [7, 8, 1, 5, 3, 4, 2, 0, 9, 6]
    >>> list(shuffle_dp)
    [6, 1, 9, 5, 2, 4, 7, 3, 8, 0]
    >>> # Reset seed for Shuffler
    >>> shuffle_dp = shuffle_dp.set_seed(0)
    >>> list(shuffle_dp)
    [7, 8, 1, 5, 3, 4, 2, 0, 9, 6]

Note:
    Even thought this ``shuffle`` operation takes a ``MapDataPipe`` as the input, it would return an
    ``IterDataPipe`` rather than a ``MapDataPipe``, because ``MapDataPipe`` should be non-sensitive to
    the order of data order for the sake of random reads, but ``IterDataPipe`` depends on the order
    of data during data-processing.",8,['IterDataPipe[_T_co]'],4,"['__init__', 'set_shuffle', 'set_seed', '__iter__', 'reset', '__len__', '__getstate__', '__setstate__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\map\combining.py,ConcaterMapDataPipe,"Concatenate multiple Map DataPipes (functional name: ``concat``).

The new index of is the cumulative sum of source DataPipes.
For example, if there are 2 source DataPipes both with length 5,
index 0 to 4 of the resulting `ConcatMapDataPipe` would refer to
elements of the first DataPipe, and 5 to 9 would refer to elements
of the second DataPipe.

Args:
    datapipes: Map DataPipes being concatenated

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.map import SequenceWrapper
    >>> dp1 = SequenceWrapper(range(3))
    >>> dp2 = SequenceWrapper(range(3))
    >>> concat_dp = dp1.concat(dp2)
    >>> list(concat_dp)
    [0, 1, 2, 0, 1, 2]",3,['MapDataPipe'],1,"['__init__', '__getitem__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\map\combining.py,ZipperMapDataPipe,"Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``).

This MataPipe is out of bound as soon as the shortest input DataPipe is exhausted.

Args:
    *datapipes: Map DataPipes being aggregated

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.map import SequenceWrapper
    >>> dp1 = SequenceWrapper(range(3))
    >>> dp2 = SequenceWrapper(range(10, 13))
    >>> zip_dp = dp1.zip(dp2)
    >>> list(zip_dp)
    [(0, 10), (1, 11), (2, 12)]",3,"['MapDataPipe[Tuple[_T_co, ...]]']",1,"['__init__', '__getitem__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\map\grouping.py,BatcherMapDataPipe,"Create mini-batches of data (functional name: ``batch``).

An outer dimension will be added as ``batch_size`` if ``drop_last`` is set to ``True``,
or ``length % batch_size`` for the last batch if ``drop_last`` is set to ``False``.

Args:
    datapipe: Iterable DataPipe being batched
    batch_size: The size of each batch
    drop_last: Option to drop the last batch if it's not full

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.map import SequenceWrapper
    >>> dp = SequenceWrapper(range(10))
    >>> batch_dp = dp.batch(batch_size=2)
    >>> list(batch_dp)
    [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]",3,['MapDataPipe[DataChunk]'],3,"['__init__', '__getitem__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\utils\data\datapipes\map\utils.py,SequenceWrapperMapDataPipe,"Wraps a sequence object into a MapDataPipe.

Args:
    sequence: Sequence object to be wrapped into an MapDataPipe
    deepcopy: Option to deepcopy input sequence object

.. note::
  If ``deepcopy`` is set to False explicitly, users should ensure
  that data pipeline doesn't contain any in-place operations over
  the iterable instance, in order to prevent data inconsistency
  across iterations.

Example:
    >>> # xdoctest: +SKIP
    >>> from torchdata.datapipes.map import SequenceWrapper
    >>> dp = SequenceWrapper(range(10))
    >>> list(dp)
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    >>> dp = SequenceWrapper({'a': 100, 'b': 200, 'c': 300, 'd': 400})
    >>> dp['a']
    100",3,['MapDataPipe'],0,"['__init__', '__getitem__', '__len__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\common.py,AotAutograd,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\debugging.py,ReluCompileError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\debugging.py,TestingOnlyCompileError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\debugging.py,ExplainOutput,"This is the output of :func:`torch._dynamo.explain()`
There is no reason to create this class directly.",1,[],8,['__str__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\debugging.py,ExplainWithBackend,"This class is intended to be used as a backend for `torch.compile`. It is
composable with other backends. When used in this way, it accumulates
information about graph breaks, ops, and other info and provides a string
representation summarizing this information.

Attributes:
    backend (str): The name of the backend to use for optimization.
    graphs (list): A list of the graphs captured by TorchDynamo.
    op_count (int): The total number of operations in all optimized graphs.
    break_reasons (list): A list of graph break reasons with stack traces.

Example Usage:
    def fn(x):
        x = torch.sigmoid(x)
        return x

    torch._dynamo.reset()
    eb = ExplainWithBackend(""inductor"")
    optimized_fn = torch.compile(fn, backend=eb)
    result = optimized_fn(torch.randn(5))
    print(eb.output())",3,[],0,"['__init__', '__call__', 'output']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\distributed.py,Bucket,,0,[],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\distributed.py,SubmodCompiler,,3,['torch.fx.interpreter.Interpreter'],0,"['__init__', 'compile_submod', 'run_node']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\distributed.py,DDPOptimizer,"Note [DDPOptimizer]
DDPOptimizer applies when dynamo compiles models wrapped in DistributedDataParallel (DDP),
breaking the dynamo graph into chunks to compile separately, with the breaks aligning to
the boundaries of gradient-allreduce buckets chosen by DDP.

Background/Motivation
 - DDP uses allreduce collectives to synchronize partial gradients computed on different workers
 - DDP groups gradient allreduces into 'buckets' to optimize communication efficiency of all-reduce
 - Parameters grouped into buckets are assumed to be adjacent in time, so they become ready
   at around the same time during backward and thus can share the same allreduce efficiently
 - Allreduces must overlap with backward compute for optimal training performance
 - DDP schedules allreduces using 'hooks' fired from the c++ autograd engine in pytorch, which
   operates when individual grads become 'ready'
 - Dynamo+AOTAutograd produces a single fused graph that runs 'atomically' from the perspective of the
   autograd engine, such that all gradients become 'ready' at the same time.  Hooks fire after the whole
   fused backward function executes, preventing any overlap of compute and communication

Algorithm
 - DDPOptimizer starts off with an FX graph traced by dynamo which represents forward.  It can traverse
   this graph in reverse order to determine the true order that gradients will become ready during backward.
 - Parameter sizes are counted in reverse order, up to a bucket size limit, at which point a new bucket is started
   and a graph break introduced
 - Each of the subgraphs is compiled by the compiler provided to dynamo by the user, and then fused back together
   into an outer module that is returned to the user

Notes
 - It would be better to enforce (by adding an API to DDP) that the bucket splits chosen here are used by DDP,
   and that DDP does not need to detect or optimize bucket order by observing execution at runtime, as it does
   in eager.
 - If Dynamo can't capture a whole graph for the portion of the model wrapped by DDP, this algorithm will currently
   produce splits that do not necessarily align with the buckets used by DDP.  This should result in performance
   degradation approaching the baseline case where graph-splits are not used, but not worse.
 - If the backend compiler fails to compile a single subgraph, it will execute eagerly despite the rest of the
   subgraphs being compiled
 - DDP has a 'parameters_and_buffers_to_ignore' field, which DDPOptimizer attempts to honor by reading markers
   left by DDP on individual parameters.  In cases where other transformations, such as reparameterization, are
   also used, the ignore markers could be lost.  If DDPOptimizer fails to ignore a parameter ignored by DDP,
   it is not catastrophic but could impact performance by choosing sub-optimal bucket splits.
 - DDPOptimizer always ignores all buffers, regardless of their ignore flag, since buffers do not require gradients,
   and therefore aren't allreduced by DDP.  (They are broadcast during forward, but this is not covered by
   DDPOptimizer)

Debugging
 - Generally, it is easiest to debug DDPOptimizer in a single process program, using pdb.
 - In many cases, the log messages are helpful (they show bucket size assignments)-
   just set TORCH_LOGS env to include any of 'dynamo', 'distributed', or 'dist_ddp'.
 - See `benchmarks/dynamo/distributed.py` for a simple harness that will run a toy model or a torchbench model
   in a single process (or with torchrun, in multiple processes)

Args:
    bucket_bytes_cap (int): Controls the size of buckets, in bytes, used to determine graphbreaks.  Should be
        set to match the equivalent parameter on the original DDP module.

    backend_compile_fn (callable): A dynamo compiler function, to be invoked to compile each subgraph.

    first_bucket_cap (int): Controls the size of the first bucket.  Should match DDP's first bucket cap.  DDP
        special-cases the first bucket size since it is sometimes optimal to start a small allreduce early.",6,[],0,"['__init__', '_ignore_parameter', 'add_param', 'add_module_params_to_bucket', 'add_param_args', 'compile_fn']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\backends\registry.py,CompiledFn,,1,['Protocol'],0,['__call__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_dynamo\repro\after_dynamo.py,WrapBackendDebug,,2,[],0,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\passes\add_runtime_assertions_for_constraints_pass.py,InputDim,,0,['NamedTuple'],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\passes\add_runtime_assertions_for_constraints_pass.py,_AddRuntimeAssertionsForInlineConstraintsPass,,4,['PassBase'],0,"['__init__', '_assert_range_constraint', '_insert_assert_async', 'call']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\passes\collect_tracepoints_pass.py,CollectTracepointsPass,Performs constant folding and constant propagation.,2,['PassBase'],0,"['__init__', 'call']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\passes\constant_folding.py,ConstantFolder,,7,['torch.fx.Interpreter'],0,"['__init__', 'is_impure', 'node_to_last_non_output_use', 'run_node', 'insertable_tensor_check', 'add_node_replacement', 'run']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\passes\functionalize_side_effectful_ops_pass.py,_FunctionalizeSideEffectfulOpsPass,"Functionalize ops with side effect in graph module by replacing the op with
functional version of it. A new dependency token (`dep_token`) will be
created and propagated through functional ops to output.
For example:
```
def f(x):
    sym_constrain_range(x.shape[0], min=1, max=3)
    return x.add(3)
```
Will be transformed to:
```
def f(x):
    dep_token0 = _make_dep_token()
    dep_token1 = _functional_sym_constrain_range(
        x.shape[0], min=1, max=3, dep_token=dep_token0
    )

    return x.add(3), dep_token1
```",4,['_ExportPassBaseDeprecatedDoNotUse'],0,"['__init__', 'call', 'call_operator', 'output']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\passes\lift_constants_pass.py,ConstantAttrMap,"A mapping class that understands how to use module constants (tensors,
ScriptObjects, FakeScriptObjects) as keys. We store tensors and FakeScriptObjects normally,
but ScriptObjects are stored by hash, because different torch.ScriptObjects can point to
the same underlying value (but we guarantee that they will `hash()` to the same value
if that's the case).",8,['collections.abc.MutableMapping'],0,"['__init__', '__getitem__', '__setitem__', 'add', '__delitem__', '__iter__', '__len__', '__contains__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\passes\remove_runtime_assertions.py,_RemoveRuntimeAssertionsPass,"Remove runtime assertions inserted by the
_AddRuntimeAssertionsForInlineConstraintsPass.",1,['PassBase'],0,['call'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\passes\replace_view_ops_with_view_copy_ops_pass.py,ReplaceViewOpsWithViewCopyOpsPass,"Our backend expects pure functional operators. For efficiency
purposes, we keep view ops around while functionalizing the exported
program. This pass replaces view ops with view copy ops for backends that
need AOT memory planning.",1,['_ExportPassBaseDeprecatedDoNotUse'],0,['call_operator'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\pass_infra\node_metadata.py,NodeMetadata,,5,[],0,"['__init__', '__getitem__', '__setitem__', '__contains__', 'copy']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\serde\aoti_schema.py,ExternKernelNode,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\serde\aoti_schema.py,ExternKernelNodes,,0,[],1,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\serde\dynamic_shapes.py,RootDim,This represents a _Dim object.,0,[],3,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\serde\dynamic_shapes.py,DynamicShapesSpec,This stores a dynamic_shapes spec for de/serialization.,0,[],2,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\serde\schema_check.py,SchemaUpdateError,,0,['Exception'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\serde\schema_check.py,_Commit,,0,[],7,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\cond_branch_class_method.py,MySubModule,,2,['torch.nn.Module'],0,"['foo', 'forward']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\cond_branch_class_method.py,CondBranchClassMethod,"The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:
  - both branches must take the same args, which must also match the branch args passed to cond.
  - both branches must return a single tensor
  - returned tensor must have the same tensor metadata, e.g. shape and dtype
  - branch function can be free function, nested function, lambda, class methods
  - branch function can not have closure variables
  - no inplace mutations on inputs or global variables


This example demonstrates using class method in cond().

NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.",3,['torch.nn.Module'],0,"['__init__', 'bar', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\cond_branch_nested_function.py,CondBranchNestedFunction,"The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:
  - both branches must take the same args, which must also match the branch args passed to cond.
  - both branches must return a single tensor
  - returned tensor must have the same tensor metadata, e.g. shape and dtype
  - branch function can be free function, nested function, lambda, class methods
  - branch function can not have closure variables
  - no inplace mutations on inputs or global variables

This example demonstrates using nested function in cond().

NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\cond_branch_nonlocal_variables.py,CondBranchNonlocalVariables,"The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:
- both branches must take the same args, which must also match the branch args passed to cond.
- both branches must return a single tensor
- returned tensor must have the same tensor metadata, e.g. shape and dtype
- branch function can be free function, nested function, lambda, class methods
- branch function can not have closure variables
- no inplace mutations on inputs or global variables

This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions.

The code below will not work because capturing closure variables is not supported.
```
my_tensor_var = x + 100
my_primitive_var = 3.14

def true_fn(y):
    nonlocal my_tensor_var, my_primitive_var
    return y + my_tensor_var + my_primitive_var

def false_fn(y):
    nonlocal my_tensor_var, my_primitive_var
    return y - my_tensor_var - my_primitive_var

return cond(x.shape[0] > 5, true_fn, false_fn, [x])
```

NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\cond_closed_over_variable.py,CondClosedOverVariable,torch.cond() supports branches closed over arbitrary variables.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\cond_operands.py,CondOperands,"The operands passed to cond() must be:
- a list of tensors
- match arguments of `true_fn` and `false_fn`

NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\cond_predicate.py,CondPredicate,"The conditional statement (aka predicate) passed to cond() must be one of the following:
  - torch.Tensor with a single element
  - boolean expression

NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\constrain_as_size_example.py,ConstrainAsSizeExample,"If the value is not known at tracing time, you can provide hint so that we
can trace further. Please look at torch._check and torch._check_is_size APIs.
torch._check_is_size is used for values that NEED to be used for constructing
tensor.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\constrain_as_value_example.py,ConstrainAsValueExample,"If the value is not known at tracing time, you can provide hint so that we
can trace further. Please look at torch._check and torch._check_is_size APIs.
torch._check is used for values that don't need to be used for constructing
tensor.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\dictionary.py,Dictionary,Dictionary structures are inlined and flattened along tracing.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\dynamic_shape_assert.py,DynamicShapeAssert,A basic usage of python assertion.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\dynamic_shape_constructor.py,DynamicShapeConstructor,"Tensor constructors should be captured with dynamic shape inputs rather
than being baked in with static shape.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\dynamic_shape_if_guard.py,DynamicShapeIfGuard,"`if` statement with backed dynamic shape predicate will be specialized into
one particular branch and generate a guard. However, export will fail if the
the dimension is marked as dynamic shape from higher level API.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\dynamic_shape_map.py,DynamicShapeMap,functorch map() maps a function over the first tensor dimension.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\dynamic_shape_round.py,DynamicShapeRound,Calling round on dynamic shapes is not supported.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\dynamic_shape_slicing.py,DynamicShapeSlicing,"Slices with dynamic shape arguments should be captured into the graph
rather than being baked in.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\dynamic_shape_view.py,DynamicShapeView,"Dynamic shapes should be propagated to view arguments instead of being
baked into the exported graph.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\fn_with_kwargs.py,FnWithKwargs,Keyword arguments are not supported at the moment.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\list_contains.py,ListContains,List containment relation can be checked on a dynamic shape or constants.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\list_unpack.py,ListUnpack,"Lists are treated as static construct, therefore unpacking should be
erased after tracing.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\model_attr_mutation.py,ModelAttrMutation,Attribute mutation is not supported.,3,['torch.nn.Module'],0,"['__init__', 'recreate_list', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\nested_function.py,NestedFunction,"Nested functions are traced through. Side effects on global captures
are not supported though.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\null_context_manager.py,NullContextManager,Null context manager in Python will be traced out.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\optional_input.py,OptionalInput,Tracing through optional input is not supported yet,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\pytree_flatten.py,PytreeFlatten,Pytree from PyTorch can be captured by TorchDynamo.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\scalar_output.py,ScalarOutput,"Returning scalar values from the graph is supported, in addition to Tensor
outputs. Symbolic shapes are captured and rank is specialized.",2,['torch.nn.Module'],0,"['__init__', 'forward']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\static_for_loop.py,StaticForLoop,A for loop with constant number of iterations should be unrolled in the exported graph.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\static_if.py,StaticIf,"`if` statement with static predicate value should be traced through with the
taken branch.",1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\tensor_setattr.py,TensorSetattr,setattr() call onto tensors is not supported.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\unsupported_operator.py,TorchSymMin,torch.sym_min operator is not supported in export.,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_export\db\examples\user_input_mutation.py,UserInputMutation,Directly mutate user input in forward,1,['torch.nn.Module'],0,['forward'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_functorch\_aot_autograd\functional_utils.py,FunctionalTensorMetadataEq,,2,[],0,"['__init__', '__eq__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_functorch\_aot_autograd\utils.py,PytreeThunk,,2,[],3,"['set', 'unflatten']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\autoheuristic_utils.py,AHFeature,"The context, that AutoHeuristic stores, is a list of features. AutoHeuristic needs to know whether a feature is
categorical (i.e., not a continuous variable) to learn a machine learning model.",1,[],0,['__init__'],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\autoheuristic_utils.py,AHOperation,"AHOperation can be used to augment the data collected by AutoHeuristic.
One might for example store features like m, k, n, but also want to use
features like m*n, or k*n, to learn a heuristic. Instead of storing features
that can be created from the collected data, one can use AHOperation to
create new features from the collected data.",2,[],0,"['__init__', 'apply_operation']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\autoheuristic_utils.py,AHContext,"This class is used to specify which information AutoHeuristic should store. For each choice, AutoHeursitic will
store the context and the collected feedback. The context could be something like the shape of a tensor, i.e.,
information that will help to learn a heuristic.",7,[],2,"['__init__', 'add_feature', 'get_numerical_and_categorical_features', 'get_feature_names_csv', 'get_feature_values_csv', 'get_value', 'apply_operations']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\autoheuristic_utils.py,AHMetadata,,2,[],0,"['__init__', 'to_dict']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\learnedheuristic_interface.py,LearnedHeuristic,LearnedHeuristic is a base class for all learned heuristics.,6,[],0,"['__init__', 'check_precondition', 'get_decision', 'get_confidence_threshold', 'get_name', 'get_decisions_ranked']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\learnedheuristic_interface.py,LearnedHeuristicRegression,,3,['LearnedHeuristic'],0,"['__init__', 'get_feedback', 'get_decision']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\learnedheuristic_interface.py,LearnedHeuristicDecision,,5,['LearnedHeuristic'],0,"['__init__', 'get_choice', 'get_decision', 'get_decisions_ranked', 'get_best_choices']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\learned_heuristic_controller.py,LearnedHeuristicController,"Class that finds and instantiates all learned heuristics. It also provides
a way to get the decision of a learned heuristic.",4,[],2,"['__init__', 'get_heuristics', 'get_decision', 'get_decisions_ranked']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\cpp_template_kernel.py,CppTemplateKernel,,19,['CppKernel'],0,"['__init__', 'render', 'def_kernel', 'call_kernel', 'dtype', 'acc_dtype', 'size', 'stride', 'index', 'slice_nd', 'view', 'permute', 'maybe_codegen_profile', 'unroll_pragma', 'define_buffer', 'reinit_buffer_if_null', 'release_buffer', 'store_pointwise_nodes', 'store_output']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\cpp_template_kernel.py,CppTemplateCaller,"CppTemplateCaller

This class represents a caller for CPP template kernels. It is a subclass of ir.ChoiceCaller.
Attributes:
    name (str): The name of the caller.
    category (str): The category of the caller.
    bmreq (CppBenchmarkRequest): The benchmark request for the caller.
    template_buffer (ir.CppTemplateBuffer): The template buffer for the caller.",6,['ir.ChoiceCaller'],0,"['__init__', 'precompile', 'benchmark', 'hash_key', 'info_dict', 'output_node']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\cpp_utils.py,CppCSEVariable,,5,['CSEVariable'],0,"['__init__', '__repr__', 'update_on_args', '_set_dependent_itervars', 'depends_on']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\cpp_utils.py,CppPrinter,,36,['ExprPrinter'],0,"['_print_Integer', '_print_Where', '_print_ModularIndexing', '_print_FloorDiv', '_print_floor', '_print_FloorToInt', '_print_TruncToInt', '_print_TruncToFloat', '_print_ToFloat', '_print_PythonMod', '_print_CMod', '_print_IntTrueDiv', '_print_PowByNatural', '_print_FloatTrueDiv', '_print_FloatPow', '_print_Pow', '_print_Rational', '_print_ceiling', '_print_CeilToInt', '_print_Min', '_print_Max', '_print_Abs', '_print_OpaqueUnaryFn_cos', '_print_OpaqueUnaryFn_cosh', '_print_OpaqueUnaryFn_acos', '_print_OpaqueUnaryFn_sin', '_print_OpaqueUnaryFn_sinh', '_print_OpaqueUnaryFn_asin', '_print_OpaqueUnaryFn_tan', '_print_OpaqueUnaryFn_tanh', '_print_OpaqueUnaryFn_atan', '_print_OpaqueUnaryFn_sqrt', '_print_RoundToInt', '_print_RoundDecimal', '_print_BooleanTrue', '_print_BooleanFalse']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\cpp_utils.py,LocalizeBufferHandler,,5,['V.WrapperHandler'],0,"['__init__', 'localize', 'load', 'store', 'store_reduction']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\cpp_utils.py,LocalBufferContext,"This class creates a context that helps to generate code involving Inductor IR with
function local buffers. These buffers are constructed during the codegen process and
are used to store intermediate results such as local accumulators. We do not want to
add them to `V.graph` since they are not global and we do not want to add them as
function arguments either. So we patch the codegen processes under this scope to support
these buffers without exposure to the outside world.",6,[],0,"['__init__', '__enter__', '__exit__', 'add_local_buffer', 'localize_function', 'localize_nodes']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\cuda_combined_scheduling.py,CUDACombinedScheduling,"Scheduler for CUDA Kernels, which delegates calls as appropriate
to the CUDA-C++ and Triton Schedulers, which both work for CUDA devices
and use a unified-wrapper for codegen.

If Scheduling code needs to be specialized for the case of mixed Triton / CUDA C++ code,
this would also be the place to do it.",14,['BaseScheduling'],0,"['__init__', 'get_backend_features', 'choose_node_backend', 'can_fuse_vertical', 'can_fuse_horizontal', 'group_fn', 'codegen_template', 'codegen_node', 'codegen_sync', 'flush', 'codegen_combo_kernel', 'benchmark_fused_nodes', 'generate_kernel_code_from_nodes', 'benchmark_combo_kernel']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\triton_split_scan.py,TritonSplitScanKernel,"Generates a triton kernel that supports ops.scan calls while also splitting
the reduction dimension over multiple triton programs.

For this kernel, loop numels will always take the form ``(xdim, rdim)``
and the grid has the shape ``(CeilDiv(rdim, RBLOCK), xdim)``. Communication
between blocks occurs within a global memory workspace buffer, which
must be zero-filled before launching the kernel.

Note that generation for ``ops.reduction`` is not supported.

For details of the communication strategy, see
https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back",7,['TritonKernel'],0,"['__init__', 'should_use_persistent_reduction', 'initialize_range_tree', 'reduction', 'scan', '_get_heuristic', '_get_grid_fn']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\ddp_fusion.py,CommBlock,,0,[],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\dedupe_symint_uses.py,_SymExprHash,Hash for a py_sym_types that will use the underlying sympy expression,2,[],1,"['__hash__', '__eq__']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\dedupe_symint_uses.py,_SymHashingDict,"Wrapper around a dictionary that will convert sym types to hash with _SymExprHash and reuse
existing sym proxies.

SymPy hash is not always reliable so optimistically hash sympy expression, and if those fail,
fallback to symnodes.",6,[],0,"['__init__', '__setitem__', '__getitem__', '__contains__', 'get', '_wrap_to_sym_expr_hash']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,GroupBatchFusionBase,,3,[],0,"['__init__', 'match', 'fuse']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,GroupFusion,"Fuse ops in a group way, e.g, fuse mm/addmm of arbitrary input shapes with fbgemm.gmm.",0,['GroupBatchFusionBase'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchFusion,"Fuse ops in a batch way, e.g, fuse mm/addmm of same input shapes with bmm.",0,['GroupBatchFusionBase'],0,[],True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchPointwiseOpsFusionFactory,,1,['BatchFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,PostGradBatchLinearFusion,Fuse ops in a batch way in post grad (aten level).,4,['BatchFusion'],0,"['_addmm_node_can_be_fused', '_is_input_2d', 'match', 'fuse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,GroupLinearFusion,,4,['GroupFusion'],0,"['_addmm_node_can_be_fused', '_mm_node_can_be_fused', 'match', 'fuse']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchPointwiseMathOpsPostGradFusion,"Batch pointwise math operator (e.g., add, mul) in post grad pass.",4,['BatchPointwiseOpsFusionFactory'],0,"['__init__', '_pointwise_node_can_be_fused', 'match', 'fuse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchLinearLHSFusion,"Batch linear left-hand side fusion. This pass tries to fuse the following patterns:

    torch.nn.functional.linear(x, w1), linear(x, w2),... * linear(x, wn)
    -> torch.mm(x, torch.cat([w1, w2,... * wn]).transpose(0, 1))

We have a separate pass to eliminate contiguous transpose in a generic way.",2,['BatchFusion'],0,"['match', 'fuse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,PreGradBatchLinearFusion,"Batch linear fusion in pre grad pass.
Fuse linear with same size with torch.baddmm",3,['BatchFusion'],0,"['_getitem_args', 'match', 'fuse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchLayernormFusion,Batch layer norm fusion in pre grad pass,2,['BatchFusion'],0,"['match', 'fuse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchPointwiseOpsPreGradFusion,"Batch pointwise ops (e.g., sigmoid, relu, tanh) fusion in pre grad pass.
We fuse it in random place, and the introduced stack node may be merged in split cat.",3,['BatchPointwiseOpsFusionFactory'],0,"['__init__', 'match', 'fuse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchPointwiseOpsPostGradFusion,"Batch pointwise ops (e.g., sigmoid, relu, tanh) fusion in post grad pass.
The introduced stack node may be merged in split cat.",3,['BatchPointwiseOpsFusionFactory'],0,"['__init__', 'match', 'fuse']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchTanhPreGradFusion,,1,['BatchPointwiseOpsPreGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchSigmoidPreGradFusion,,1,['BatchPointwiseOpsPreGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchReLuPreGradFusion,,1,['BatchPointwiseOpsPreGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchTanhPostGradFusion,,1,['BatchPointwiseOpsPostGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchSigmoidPostGradFusion,,1,['BatchPointwiseOpsPostGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchReLuPostGradFusion,,1,['BatchPointwiseOpsPostGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchAddPostGradFusion,,1,['BatchPointwiseMathOpsPostGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchSubPostGradFusion,,1,['BatchPointwiseMathOpsPostGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchDivPostGradFusion,,1,['BatchPointwiseMathOpsPostGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,BatchMulPostGradFusion,,1,['BatchPointwiseMathOpsPostGradFusion'],0,['__init__'],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\group_batch_fusion.py,_OrderedSet,,5,[],0,"['__init__', '__contains__', '__len__', 'append', '__iter__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\joint_graph.py,UniformValueConstantFolder,"Runs constant folding and replaces tensors that have a unifrom value
with a tensor constructor call: aten.full([shape], value, ...)",6,['ConstantFolder'],0,"['__init__', '_support_dynamic_shape', 'insertable_tensor_check', 'add_node_replacement', 'insert_placerholder_values', '_deduce_value']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\misc_patterns.py,NumpyCompatNormalization,,2,[],3,"['__init__', '__call__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\post_grad.py,ConstructorMoverPass,,7,[],0,"['__init__', 'allow_cpu_device', 'cannot_be_moved', 'get_node_device', 'get_cpu_indeg_count', '__call__', 'find_movable_constructors']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\pre_grad.py,NormalizedLinearNode,,4,[],0,"['__init__', 'get_input', 'get_weight', 'get_bias']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\pre_grad.py,NormalizedMatmulNode,,3,[],0,"['__init__', 'get_input', 'get_other']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\reinplace.py,InplaceableOp,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\reinplace.py,ViewOp,,0,[],3,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\split_cat.py,TorchSplit,"Matches a call to torch.split if it is in a normalized form. Ensures that all users of
splits are unique getitems.",2,['CallFunction'],0,"['__init__', '_match']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\split_cat.py,SplitCatSimplifier,"Helper class to simplify split-cat pattern. In simple cases, both split and cat node can be removed in a ""split->cat""
pattern. However, there are various cases where they can't and we need to simplify split/ add transforms before cat.
Some such cases are:
    1. Final node has additional args (not coming from the initial split)
    2. Shuffling of args between split/cat
    3. Some final nodes are non-(cat/stack)
    4. Split-dim != cat-dim (but equal split)

Note that any combination of the above cases can happen.

To deal with 1, 2, & 3 - we iterate over all users of split. And figure out common ""ranges"" that can be merged.
Then, we simplify the split accordingly. In the best case, split can be entirely removed.

To deal with 4, we add some transformations (unflatten + movedim) (See `get_transform_params`).

Finally, depending on final node being cat or stack, unsqueeze/flatten needs to be added.",12,[],0,"['simplify', 'get_user_input_list', 'get_merged_user_inputs', 'get_non_cat_node_input', 'merge_consecutive_inputs', 'get_simplified_split_ranges', 'has_non_overlapping_ranges', 'fill_gaps', 'get_transform_params', 'replace_split', 'replace_cat', 'erase_old_nodes']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\split_cat.py,UnbindCatRemover,"Helper class to merge Unbind->Cat/Stack. Many of the cases are similar to SplitCatSimplifier.

Unbind can't be simplified like splits. So, we can only remove the unbind node. Other than this,
other cases like multiple users, additional args, dim mismatch are similar to `SplitCatSimplifier`,
hence we extend that class.",3,['SplitCatSimplifier'],0,"['remove_unbind', 'get_simplified_split_ranges', 'get_transform_params']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\fx_passes\split_cat.py,GetItem,,2,['CallFunction'],0,"['__init__', 'find_anchor_nodes']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\kernel\conv.py,ConvLayoutParams,,0,['TypedDict'],6,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\package\package.py,PT2ArchiveWriter,,5,[],0,"['__init__', '__enter__', '__exit__', 'writestr', 'write_file']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\package\package.py,PT2ArchiveReader,,7,[],0,"['__init__', '__enter__', '__exit__', 'read', 'extract_to_path', 'extractall', 'get_file_names']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\runtime\triton_heuristics.py,CachingAutotuner,"Simplified version of Triton autotuner that has no invalidation
key and caches the best config to disk to improve cold start times.
Unlike the main triton Autotuner, this version can precompile all
configs, and does not rely on the Triton JIT.",11,['KernelInterface'],0,"['__init__', 'precompile', 'get_device_interface', '_precompile_config', 'bench', 'clone_args', 'benchmark_all_configs', 'autotune_to_one_config', 'save_gpu_kernel', 'coordinate_descent_tuning', 'run']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\runtime\triton_heuristics.py,DebugAutotuner,,2,['CachingAutotuner'],0,"['__init__', 'run']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\artifacts\_MixedMMA100.py,MixedMMA100,,7,['LearnedHeuristicDecision'],0,"['__init__', 'check_precondition', 'get_confidence_threshold', 'get_choice', 'fill_choices', 'get_name', 'get_best_choices']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\artifacts\_MixedMMH100.py,MixedMMH100,,7,['LearnedHeuristicDecision'],0,"['__init__', 'check_precondition', 'get_confidence_threshold', 'get_choice', 'fill_choices', 'get_name', 'get_best_choices']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\artifacts\_MMRankingA100.py,MMRankingA100,,7,['LearnedHeuristicDecision'],0,"['__init__', 'check_precondition', 'get_confidence_threshold', 'get_choice', 'fill_choices', 'get_name', 'get_best_choices']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\artifacts\_MMRankingH100.py,MMRankingH100,,7,['LearnedHeuristicDecision'],0,"['__init__', 'check_precondition', 'get_confidence_threshold', 'get_choice', 'fill_choices', 'get_name', 'get_best_choices']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\autoheuristic\artifacts\_PadMMA100.py,PadMMA100,,6,['LearnedHeuristicRegression'],0,"['__init__', 'check_precondition', 'get_feedback', 'get_confidence_threshold', 'get_name', 'predict']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\cuda\device_op_overrides.py,CUDADeviceOpOverrides,,4,['DeviceOpOverrides'],0,"['import_get_raw_stream_as', 'set_device', 'synchronize', 'device_guard']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\rocm\rocm_benchmark_request.py,ROCmBenchmarkRequest,,7,['GPUDeviceBenchmarkRequest'],0,"['__init__', 'precompile', 'make_run_fn', 'update_workspace_size', 'ensure_dll_loaded', 'cleanup_run_fn', '__str__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\rocm\rocm_template_buffer.py,ROCmTemplateBuffer,,2,['TemplateBuffer'],0,"['__init__', 'get_workspace_size']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_inductor\codegen\xpu\device_op_overrides.py,XPUDeviceOpOverrides,,4,['DeviceOpOverrides'],0,"['import_get_raw_stream_as', 'set_device', 'synchronize', 'device_guard']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_vendor\packaging\_structures.py,InfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torch\_vendor\packaging\_structures.py,NegativeInfinityType,,8,[],0,"['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__gt__', '__ge__', '__neg__']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\api\translate.py,UnsatError,,0,['RuntimeError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\api\ufunc.py,UfunctorBindings,,0,[],2,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\static_runtime\generator.py,GenOpDispatcher,,4,[],0,"['out_variant', 'view', 'out_variant_op_generator', 'view_op_generator']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\static_runtime\generator.py,GenOpTestCase,,4,[],0,"['out_variant', 'view', 'out_variant_op_test_case_generator', 'view_op_test_case_generator']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\api\types\types.py,OptionalCType,,3,['CType'],1,"['cpp_type', 'cpp_type_registration_declarations', 'remove_const_ref']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\api\types\types.py,ListCType,,3,['CType'],1,"['cpp_type', 'cpp_type_registration_declarations', 'remove_const_ref']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\api\types\types.py,ArrayRefCType,,3,['CType'],1,"['cpp_type', 'cpp_type_registration_declarations', 'remove_const_ref']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\api\types\types.py,VectorizedCType,,3,['CType'],1,"['cpp_type', 'cpp_type_registration_declarations', 'remove_const_ref']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\executorch\api\unboxing.py,Unboxing,"Takes a sequence of Bindings and unbox EValues to these Bindings. Return generated code that performs correct unboxing.
A sample generated code:
// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
void mul_out(EValue** stack) {
    EValue& self = *stack[0];
    EValue& other = *stack[1];
    EValue& out = *stack[2];
    const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();

    EXECUTORCH_SCOPE_PROF(""native_call_mul.out"");
    torch::executor::mul_outf(self_base, other_base, out_base);


}",5,[],1,"['convert_arguments', 'argumenttype_evalue_convert', '_gen_code_base_type', '_gen_code_optional_type', '_gen_code_list_type']",True
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\executorch\api\types\types.py,OptionalCType,,3,['CType'],1,"['cpp_type', 'cpp_type_registration_declarations', 'remove_const_ref']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\torchgen\executorch\api\types\types.py,ArrayRefCType,,3,['CType'],1,"['cpp_type', 'cpp_type_registration_declarations', 'remove_const_ref']",False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\urllib3\util\ssl_match_hostname.py,CertificateError,,0,['ValueError'],0,[],False
c:\mygit\compuse\computer_use_demo\.venv\Lib\site-packages\urllib3\contrib\emscripten\request.py,EmscriptenRequest,,2,[],7,"['set_header', 'set_body']",False
